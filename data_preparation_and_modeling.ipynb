{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier,VotingClassifier,RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#from sklearn import tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix, plot_roc_curve\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_major = df_train[(df_train[\"class\"]==\"nomral\")|(df_train[\"class\"]==\"dos\")|(df_train[\"class\"]==\"probe\")]\\ndf_major = df_train.sample(n=17672)\\ndf_minor = df_train[(df_train[\"class\"]==\"r2l\")|(df_train[\"class\"]==\"u2r\")]\\ndf_train = pd.concat([df_major, df_minor])\\nprint(df_minor.shape[0])\\nprint(df_major.shape[0])\\nprint(df_train.shape[0])'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = [\"duration\",\"protocol\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\n",
    "            \"num_failed_logins\",\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\n",
    "            \"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\"is_hot_logins\",\"is_giest_login\",\"count\",\"srv_count\",\n",
    "             \"serror_rate\",\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\n",
    "            \"dst_host_count\",\"dst_host_srv_count\",\"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "            \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\n",
    "            \"class\",\"difficulty_score\"]\n",
    "df_train = pd.read_csv(\"./train.csv\",header=None, names=col_names)\n",
    "df_test = pd.read_csv(\"./test.csv\",header=None, names=col_names)\n",
    "\n",
    "#Apply dataset balancing\n",
    "'''df_major = df_train[(df_train[\"class\"]==\"nomral\")|(df_train[\"class\"]==\"dos\")|(df_train[\"class\"]==\"probe\")]\n",
    "df_major = df_train.sample(n=17672)\n",
    "df_minor = df_train[(df_train[\"class\"]==\"r2l\")|(df_train[\"class\"]==\"u2r\")]\n",
    "df_train = pd.concat([df_major, df_minor])\n",
    "print(df_minor.shape[0])\n",
    "print(df_major.shape[0])\n",
    "print(df_train.shape[0])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['protocol'] = df_train['protocol'].astype('category')\n",
    "df_train['protocol']= df_train['protocol'].cat.codes\n",
    "df_train['service'] = df_train['service'].astype('category')\n",
    "df_train['service']= df_train['service'].cat.codes\n",
    "df_train['flag'] = df_train['flag'].astype('category')\n",
    "df_train['flag']= df_train['flag'].cat.codes\n",
    "\n",
    "df_test['protocol'] = df_test['protocol'].astype('category')\n",
    "df_test['protocol']= df_test['protocol'].cat.codes\n",
    "df_test['service'] = df_test['service'].astype('category')\n",
    "df_test['service']= df_test['service'].cat.codes\n",
    "df_test['flag'] = df_test['flag'].astype('category')\n",
    "df_test['flag']= df_test['flag'].cat.codes\n",
    "\n",
    "y_train = df_train.iloc[:, 41]\n",
    "y_test = df_test.iloc[:,41]\n",
    "x_train = df_train.iloc[:,0:41]\n",
    "x_test = df_test.iloc[:,0:41]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Hyperparameters:\n",
      " {'criterion': 'entropy', 'min_samples_split': 2}\n",
      "total accuracy: 0.7623757984386089\n",
      "dos accuracy: 0.8153660498793243\n",
      "normal accuracy 0.9693131500360416\n",
      "probe accuracy 0.6100784799669559\n",
      "r2l accuracy 0.08144087705559906\n",
      "u2r accuracy 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         dos       0.96      0.82      0.88      7458\n",
      "      normal       0.67      0.97      0.79      9711\n",
      "       probe       0.74      0.61      0.67      2421\n",
      "         r2l       0.95      0.08      0.15      2554\n",
      "         u2r       0.57      0.02      0.04       400\n",
      "\n",
      "    accuracy                           0.76     22544\n",
      "   macro avg       0.78      0.50      0.51     22544\n",
      "weighted avg       0.80      0.76      0.72     22544\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Decision-tree Classifier\n",
    "dt_params = {'criterion' : ['gini', 'entropy'],\n",
    "             'min_samples_split' : [2, 4, 6, 8, 10]}\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_gscv = GridSearchCV(dt_clf, dt_params, n_jobs=-1, verbose=10)\n",
    "\n",
    "dt_gscv.fit(x_train, y_train)\n",
    "best_dt_clf = dt_gscv.best_estimator_\n",
    "best_dt_params = dt_gscv.best_params_\n",
    "\n",
    "dump(best_dt_clf, 'best_dt_clf.joblib')\n",
    "\n",
    "print(\"Best Hyperparameters:\\n\", best_dt_params)\n",
    "y_pred = best_dt_clf.predict(x_test)\n",
    "print(\"total accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(\"dos accuracy:\",result[0][0]/sum(result[0]))\n",
    "print(\"normal accuracy\",result[1][1]/sum(result[1]))\n",
    "print(\"probe accuracy\",result[2][2]/sum(result[2]))\n",
    "print(\"r2l accuracy\",result[3][3]/sum(result[3]))\n",
    "print(\"u2r accuracy\",result[4][4]/sum(result[4]))\n",
    "\n",
    "plot_confusion_matrix(best_dt_clf, x_test, y_test)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Best Hyperparameters:\n",
      " {'criterion': 'entropy', 'min_samples_split': 2, 'n_estimators': 200}\n",
      "total accuracy: 0.7422374024130589\n",
      "dos accuracy: 0.7692410834003754\n",
      "normal accuracy 0.9726083822469365\n",
      "probe accuracy 0.630731102850062\n",
      "r2l accuracy 0.009005481597494126\n",
      "u2r accuracy 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         dos       0.96      0.77      0.85      7458\n",
      "      normal       0.64      0.97      0.77      9711\n",
      "       probe       0.86      0.63      0.73      2421\n",
      "         r2l       0.96      0.01      0.02      2554\n",
      "         u2r       0.50      0.00      0.00       400\n",
      "\n",
      "    accuracy                           0.74     22544\n",
      "   macro avg       0.78      0.48      0.48     22544\n",
      "weighted avg       0.80      0.74      0.70     22544\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/10] START criterion=gini, min_samples_split=2........................\n",
      "[CV 1/5; 1/10] END criterion=gini, min_samples_split=2;, score=0.998 total time=   1.8s\n",
      "[CV 5/5; 2/10] START criterion=gini, min_samples_split=4........................\n",
      "[CV 5/5; 2/10] END criterion=gini, min_samples_split=4;, score=0.997 total time=   1.4s\n",
      "[CV 4/5; 4/10] START criterion=gini, min_samples_split=8........................\n",
      "[CV 4/5; 4/10] END criterion=gini, min_samples_split=8;, score=0.998 total time=   1.5s\n",
      "[CV 1/5; 6/10] START criterion=entropy, min_samples_split=2.....................\n",
      "[CV 1/5; 6/10] END criterion=entropy, min_samples_split=2;, score=0.997 total time=   1.1s\n",
      "[CV 4/5; 7/10] START criterion=entropy, min_samples_split=4.....................\n",
      "[CV 4/5; 7/10] END criterion=entropy, min_samples_split=4;, score=0.998 total time=   1.2s\n",
      "[CV 4/5; 9/10] START criterion=entropy, min_samples_split=8.....................\n",
      "[CV 4/5; 9/10] END criterion=entropy, min_samples_split=8;, score=0.998 total time=   1.0s\n",
      "[CV 1/5; 1/10] START criterion=gini, min_samples_split=2........................\n",
      "[CV 1/5; 1/10] END criterion=gini, min_samples_split=2;, score=0.998 total time=   1.4s\n",
      "[CV 4/5; 2/10] START criterion=gini, min_samples_split=4........................\n",
      "[CV 4/5; 2/10] END criterion=gini, min_samples_split=4;, score=0.998 total time=   1.6s\n",
      "[CV 2/5; 4/10] START criterion=gini, min_samples_split=8........................\n",
      "[CV 2/5; 4/10] END criterion=gini, min_samples_split=8;, score=0.998 total time=   1.4s\n",
      "[CV 5/5; 5/10] START criterion=gini, min_samples_split=10.......................\n",
      "[CV 5/5; 5/10] END criterion=gini, min_samples_split=10;, score=0.997 total time=   1.4s\n",
      "[CV 4/5; 8/10] START criterion=entropy, min_samples_split=6.....................\n",
      "[CV 4/5; 8/10] END criterion=entropy, min_samples_split=6;, score=0.998 total time=   1.0s\n",
      "[CV 2/5; 10/10] START criterion=entropy, min_samples_split=10...................\n",
      "[CV 2/5; 10/10] END criterion=entropy, min_samples_split=10;, score=0.997 total time=   1.0s\n",
      "[CV 2/5; 2/10] START criterion=gini, min_samples_split=4........................\n",
      "[CV 2/5; 2/10] END criterion=gini, min_samples_split=4;, score=0.998 total time=   1.8s\n",
      "[CV 5/5; 3/10] START criterion=gini, min_samples_split=6........................\n",
      "[CV 5/5; 3/10] END criterion=gini, min_samples_split=6;, score=0.997 total time=   2.0s\n",
      "[CV 4/5; 5/10] START criterion=gini, min_samples_split=10.......................\n",
      "[CV 4/5; 5/10] END criterion=gini, min_samples_split=10;, score=0.998 total time=   1.6s\n",
      "[CV 1/5; 7/10] START criterion=entropy, min_samples_split=4.....................\n",
      "[CV 1/5; 7/10] END criterion=entropy, min_samples_split=4;, score=0.997 total time=   1.0s\n",
      "[CV 3/5; 8/10] START criterion=entropy, min_samples_split=6.....................\n",
      "[CV 3/5; 8/10] END criterion=entropy, min_samples_split=6;, score=0.998 total time=   1.0s\n",
      "[CV 1/5; 10/10] START criterion=entropy, min_samples_split=10...................\n",
      "[CV 1/5; 10/10] END criterion=entropy, min_samples_split=10;, score=0.997 total time=   1.0s\n",
      "[CV 4/5; 1/30] START criterion=gini, min_samples_split=2, n_estimators=200......\n",
      "[CV 4/5; 1/30] END criterion=gini, min_samples_split=2, n_estimators=200;, score=0.999 total time=  27.5s\n",
      "[CV 2/5; 3/30] START criterion=gini, min_samples_split=2, n_estimators=600......\n",
      "[CV 2/5; 3/30] END criterion=gini, min_samples_split=2, n_estimators=600;, score=0.999 total time= 1.5min\n",
      "[CV 1/5; 5/30] START criterion=gini, min_samples_split=2, n_estimators=1000.....\n",
      "[CV 1/5; 5/30] END criterion=gini, min_samples_split=2, n_estimators=1000;, score=0.999 total time= 2.4min\n",
      "[CV 1/5; 7/30] START criterion=gini, min_samples_split=4, n_estimators=400......\n",
      "[CV 1/5; 7/30] END criterion=gini, min_samples_split=4, n_estimators=400;, score=0.999 total time=  55.2s\n",
      "[CV 3/5; 8/30] START criterion=gini, min_samples_split=4, n_estimators=600......\n",
      "[CV 3/5; 8/30] END criterion=gini, min_samples_split=4, n_estimators=600;, score=0.999 total time= 1.3min\n",
      "[CV 1/5; 10/30] START criterion=gini, min_samples_split=4, n_estimators=1000....\n",
      "[CV 1/5; 10/30] END criterion=gini, min_samples_split=4, n_estimators=1000;, score=0.999 total time= 2.3min\n",
      "[CV 2/5; 12/30] START criterion=gini, min_samples_split=6, n_estimators=400.....\n",
      "[CV 2/5; 12/30] END criterion=gini, min_samples_split=6, n_estimators=400;, score=0.999 total time=  56.0s\n",
      "[CV 2/5; 14/30] START criterion=gini, min_samples_split=6, n_estimators=800.....\n",
      "[CV 2/5; 14/30] END criterion=gini, min_samples_split=6, n_estimators=800;, score=0.999 total time= 2.1min\n",
      "[CV 5/5; 15/30] START criterion=gini, min_samples_split=6, n_estimators=1000....\n",
      "[CV 5/5; 15/30] END criterion=gini, min_samples_split=6, n_estimators=1000;, score=0.999 total time= 2.5min\n",
      "[CV 1/5; 19/30] START criterion=entropy, min_samples_split=2, n_estimators=800..\n",
      "[CV 1/5; 19/30] END criterion=entropy, min_samples_split=2, n_estimators=800;, score=0.999 total time= 1.7min\n",
      "[CV 4/5; 20/30] START criterion=entropy, min_samples_split=2, n_estimators=1000.\n",
      "[CV 4/5; 20/30] END criterion=entropy, min_samples_split=2, n_estimators=1000;, score=0.999 total time= 2.1min\n",
      "[CV 4/5; 23/30] START criterion=entropy, min_samples_split=4, n_estimators=600..\n",
      "[CV 4/5; 23/30] END criterion=entropy, min_samples_split=4, n_estimators=600;, score=0.999 total time= 1.3min\n",
      "[CV 2/5; 25/30] START criterion=entropy, min_samples_split=4, n_estimators=1000.\n",
      "[CV 2/5; 25/30] END criterion=entropy, min_samples_split=4, n_estimators=1000;, score=0.999 total time= 2.2min\n",
      "[CV 5/5; 27/30] START criterion=entropy, min_samples_split=6, n_estimators=400..\n",
      "[CV 5/5; 27/30] END criterion=entropy, min_samples_split=6, n_estimators=400;, score=0.999 total time=  51.3s\n",
      "[CV 3/5; 29/30] START criterion=entropy, min_samples_split=6, n_estimators=800..\n",
      "[CV 3/5; 29/30] END criterion=entropy, min_samples_split=6, n_estimators=800;, score=0.999 total time= 1.7min\n",
      "[CV 1/5; 2/10] START criterion=gini, min_samples_split=4........................\n",
      "[CV 1/5; 2/10] END criterion=gini, min_samples_split=4;, score=0.997 total time=   1.8s\n",
      "[CV 4/5; 2/10] START criterion=gini, min_samples_split=4........................\n",
      "[CV 4/5; 2/10] END criterion=gini, min_samples_split=4;, score=0.998 total time=   1.4s\n",
      "[CV 2/5; 4/10] START criterion=gini, min_samples_split=8........................\n",
      "[CV 2/5; 4/10] END criterion=gini, min_samples_split=8;, score=0.997 total time=   1.5s\n",
      "[CV 5/5; 5/10] START criterion=gini, min_samples_split=10.......................\n",
      "[CV 5/5; 5/10] END criterion=gini, min_samples_split=10;, score=0.997 total time=   1.6s\n",
      "[CV 5/5; 8/10] START criterion=entropy, min_samples_split=6.....................\n",
      "[CV 5/5; 8/10] END criterion=entropy, min_samples_split=6;, score=0.998 total time=   1.1s\n",
      "[CV 3/5; 10/10] START criterion=entropy, min_samples_split=10...................\n",
      "[CV 3/5; 10/10] END criterion=entropy, min_samples_split=10;, score=0.998 total time=   0.9s\n",
      "[CV 3/5; 2/10] START criterion=gini, min_samples_split=4........................\n",
      "[CV 3/5; 2/10] END criterion=gini, min_samples_split=4;, score=0.997 total time=   1.9s\n",
      "[CV 1/5; 4/10] START criterion=gini, min_samples_split=8........................\n",
      "[CV 1/5; 4/10] END criterion=gini, min_samples_split=8;, score=0.997 total time=   1.4s\n",
      "[CV 4/5; 5/10] START criterion=gini, min_samples_split=10.......................\n",
      "[CV 4/5; 5/10] END criterion=gini, min_samples_split=10;, score=0.998 total time=   1.4s\n",
      "[CV 1/5; 7/10] START criterion=entropy, min_samples_split=4.....................\n",
      "[CV 1/5; 7/10] END criterion=entropy, min_samples_split=4;, score=0.997 total time=   1.0s\n",
      "[CV 3/5; 8/10] START criterion=entropy, min_samples_split=6.....................\n",
      "[CV 3/5; 8/10] END criterion=entropy, min_samples_split=6;, score=0.998 total time=   1.0s\n",
      "[CV 1/5; 10/10] START criterion=entropy, min_samples_split=10...................\n",
      "[CV 1/5; 10/10] END criterion=entropy, min_samples_split=10;, score=0.997 total time=   1.0s\n",
      "[CV 2/5; 1/10] START criterion=gini, min_samples_split=2........................\n",
      "[CV 2/5; 1/10] END criterion=gini, min_samples_split=2;, score=0.998 total time=   1.8s\n",
      "[CV 1/5; 3/10] START criterion=gini, min_samples_split=6........................\n",
      "[CV 1/5; 3/10] END criterion=gini, min_samples_split=6;, score=0.997 total time=   1.9s\n",
      "[CV 3/5; 4/10] START criterion=gini, min_samples_split=8........................\n",
      "[CV 3/5; 4/10] END criterion=gini, min_samples_split=8;, score=0.997 total time=   1.6s\n",
      "[CV 1/5; 6/10] START criterion=entropy, min_samples_split=2.....................\n",
      "[CV 1/5; 6/10] END criterion=entropy, min_samples_split=2;, score=0.998 total time=   1.0s\n",
      "[CV 3/5; 7/10] START criterion=entropy, min_samples_split=4.....................\n",
      "[CV 3/5; 7/10] END criterion=entropy, min_samples_split=4;, score=0.998 total time=   1.0s\n",
      "[CV 1/5; 9/10] START criterion=entropy, min_samples_split=8.....................\n",
      "[CV 1/5; 9/10] END criterion=entropy, min_samples_split=8;, score=0.997 total time=   1.0s\n",
      "[CV 4/5; 10/10] START criterion=entropy, min_samples_split=10...................\n",
      "[CV 4/5; 10/10] END criterion=entropy, min_samples_split=10;, score=0.998 total time=   0.7s\n",
      "[CV 5/5; 1/30] START criterion=gini, min_samples_split=2, n_estimators=200......\n",
      "[CV 5/5; 1/30] END criterion=gini, min_samples_split=2, n_estimators=200;, score=0.999 total time=  28.0s\n",
      "[CV 3/5; 3/30] START criterion=gini, min_samples_split=2, n_estimators=600......\n",
      "[CV 3/5; 3/30] END criterion=gini, min_samples_split=2, n_estimators=600;, score=0.999 total time= 1.5min\n",
      "[CV 5/5; 4/30] START criterion=gini, min_samples_split=2, n_estimators=800......\n",
      "[CV 5/5; 4/30] END criterion=gini, min_samples_split=2, n_estimators=800;, score=0.999 total time= 1.9min\n",
      "[CV 4/5; 6/30] START criterion=gini, min_samples_split=4, n_estimators=200......\n",
      "[CV 4/5; 6/30] END criterion=gini, min_samples_split=4, n_estimators=200;, score=0.999 total time=  32.6s\n",
      "[CV 3/5; 7/30] START criterion=gini, min_samples_split=4, n_estimators=400......\n",
      "[CV 3/5; 7/30] END criterion=gini, min_samples_split=4, n_estimators=400;, score=0.999 total time=  55.5s\n",
      "[CV 5/5; 8/30] START criterion=gini, min_samples_split=4, n_estimators=600......\n",
      "[CV 5/5; 8/30] END criterion=gini, min_samples_split=4, n_estimators=600;, score=0.999 total time= 1.4min\n",
      "[CV 3/5; 10/30] START criterion=gini, min_samples_split=4, n_estimators=1000....\n",
      "[CV 3/5; 10/30] END criterion=gini, min_samples_split=4, n_estimators=1000;, score=0.999 total time= 2.3min\n",
      "[CV 5/5; 12/30] START criterion=gini, min_samples_split=6, n_estimators=400.....\n",
      "[CV 5/5; 12/30] END criterion=gini, min_samples_split=6, n_estimators=400;, score=0.999 total time=  55.9s\n",
      "[CV 3/5; 14/30] START criterion=gini, min_samples_split=6, n_estimators=800.....\n",
      "[CV 3/5; 14/30] END criterion=gini, min_samples_split=6, n_estimators=800;, score=0.999 total time= 2.1min\n",
      "[CV 1/5; 16/30] START criterion=entropy, min_samples_split=2, n_estimators=200..\n",
      "[CV 1/5; 16/30] END criterion=entropy, min_samples_split=2, n_estimators=200;, score=0.999 total time=  29.0s\n",
      "[CV 2/5; 16/30] START criterion=entropy, min_samples_split=2, n_estimators=200..\n",
      "[CV 2/5; 16/30] END criterion=entropy, min_samples_split=2, n_estimators=200;, score=0.999 total time=  25.8s\n",
      "[CV 4/5; 16/30] START criterion=entropy, min_samples_split=2, n_estimators=200..\n",
      "[CV 4/5; 16/30] END criterion=entropy, min_samples_split=2, n_estimators=200;, score=0.999 total time=  27.0s\n",
      "[CV 2/5; 17/30] START criterion=entropy, min_samples_split=2, n_estimators=400..\n",
      "[CV 2/5; 17/30] END criterion=entropy, min_samples_split=2, n_estimators=400;, score=0.999 total time=  48.0s\n",
      "[CV 3/5; 18/30] START criterion=entropy, min_samples_split=2, n_estimators=600..\n",
      "[CV 3/5; 18/30] END criterion=entropy, min_samples_split=2, n_estimators=600;, score=0.999 total time= 1.3min\n",
      "[CV 1/5; 20/30] START criterion=entropy, min_samples_split=2, n_estimators=1000.\n",
      "[CV 1/5; 20/30] END criterion=entropy, min_samples_split=2, n_estimators=1000;, score=0.999 total time= 2.1min\n",
      "[CV 3/5; 22/30] START criterion=entropy, min_samples_split=4, n_estimators=400..\n",
      "[CV 3/5; 22/30] END criterion=entropy, min_samples_split=4, n_estimators=400;, score=0.999 total time=  50.2s\n",
      "[CV 5/5; 23/30] START criterion=entropy, min_samples_split=4, n_estimators=600..\n",
      "[CV 5/5; 23/30] END criterion=entropy, min_samples_split=4, n_estimators=600;, score=0.999 total time= 1.3min\n",
      "[CV 3/5; 25/30] START criterion=entropy, min_samples_split=4, n_estimators=1000.\n",
      "[CV 3/5; 25/30] END criterion=entropy, min_samples_split=4, n_estimators=1000;, score=0.999 total time= 2.2min\n",
      "[CV 1/5; 28/30] START criterion=entropy, min_samples_split=6, n_estimators=600..\n",
      "[CV 1/5; 28/30] END criterion=entropy, min_samples_split=6, n_estimators=600;, score=0.999 total time= 1.3min\n",
      "[CV 4/5; 29/30] START criterion=entropy, min_samples_split=6, n_estimators=800..\n",
      "[CV 4/5; 29/30] END criterion=entropy, min_samples_split=6, n_estimators=800;, score=0.999 total time= 1.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 2/10] START criterion=gini, min_samples_split=4........................\n",
      "[CV 3/5; 2/10] END criterion=gini, min_samples_split=4;, score=0.997 total time=   1.9s\n",
      "[CV 4/5; 3/10] START criterion=gini, min_samples_split=6........................\n",
      "[CV 4/5; 3/10] END criterion=gini, min_samples_split=6;, score=0.998 total time=   1.4s\n",
      "[CV 2/5; 5/10] START criterion=gini, min_samples_split=10.......................\n",
      "[CV 2/5; 5/10] END criterion=gini, min_samples_split=10;, score=0.997 total time=   1.5s\n",
      "[CV 5/5; 6/10] START criterion=entropy, min_samples_split=2.....................\n",
      "[CV 5/5; 6/10] END criterion=entropy, min_samples_split=2;, score=0.998 total time=   1.1s\n",
      "[CV 2/5; 8/10] START criterion=entropy, min_samples_split=6.....................\n",
      "[CV 2/5; 8/10] END criterion=entropy, min_samples_split=6;, score=0.997 total time=   1.1s\n",
      "[CV 5/5; 9/10] START criterion=entropy, min_samples_split=8.....................\n",
      "[CV 5/5; 9/10] END criterion=entropy, min_samples_split=8;, score=0.997 total time=   1.0s\n",
      "[CV 3/5; 1/10] START criterion=gini, min_samples_split=2........................\n",
      "[CV 3/5; 1/10] END criterion=gini, min_samples_split=2;, score=0.997 total time=   1.7s\n",
      "[CV 3/5; 3/10] START criterion=gini, min_samples_split=6........................\n",
      "[CV 3/5; 3/10] END criterion=gini, min_samples_split=6;, score=0.998 total time=   1.7s\n",
      "[CV 2/5; 5/10] START criterion=gini, min_samples_split=10.......................\n",
      "[CV 2/5; 5/10] END criterion=gini, min_samples_split=10;, score=0.998 total time=   1.5s\n",
      "[CV 5/5; 6/10] START criterion=entropy, min_samples_split=2.....................\n",
      "[CV 5/5; 6/10] END criterion=entropy, min_samples_split=2;, score=0.998 total time=   1.0s\n",
      "[CV 2/5; 8/10] START criterion=entropy, min_samples_split=6.....................\n",
      "[CV 2/5; 8/10] END criterion=entropy, min_samples_split=6;, score=0.997 total time=   1.0s\n",
      "[CV 5/5; 9/10] START criterion=entropy, min_samples_split=8.....................\n",
      "[CV 5/5; 9/10] END criterion=entropy, min_samples_split=8;, score=0.998 total time=   1.0s\n",
      "[CV 3/5; 1/10] START criterion=gini, min_samples_split=2........................\n",
      "[CV 3/5; 1/10] END criterion=gini, min_samples_split=2;, score=0.997 total time=   1.8s\n",
      "[CV 4/5; 3/10] START criterion=gini, min_samples_split=6........................\n",
      "[CV 4/5; 3/10] END criterion=gini, min_samples_split=6;, score=0.998 total time=   2.0s\n",
      "[CV 2/5; 5/10] START criterion=gini, min_samples_split=10.......................\n",
      "[CV 2/5; 5/10] END criterion=gini, min_samples_split=10;, score=0.997 total time=   1.6s\n",
      "[CV 5/5; 6/10] START criterion=entropy, min_samples_split=2.....................\n",
      "[CV 5/5; 6/10] END criterion=entropy, min_samples_split=2;, score=0.998 total time=   1.0s\n",
      "[CV 2/5; 8/10] START criterion=entropy, min_samples_split=6.....................\n",
      "[CV 2/5; 8/10] END criterion=entropy, min_samples_split=6;, score=0.997 total time=   1.0s\n",
      "[CV 5/5; 9/10] START criterion=entropy, min_samples_split=8.....................\n",
      "[CV 5/5; 9/10] END criterion=entropy, min_samples_split=8;, score=0.998 total time=   1.0s\n",
      "[CV 1/5; 2/30] START criterion=gini, min_samples_split=2, n_estimators=400......\n",
      "[CV 1/5; 2/30] END criterion=gini, min_samples_split=2, n_estimators=400;, score=0.999 total time=  56.8s\n",
      "[CV 4/5; 3/30] START criterion=gini, min_samples_split=2, n_estimators=600......\n",
      "[CV 4/5; 3/30] END criterion=gini, min_samples_split=2, n_estimators=600;, score=0.999 total time= 1.5min\n",
      "[CV 3/5; 5/30] START criterion=gini, min_samples_split=2, n_estimators=1000.....\n",
      "[CV 3/5; 5/30] END criterion=gini, min_samples_split=2, n_estimators=1000;, score=0.999 total time= 2.4min\n",
      "[CV 5/5; 7/30] START criterion=gini, min_samples_split=4, n_estimators=400......\n",
      "[CV 5/5; 7/30] END criterion=gini, min_samples_split=4, n_estimators=400;, score=0.999 total time=  54.9s\n",
      "[CV 2/5; 9/30] START criterion=gini, min_samples_split=4, n_estimators=800......\n",
      "[CV 2/5; 9/30] END criterion=gini, min_samples_split=4, n_estimators=800;, score=0.999 total time= 1.8min\n",
      "[CV 5/5; 10/30] START criterion=gini, min_samples_split=4, n_estimators=1000....\n",
      "[CV 5/5; 10/30] END criterion=gini, min_samples_split=4, n_estimators=1000;, score=0.999 total time= 2.3min\n",
      "[CV 1/5; 14/30] START criterion=gini, min_samples_split=6, n_estimators=800.....\n",
      "[CV 1/5; 14/30] END criterion=gini, min_samples_split=6, n_estimators=800;, score=0.999 total time= 2.0min\n",
      "[CV 4/5; 15/30] START criterion=gini, min_samples_split=6, n_estimators=1000....\n",
      "[CV 4/5; 15/30] END criterion=gini, min_samples_split=6, n_estimators=1000;, score=0.999 total time= 2.4min\n",
      "[CV 5/5; 18/30] START criterion=entropy, min_samples_split=2, n_estimators=600..\n",
      "[CV 5/5; 18/30] END criterion=entropy, min_samples_split=2, n_estimators=600;, score=0.999 total time= 1.3min\n",
      "[CV 3/5; 20/30] START criterion=entropy, min_samples_split=2, n_estimators=1000.\n",
      "[CV 3/5; 20/30] END criterion=entropy, min_samples_split=2, n_estimators=1000;, score=0.999 total time= 2.1min\n",
      "[CV 1/5; 23/30] START criterion=entropy, min_samples_split=4, n_estimators=600..\n",
      "[CV 1/5; 23/30] END criterion=entropy, min_samples_split=4, n_estimators=600;, score=0.999 total time= 1.3min\n",
      "[CV 4/5; 24/30] START criterion=entropy, min_samples_split=4, n_estimators=800..\n",
      "[CV 4/5; 24/30] END criterion=entropy, min_samples_split=4, n_estimators=800;, score=0.999 total time= 1.7min\n",
      "[CV 3/5; 26/30] START criterion=entropy, min_samples_split=6, n_estimators=200..\n",
      "[CV 3/5; 26/30] END criterion=entropy, min_samples_split=6, n_estimators=200;, score=0.999 total time=  26.6s\n",
      "[CV 1/5; 27/30] START criterion=entropy, min_samples_split=6, n_estimators=400..\n",
      "[CV 1/5; 27/30] END criterion=entropy, min_samples_split=6, n_estimators=400;, score=0.999 total time=  52.1s\n",
      "[CV 2/5; 28/30] START criterion=entropy, min_samples_split=6, n_estimators=600..\n",
      "[CV 2/5; 28/30] END criterion=entropy, min_samples_split=6, n_estimators=600;, score=0.999 total time= 1.3min\n",
      "[CV 5/5; 29/30] START criterion=entropy, min_samples_split=6, n_estimators=800..\n",
      "[CV 5/5; 29/30] END criterion=entropy, min_samples_split=6, n_estimators=800;, score=0.999 total time= 1.6min\n",
      "[CV 2/5; 1/10] START criterion=gini, min_samples_split=2........................\n",
      "[CV 2/5; 1/10] END criterion=gini, min_samples_split=2;, score=0.998 total time=   1.9s\n",
      "[CV 3/5; 3/10] START criterion=gini, min_samples_split=6........................\n",
      "[CV 3/5; 3/10] END criterion=gini, min_samples_split=6;, score=0.997 total time=   1.5s\n",
      "[CV 4/5; 5/10] START criterion=gini, min_samples_split=10.......................\n",
      "[CV 4/5; 5/10] END criterion=gini, min_samples_split=10;, score=0.998 total time=   1.5s\n",
      "[CV 1/5; 7/10] START criterion=entropy, min_samples_split=4.....................\n",
      "[CV 1/5; 7/10] END criterion=entropy, min_samples_split=4;, score=0.997 total time=   1.1s\n",
      "[CV 3/5; 8/10] START criterion=entropy, min_samples_split=6.....................\n",
      "[CV 3/5; 8/10] END criterion=entropy, min_samples_split=6;, score=0.998 total time=   1.1s\n",
      "[CV 1/5; 10/10] START criterion=entropy, min_samples_split=10...................\n",
      "[CV 1/5; 10/10] END criterion=entropy, min_samples_split=10;, score=0.997 total time=   1.0s\n",
      "[CV 4/5; 1/10] START criterion=gini, min_samples_split=2........................\n",
      "[CV 4/5; 1/10] END criterion=gini, min_samples_split=2;, score=0.998 total time=   1.6s\n",
      "[CV 1/5; 3/10] START criterion=gini, min_samples_split=6........................\n",
      "[CV 1/5; 3/10] END criterion=gini, min_samples_split=6;, score=0.997 total time=   1.5s\n",
      "[CV 3/5; 4/10] START criterion=gini, min_samples_split=8........................\n",
      "[CV 3/5; 4/10] END criterion=gini, min_samples_split=8;, score=0.997 total time=   1.5s\n",
      "[CV 3/5; 6/10] START criterion=entropy, min_samples_split=2.....................\n",
      "[CV 3/5; 6/10] END criterion=entropy, min_samples_split=2;, score=0.998 total time=   1.0s\n",
      "[CV 5/5; 7/10] START criterion=entropy, min_samples_split=4.....................\n",
      "[CV 5/5; 7/10] END criterion=entropy, min_samples_split=4;, score=0.998 total time=   1.0s\n",
      "[CV 3/5; 9/10] START criterion=entropy, min_samples_split=8.....................\n",
      "[CV 3/5; 9/10] END criterion=entropy, min_samples_split=8;, score=0.998 total time=   1.0s\n",
      "[CV 1/5; 1/10] START criterion=gini, min_samples_split=2........................\n",
      "[CV 1/5; 1/10] END criterion=gini, min_samples_split=2;, score=0.998 total time=   1.6s\n",
      "[CV 4/5; 2/10] START criterion=gini, min_samples_split=4........................\n",
      "[CV 4/5; 2/10] END criterion=gini, min_samples_split=4;, score=0.998 total time=   1.9s\n",
      "[CV 2/5; 4/10] START criterion=gini, min_samples_split=8........................\n",
      "[CV 2/5; 4/10] END criterion=gini, min_samples_split=8;, score=0.997 total time=   1.5s\n",
      "[CV 5/5; 5/10] START criterion=gini, min_samples_split=10.......................\n",
      "[CV 5/5; 5/10] END criterion=gini, min_samples_split=10;, score=0.998 total time=   1.5s\n",
      "[CV 5/5; 8/10] START criterion=entropy, min_samples_split=6.....................\n",
      "[CV 5/5; 8/10] END criterion=entropy, min_samples_split=6;, score=0.998 total time=   1.0s\n",
      "[CV 3/5; 10/10] START criterion=entropy, min_samples_split=10...................\n",
      "[CV 3/5; 10/10] END criterion=entropy, min_samples_split=10;, score=0.998 total time=   0.9s\n",
      "[CV 2/5; 1/30] START criterion=gini, min_samples_split=2, n_estimators=200......\n",
      "[CV 2/5; 1/30] END criterion=gini, min_samples_split=2, n_estimators=200;, score=0.999 total time=  27.3s\n",
      "[CV 5/5; 2/30] START criterion=gini, min_samples_split=2, n_estimators=400......\n",
      "[CV 5/5; 2/30] END criterion=gini, min_samples_split=2, n_estimators=400;, score=0.999 total time= 1.0min\n",
      "[CV 3/5; 4/30] START criterion=gini, min_samples_split=2, n_estimators=800......\n",
      "[CV 3/5; 4/30] END criterion=gini, min_samples_split=2, n_estimators=800;, score=0.999 total time= 1.9min\n",
      "[CV 1/5; 6/30] START criterion=gini, min_samples_split=4, n_estimators=200......\n",
      "[CV 1/5; 6/30] END criterion=gini, min_samples_split=4, n_estimators=200;, score=0.999 total time=  30.4s\n",
      "[CV 3/5; 6/30] START criterion=gini, min_samples_split=4, n_estimators=200......\n",
      "[CV 3/5; 6/30] END criterion=gini, min_samples_split=4, n_estimators=200;, score=0.999 total time=  33.2s\n",
      "[CV 2/5; 7/30] START criterion=gini, min_samples_split=4, n_estimators=400......\n",
      "[CV 2/5; 7/30] END criterion=gini, min_samples_split=4, n_estimators=400;, score=0.999 total time=  56.2s\n",
      "[CV 4/5; 8/30] START criterion=gini, min_samples_split=4, n_estimators=600......\n",
      "[CV 4/5; 8/30] END criterion=gini, min_samples_split=4, n_estimators=600;, score=0.999 total time= 1.3min\n",
      "[CV 2/5; 10/30] START criterion=gini, min_samples_split=4, n_estimators=1000....\n",
      "[CV 2/5; 10/30] END criterion=gini, min_samples_split=4, n_estimators=1000;, score=0.999 total time= 2.3min\n",
      "[CV 1/5; 13/30] START criterion=gini, min_samples_split=6, n_estimators=600.....\n",
      "[CV 1/5; 13/30] END criterion=gini, min_samples_split=6, n_estimators=600;, score=0.999 total time= 1.4min\n",
      "[CV 4/5; 14/30] START criterion=gini, min_samples_split=6, n_estimators=800.....\n",
      "[CV 4/5; 14/30] END criterion=gini, min_samples_split=6, n_estimators=800;, score=0.999 total time= 2.2min\n",
      "[CV 3/5; 16/30] START criterion=entropy, min_samples_split=2, n_estimators=200..\n",
      "[CV 3/5; 16/30] END criterion=entropy, min_samples_split=2, n_estimators=200;, score=0.999 total time=  26.1s\n",
      "[CV 5/5; 16/30] START criterion=entropy, min_samples_split=2, n_estimators=200..\n",
      "[CV 5/5; 16/30] END criterion=entropy, min_samples_split=2, n_estimators=200;, score=0.999 total time=  26.7s\n",
      "[CV 3/5; 17/30] START criterion=entropy, min_samples_split=2, n_estimators=400..\n",
      "[CV 3/5; 17/30] END criterion=entropy, min_samples_split=2, n_estimators=400;, score=0.999 total time=  47.8s\n",
      "[CV 4/5; 18/30] START criterion=entropy, min_samples_split=2, n_estimators=600..\n",
      "[CV 4/5; 18/30] END criterion=entropy, min_samples_split=2, n_estimators=600;, score=0.999 total time= 1.3min\n",
      "[CV 2/5; 20/30] START criterion=entropy, min_samples_split=2, n_estimators=1000.\n",
      "[CV 2/5; 20/30] END criterion=entropy, min_samples_split=2, n_estimators=1000;, score=0.999 total time= 2.1min\n",
      "[CV 4/5; 22/30] START criterion=entropy, min_samples_split=4, n_estimators=400..\n",
      "[CV 4/5; 22/30] END criterion=entropy, min_samples_split=4, n_estimators=400;, score=0.999 total time=  50.3s\n",
      "[CV 1/5; 24/30] START criterion=entropy, min_samples_split=4, n_estimators=800..\n",
      "[CV 1/5; 24/30] END criterion=entropy, min_samples_split=4, n_estimators=800;, score=0.999 total time= 1.7min\n",
      "[CV 4/5; 25/30] START criterion=entropy, min_samples_split=4, n_estimators=1000.\n",
      "[CV 4/5; 25/30] END criterion=entropy, min_samples_split=4, n_estimators=1000;, score=0.999 total time= 2.2min\n",
      "[CV 3/5; 28/30] START criterion=entropy, min_samples_split=6, n_estimators=600..\n",
      "[CV 3/5; 28/30] END criterion=entropy, min_samples_split=6, n_estimators=600;, score=0.999 total time= 1.3min\n",
      "[CV 1/5; 30/30] START criterion=entropy, min_samples_split=6, n_estimators=1000.\n",
      "[CV 1/5; 30/30] END criterion=entropy, min_samples_split=6, n_estimators=1000;, score=0.999 total time= 1.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/10] START criterion=gini, min_samples_split=2........................\n",
      "[CV 3/5; 1/10] END criterion=gini, min_samples_split=2;, score=0.997 total time=   1.9s\n",
      "[CV 1/5; 4/10] START criterion=gini, min_samples_split=8........................\n",
      "[CV 1/5; 4/10] END criterion=gini, min_samples_split=8;, score=0.997 total time=   1.3s\n",
      "[CV 5/5; 4/10] START criterion=gini, min_samples_split=8........................\n",
      "[CV 5/5; 4/10] END criterion=gini, min_samples_split=8;, score=0.997 total time=   1.5s\n",
      "[CV 4/5; 6/10] START criterion=entropy, min_samples_split=2.....................\n",
      "[CV 4/5; 6/10] END criterion=entropy, min_samples_split=2;, score=0.998 total time=   1.1s\n",
      "[CV 1/5; 8/10] START criterion=entropy, min_samples_split=6.....................\n",
      "[CV 1/5; 8/10] END criterion=entropy, min_samples_split=6;, score=0.998 total time=   1.1s\n",
      "[CV 3/5; 9/10] START criterion=entropy, min_samples_split=8.....................\n",
      "[CV 3/5; 9/10] END criterion=entropy, min_samples_split=8;, score=0.998 total time=   1.0s\n",
      "[CV 5/5; 1/10] START criterion=gini, min_samples_split=2........................\n",
      "[CV 5/5; 1/10] END criterion=gini, min_samples_split=2;, score=0.998 total time=   1.7s\n",
      "[CV 4/5; 3/10] START criterion=gini, min_samples_split=6........................\n",
      "[CV 4/5; 3/10] END criterion=gini, min_samples_split=6;, score=0.998 total time=   1.5s\n",
      "[CV 5/5; 4/10] START criterion=gini, min_samples_split=8........................\n",
      "[CV 5/5; 4/10] END criterion=gini, min_samples_split=8;, score=0.997 total time=   1.4s\n",
      "[CV 4/5; 6/10] START criterion=entropy, min_samples_split=2.....................\n",
      "[CV 4/5; 6/10] END criterion=entropy, min_samples_split=2;, score=0.998 total time=   1.0s\n",
      "[CV 1/5; 8/10] START criterion=entropy, min_samples_split=6.....................\n",
      "[CV 1/5; 8/10] END criterion=entropy, min_samples_split=6;, score=0.998 total time=   1.0s\n",
      "[CV 4/5; 9/10] START criterion=entropy, min_samples_split=8.....................\n",
      "[CV 4/5; 9/10] END criterion=entropy, min_samples_split=8;, score=0.998 total time=   1.0s\n",
      "[CV 3/5; 2/10] START criterion=gini, min_samples_split=4........................\n",
      "[CV 3/5; 2/10] END criterion=gini, min_samples_split=4;, score=0.997 total time=   1.9s\n",
      "[CV 1/5; 4/10] START criterion=gini, min_samples_split=8........................\n",
      "[CV 1/5; 4/10] END criterion=gini, min_samples_split=8;, score=0.997 total time=   1.9s\n",
      "[CV 1/5; 5/10] START criterion=gini, min_samples_split=10.......................\n",
      "[CV 1/5; 5/10] END criterion=gini, min_samples_split=10;, score=0.997 total time=   1.5s\n",
      "[CV 3/5; 6/10] START criterion=entropy, min_samples_split=2.....................\n",
      "[CV 3/5; 6/10] END criterion=entropy, min_samples_split=2;, score=0.998 total time=   1.0s\n",
      "[CV 5/5; 7/10] START criterion=entropy, min_samples_split=4.....................\n",
      "[CV 5/5; 7/10] END criterion=entropy, min_samples_split=4;, score=0.998 total time=   1.0s\n",
      "[CV 3/5; 9/10] START criterion=entropy, min_samples_split=8.....................\n",
      "[CV 3/5; 9/10] END criterion=entropy, min_samples_split=8;, score=0.998 total time=   1.0s\n",
      "[CV 1/5; 1/30] START criterion=gini, min_samples_split=2, n_estimators=200......\n",
      "[CV 1/5; 1/30] END criterion=gini, min_samples_split=2, n_estimators=200;, score=0.999 total time=  27.2s\n",
      "[CV 4/5; 2/30] START criterion=gini, min_samples_split=2, n_estimators=400......\n",
      "[CV 4/5; 2/30] END criterion=gini, min_samples_split=2, n_estimators=400;, score=0.999 total time= 1.0min\n",
      "[CV 2/5; 4/30] START criterion=gini, min_samples_split=2, n_estimators=800......\n",
      "[CV 2/5; 4/30] END criterion=gini, min_samples_split=2, n_estimators=800;, score=0.999 total time= 1.9min\n",
      "[CV 5/5; 5/30] START criterion=gini, min_samples_split=2, n_estimators=1000.....\n",
      "[CV 5/5; 5/30] END criterion=gini, min_samples_split=2, n_estimators=1000;, score=0.999 total time= 2.4min\n",
      "[CV 3/5; 9/30] START criterion=gini, min_samples_split=4, n_estimators=800......\n",
      "[CV 3/5; 9/30] END criterion=gini, min_samples_split=4, n_estimators=800;, score=0.999 total time= 1.8min\n",
      "[CV 1/5; 11/30] START criterion=gini, min_samples_split=6, n_estimators=200.....\n",
      "[CV 1/5; 11/30] END criterion=gini, min_samples_split=6, n_estimators=200;, score=0.999 total time=  27.1s\n",
      "[CV 3/5; 11/30] START criterion=gini, min_samples_split=6, n_estimators=200.....\n",
      "[CV 3/5; 11/30] END criterion=gini, min_samples_split=6, n_estimators=200;, score=0.999 total time=  26.5s\n",
      "[CV 1/5; 12/30] START criterion=gini, min_samples_split=6, n_estimators=400.....\n",
      "[CV 1/5; 12/30] END criterion=gini, min_samples_split=6, n_estimators=400;, score=0.999 total time=  54.3s\n",
      "[CV 2/5; 13/30] START criterion=gini, min_samples_split=6, n_estimators=600.....\n",
      "[CV 2/5; 13/30] END criterion=gini, min_samples_split=6, n_estimators=600;, score=0.999 total time= 1.4min\n",
      "[CV 5/5; 14/30] START criterion=gini, min_samples_split=6, n_estimators=800.....\n",
      "[CV 5/5; 14/30] END criterion=gini, min_samples_split=6, n_estimators=800;, score=0.999 total time= 2.2min\n",
      "[CV 1/5; 17/30] START criterion=entropy, min_samples_split=2, n_estimators=400..\n",
      "[CV 1/5; 17/30] END criterion=entropy, min_samples_split=2, n_estimators=400;, score=0.999 total time=  49.2s\n",
      "[CV 4/5; 17/30] START criterion=entropy, min_samples_split=2, n_estimators=400..\n",
      "[CV 4/5; 17/30] END criterion=entropy, min_samples_split=2, n_estimators=400;, score=0.999 total time=  49.7s\n",
      "[CV 2/5; 19/30] START criterion=entropy, min_samples_split=2, n_estimators=800..\n",
      "[CV 2/5; 19/30] END criterion=entropy, min_samples_split=2, n_estimators=800;, score=0.999 total time= 1.7min\n",
      "[CV 5/5; 20/30] START criterion=entropy, min_samples_split=2, n_estimators=1000.\n",
      "[CV 5/5; 20/30] END criterion=entropy, min_samples_split=2, n_estimators=1000;, score=0.999 total time= 2.1min\n",
      "[CV 2/5; 24/30] START criterion=entropy, min_samples_split=4, n_estimators=800..\n",
      "[CV 2/5; 24/30] END criterion=entropy, min_samples_split=4, n_estimators=800;, score=0.999 total time= 1.7min\n",
      "[CV 5/5; 25/30] START criterion=entropy, min_samples_split=4, n_estimators=1000.\n",
      "[CV 5/5; 25/30] END criterion=entropy, min_samples_split=4, n_estimators=1000;, score=0.999 total time= 2.2min\n",
      "[CV 4/5; 28/30] START criterion=entropy, min_samples_split=6, n_estimators=600..\n",
      "[CV 4/5; 28/30] END criterion=entropy, min_samples_split=6, n_estimators=600;, score=0.999 total time= 1.3min\n",
      "[CV 2/5; 30/30] START criterion=entropy, min_samples_split=6, n_estimators=1000.\n",
      "[CV 2/5; 30/30] END criterion=entropy, min_samples_split=6, n_estimators=1000;, score=0.999 total time= 1.7min\n",
      "[CV 5/5; 1/10] START criterion=gini, min_samples_split=2........................\n",
      "[CV 5/5; 1/10] END criterion=gini, min_samples_split=2;, score=0.998 total time=   1.9s\n",
      "[CV 5/5; 3/10] START criterion=gini, min_samples_split=6........................\n",
      "[CV 5/5; 3/10] END criterion=gini, min_samples_split=6;, score=0.998 total time=   1.4s\n",
      "[CV 3/5; 5/10] START criterion=gini, min_samples_split=10.......................\n",
      "[CV 3/5; 5/10] END criterion=gini, min_samples_split=10;, score=0.997 total time=   1.6s\n",
      "[CV 2/5; 7/10] START criterion=entropy, min_samples_split=4.....................\n",
      "[CV 2/5; 7/10] END criterion=entropy, min_samples_split=4;, score=0.998 total time=   1.1s\n",
      "[CV 4/5; 8/10] START criterion=entropy, min_samples_split=6.....................\n",
      "[CV 4/5; 8/10] END criterion=entropy, min_samples_split=6;, score=0.998 total time=   1.1s\n",
      "[CV 2/5; 10/10] START criterion=entropy, min_samples_split=10...................\n",
      "[CV 2/5; 10/10] END criterion=entropy, min_samples_split=10;, score=0.997 total time=   1.0s\n",
      "[CV 2/5; 1/10] START criterion=gini, min_samples_split=2........................\n",
      "[CV 2/5; 1/10] END criterion=gini, min_samples_split=2;, score=0.998 total time=   1.5s\n",
      "[CV 5/5; 2/10] START criterion=gini, min_samples_split=4........................\n",
      "[CV 5/5; 2/10] END criterion=gini, min_samples_split=4;, score=0.997 total time=   1.6s\n",
      "[CV 4/5; 4/10] START criterion=gini, min_samples_split=8........................\n",
      "[CV 4/5; 4/10] END criterion=gini, min_samples_split=8;, score=0.998 total time=   1.4s\n",
      "[CV 1/5; 6/10] START criterion=entropy, min_samples_split=2.....................\n",
      "[CV 1/5; 6/10] END criterion=entropy, min_samples_split=2;, score=0.997 total time=   1.0s\n",
      "[CV 4/5; 7/10] START criterion=entropy, min_samples_split=4.....................\n",
      "[CV 4/5; 7/10] END criterion=entropy, min_samples_split=4;, score=0.998 total time=   1.0s\n",
      "[CV 2/5; 9/10] START criterion=entropy, min_samples_split=8.....................\n",
      "[CV 2/5; 9/10] END criterion=entropy, min_samples_split=8;, score=0.997 total time=   1.0s\n",
      "[CV 5/5; 10/10] START criterion=entropy, min_samples_split=10...................\n",
      "[CV 5/5; 10/10] END criterion=entropy, min_samples_split=10;, score=0.998 total time=   0.7s\n",
      "[CV 5/5; 1/10] START criterion=gini, min_samples_split=2........................\n",
      "[CV 5/5; 1/10] END criterion=gini, min_samples_split=2;, score=0.998 total time=   1.8s\n",
      "[CV 3/5; 3/10] START criterion=gini, min_samples_split=6........................\n",
      "[CV 3/5; 3/10] END criterion=gini, min_samples_split=6;, score=0.997 total time=   2.0s\n",
      "[CV 3/5; 5/10] START criterion=gini, min_samples_split=10.......................\n",
      "[CV 3/5; 5/10] END criterion=gini, min_samples_split=10;, score=0.997 total time=   1.6s\n",
      "[CV 2/5; 7/10] START criterion=entropy, min_samples_split=4.....................\n",
      "[CV 2/5; 7/10] END criterion=entropy, min_samples_split=4;, score=0.998 total time=   1.0s\n",
      "[CV 4/5; 8/10] START criterion=entropy, min_samples_split=6.....................\n",
      "[CV 4/5; 8/10] END criterion=entropy, min_samples_split=6;, score=0.998 total time=   1.0s\n",
      "[CV 2/5; 10/10] START criterion=entropy, min_samples_split=10...................\n",
      "[CV 2/5; 10/10] END criterion=entropy, min_samples_split=10;, score=0.997 total time=   1.0s\n",
      "[CV 2/5; 2/30] START criterion=gini, min_samples_split=2, n_estimators=400......\n",
      "[CV 2/5; 2/30] END criterion=gini, min_samples_split=2, n_estimators=400;, score=0.999 total time=  58.0s\n",
      "[CV 1/5; 4/30] START criterion=gini, min_samples_split=2, n_estimators=800......\n",
      "[CV 1/5; 4/30] END criterion=gini, min_samples_split=2, n_estimators=800;, score=0.999 total time= 1.9min\n",
      "[CV 4/5; 5/30] START criterion=gini, min_samples_split=2, n_estimators=1000.....\n",
      "[CV 4/5; 5/30] END criterion=gini, min_samples_split=2, n_estimators=1000;, score=0.999 total time= 2.4min\n",
      "[CV 2/5; 8/30] START criterion=gini, min_samples_split=4, n_estimators=600......\n",
      "[CV 2/5; 8/30] END criterion=gini, min_samples_split=4, n_estimators=600;, score=0.999 total time= 1.3min\n",
      "[CV 5/5; 9/30] START criterion=gini, min_samples_split=4, n_estimators=800......\n",
      "[CV 5/5; 9/30] END criterion=gini, min_samples_split=4, n_estimators=800;, score=0.999 total time= 1.8min\n",
      "[CV 5/5; 11/30] START criterion=gini, min_samples_split=6, n_estimators=200.....\n",
      "[CV 5/5; 11/30] END criterion=gini, min_samples_split=6, n_estimators=200;, score=0.999 total time=  26.8s\n",
      "[CV 4/5; 12/30] START criterion=gini, min_samples_split=6, n_estimators=400.....\n",
      "[CV 4/5; 12/30] END criterion=gini, min_samples_split=6, n_estimators=400;, score=0.999 total time=  54.4s\n",
      "[CV 5/5; 13/30] START criterion=gini, min_samples_split=6, n_estimators=600.....\n",
      "[CV 5/5; 13/30] END criterion=gini, min_samples_split=6, n_estimators=600;, score=0.999 total time= 1.5min\n",
      "[CV 3/5; 15/30] START criterion=gini, min_samples_split=6, n_estimators=1000....\n",
      "[CV 3/5; 15/30] END criterion=gini, min_samples_split=6, n_estimators=1000;, score=0.999 total time= 2.5min\n",
      "[CV 1/5; 18/30] START criterion=entropy, min_samples_split=2, n_estimators=600..\n",
      "[CV 1/5; 18/30] END criterion=entropy, min_samples_split=2, n_estimators=600;, score=0.999 total time= 1.3min\n",
      "[CV 4/5; 19/30] START criterion=entropy, min_samples_split=2, n_estimators=800..\n",
      "[CV 4/5; 19/30] END criterion=entropy, min_samples_split=2, n_estimators=800;, score=0.999 total time= 1.6min\n",
      "[CV 3/5; 21/30] START criterion=entropy, min_samples_split=4, n_estimators=200..\n",
      "[CV 3/5; 21/30] END criterion=entropy, min_samples_split=4, n_estimators=200;, score=0.999 total time=  27.1s\n",
      "[CV 1/5; 22/30] START criterion=entropy, min_samples_split=4, n_estimators=400..\n",
      "[CV 1/5; 22/30] END criterion=entropy, min_samples_split=4, n_estimators=400;, score=0.999 total time=  50.8s\n",
      "[CV 2/5; 23/30] START criterion=entropy, min_samples_split=4, n_estimators=600..\n",
      "[CV 2/5; 23/30] END criterion=entropy, min_samples_split=4, n_estimators=600;, score=0.999 total time= 1.3min\n",
      "[CV 5/5; 24/30] START criterion=entropy, min_samples_split=4, n_estimators=800..\n",
      "[CV 5/5; 24/30] END criterion=entropy, min_samples_split=4, n_estimators=800;, score=0.999 total time= 1.7min\n",
      "[CV 4/5; 26/30] START criterion=entropy, min_samples_split=6, n_estimators=200..\n",
      "[CV 4/5; 26/30] END criterion=entropy, min_samples_split=6, n_estimators=200;, score=0.999 total time=  26.4s\n",
      "[CV 2/5; 27/30] START criterion=entropy, min_samples_split=6, n_estimators=400..\n",
      "[CV 2/5; 27/30] END criterion=entropy, min_samples_split=6, n_estimators=400;, score=0.999 total time=  51.9s\n",
      "[CV 5/5; 28/30] START criterion=entropy, min_samples_split=6, n_estimators=600..\n",
      "[CV 5/5; 28/30] END criterion=entropy, min_samples_split=6, n_estimators=600;, score=0.999 total time= 1.3min\n",
      "[CV 3/5; 30/30] START criterion=entropy, min_samples_split=6, n_estimators=1000.\n",
      "[CV 3/5; 30/30] END criterion=entropy, min_samples_split=6, n_estimators=1000;, score=0.999 total time= 1.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/10] START criterion=gini, min_samples_split=2........................\n",
      "[CV 4/5; 1/10] END criterion=gini, min_samples_split=2;, score=0.998 total time=   1.8s\n",
      "[CV 1/5; 3/10] START criterion=gini, min_samples_split=6........................\n",
      "[CV 1/5; 3/10] END criterion=gini, min_samples_split=6;, score=0.998 total time=   1.3s\n",
      "[CV 3/5; 4/10] START criterion=gini, min_samples_split=8........................\n",
      "[CV 3/5; 4/10] END criterion=gini, min_samples_split=8;, score=0.997 total time=   1.5s\n",
      "[CV 3/5; 6/10] START criterion=entropy, min_samples_split=2.....................\n",
      "[CV 3/5; 6/10] END criterion=entropy, min_samples_split=2;, score=0.998 total time=   1.1s\n",
      "[CV 5/5; 7/10] START criterion=entropy, min_samples_split=4.....................\n",
      "[CV 5/5; 7/10] END criterion=entropy, min_samples_split=4;, score=0.998 total time=   1.1s\n",
      "[CV 2/5; 9/10] START criterion=entropy, min_samples_split=8.....................\n",
      "[CV 2/5; 9/10] END criterion=entropy, min_samples_split=8;, score=0.997 total time=   1.0s\n",
      "[CV 5/5; 10/10] START criterion=entropy, min_samples_split=10...................\n",
      "[CV 5/5; 10/10] END criterion=entropy, min_samples_split=10;, score=0.998 total time=   0.7s\n",
      "[CV 1/5; 2/10] START criterion=gini, min_samples_split=4........................\n",
      "[CV 1/5; 2/10] END criterion=gini, min_samples_split=4;, score=0.998 total time=   1.5s\n",
      "[CV 2/5; 3/10] START criterion=gini, min_samples_split=6........................\n",
      "[CV 2/5; 3/10] END criterion=gini, min_samples_split=6;, score=0.998 total time=   1.6s\n",
      "[CV 1/5; 5/10] START criterion=gini, min_samples_split=10.......................\n",
      "[CV 1/5; 5/10] END criterion=gini, min_samples_split=10;, score=0.997 total time=   1.3s\n",
      "[CV 2/5; 6/10] START criterion=entropy, min_samples_split=2.....................\n",
      "[CV 2/5; 6/10] END criterion=entropy, min_samples_split=2;, score=0.998 total time=   1.0s\n",
      "[CV 3/5; 7/10] START criterion=entropy, min_samples_split=4.....................\n",
      "[CV 3/5; 7/10] END criterion=entropy, min_samples_split=4;, score=0.998 total time=   1.0s\n",
      "[CV 1/5; 9/10] START criterion=entropy, min_samples_split=8.....................\n",
      "[CV 1/5; 9/10] END criterion=entropy, min_samples_split=8;, score=0.997 total time=   1.0s\n",
      "[CV 4/5; 10/10] START criterion=entropy, min_samples_split=10...................\n",
      "[CV 4/5; 10/10] END criterion=entropy, min_samples_split=10;, score=0.998 total time=   0.7s\n",
      "[CV 1/5; 2/10] START criterion=gini, min_samples_split=4........................\n",
      "[CV 1/5; 2/10] END criterion=gini, min_samples_split=4;, score=0.997 total time=   1.7s\n",
      "[CV 5/5; 2/10] START criterion=gini, min_samples_split=4........................\n",
      "[CV 5/5; 2/10] END criterion=gini, min_samples_split=4;, score=0.998 total time=   2.0s\n",
      "[CV 4/5; 4/10] START criterion=gini, min_samples_split=8........................\n",
      "[CV 4/5; 4/10] END criterion=gini, min_samples_split=8;, score=0.998 total time=   1.5s\n",
      "[CV 2/5; 6/10] START criterion=entropy, min_samples_split=2.....................\n",
      "[CV 2/5; 6/10] END criterion=entropy, min_samples_split=2;, score=0.998 total time=   1.0s\n",
      "[CV 4/5; 7/10] START criterion=entropy, min_samples_split=4.....................\n",
      "[CV 4/5; 7/10] END criterion=entropy, min_samples_split=4;, score=0.998 total time=   1.0s\n",
      "[CV 2/5; 9/10] START criterion=entropy, min_samples_split=8.....................\n",
      "[CV 2/5; 9/10] END criterion=entropy, min_samples_split=8;, score=0.997 total time=   1.0s\n",
      "[CV 5/5; 10/10] START criterion=entropy, min_samples_split=10...................\n",
      "[CV 5/5; 10/10] END criterion=entropy, min_samples_split=10;, score=0.998 total time=   0.7s\n",
      "[CV 3/5; 2/30] START criterion=gini, min_samples_split=2, n_estimators=400......\n",
      "[CV 3/5; 2/30] END criterion=gini, min_samples_split=2, n_estimators=400;, score=0.999 total time=  56.8s\n",
      "[CV 5/5; 3/30] START criterion=gini, min_samples_split=2, n_estimators=600......\n",
      "[CV 5/5; 3/30] END criterion=gini, min_samples_split=2, n_estimators=600;, score=0.999 total time= 1.5min\n",
      "[CV 2/5; 5/30] START criterion=gini, min_samples_split=2, n_estimators=1000.....\n",
      "[CV 2/5; 5/30] END criterion=gini, min_samples_split=2, n_estimators=1000;, score=0.999 total time= 2.4min\n",
      "[CV 1/5; 8/30] START criterion=gini, min_samples_split=4, n_estimators=600......\n",
      "[CV 1/5; 8/30] END criterion=gini, min_samples_split=4, n_estimators=600;, score=0.999 total time= 1.4min\n",
      "[CV 4/5; 9/30] START criterion=gini, min_samples_split=4, n_estimators=800......\n",
      "[CV 4/5; 9/30] END criterion=gini, min_samples_split=4, n_estimators=800;, score=0.999 total time= 1.8min\n",
      "[CV 2/5; 11/30] START criterion=gini, min_samples_split=6, n_estimators=200.....\n",
      "[CV 2/5; 11/30] END criterion=gini, min_samples_split=6, n_estimators=200;, score=0.999 total time=  27.3s\n",
      "[CV 4/5; 11/30] START criterion=gini, min_samples_split=6, n_estimators=200.....\n",
      "[CV 4/5; 11/30] END criterion=gini, min_samples_split=6, n_estimators=200;, score=0.999 total time=  26.9s\n",
      "[CV 3/5; 12/30] START criterion=gini, min_samples_split=6, n_estimators=400.....\n",
      "[CV 3/5; 12/30] END criterion=gini, min_samples_split=6, n_estimators=400;, score=0.999 total time=  54.7s\n",
      "[CV 4/5; 13/30] START criterion=gini, min_samples_split=6, n_estimators=600.....\n",
      "[CV 4/5; 13/30] END criterion=gini, min_samples_split=6, n_estimators=600;, score=0.999 total time= 1.5min\n",
      "[CV 2/5; 15/30] START criterion=gini, min_samples_split=6, n_estimators=1000....\n",
      "[CV 2/5; 15/30] END criterion=gini, min_samples_split=6, n_estimators=1000;, score=0.999 total time= 2.5min\n",
      "[CV 2/5; 18/30] START criterion=entropy, min_samples_split=2, n_estimators=600..\n",
      "[CV 2/5; 18/30] END criterion=entropy, min_samples_split=2, n_estimators=600;, score=0.999 total time= 1.3min\n",
      "[CV 5/5; 19/30] START criterion=entropy, min_samples_split=2, n_estimators=800..\n",
      "[CV 5/5; 19/30] END criterion=entropy, min_samples_split=2, n_estimators=800;, score=0.999 total time= 1.7min\n",
      "[CV 4/5; 21/30] START criterion=entropy, min_samples_split=4, n_estimators=200..\n",
      "[CV 4/5; 21/30] END criterion=entropy, min_samples_split=4, n_estimators=200;, score=0.999 total time=  28.2s\n",
      "[CV 2/5; 22/30] START criterion=entropy, min_samples_split=4, n_estimators=400..\n",
      "[CV 2/5; 22/30] END criterion=entropy, min_samples_split=4, n_estimators=400;, score=0.999 total time=  50.6s\n",
      "[CV 3/5; 23/30] START criterion=entropy, min_samples_split=4, n_estimators=600..\n",
      "[CV 3/5; 23/30] END criterion=entropy, min_samples_split=4, n_estimators=600;, score=0.999 total time= 1.3min\n",
      "[CV 1/5; 25/30] START criterion=entropy, min_samples_split=4, n_estimators=1000.\n",
      "[CV 1/5; 25/30] END criterion=entropy, min_samples_split=4, n_estimators=1000;, score=0.999 total time= 2.2min\n",
      "[CV 3/5; 27/30] START criterion=entropy, min_samples_split=6, n_estimators=400..\n",
      "[CV 3/5; 27/30] END criterion=entropy, min_samples_split=6, n_estimators=400;, score=0.999 total time=  51.6s\n",
      "[CV 1/5; 29/30] START criterion=entropy, min_samples_split=6, n_estimators=800..\n",
      "[CV 1/5; 29/30] END criterion=entropy, min_samples_split=6, n_estimators=800;, score=0.999 total time= 1.7min\n",
      "[CV 4/5; 30/30] START criterion=entropy, min_samples_split=6, n_estimators=1000.\n",
      "[CV 4/5; 30/30] END criterion=entropy, min_samples_split=6, n_estimators=1000;, score=0.999 total time= 1.4min\n",
      "[CV 2/5; 2/10] START criterion=gini, min_samples_split=4........................\n",
      "[CV 2/5; 2/10] END criterion=gini, min_samples_split=4;, score=0.998 total time=   1.9s\n",
      "[CV 2/5; 3/10] START criterion=gini, min_samples_split=6........................\n",
      "[CV 2/5; 3/10] END criterion=gini, min_samples_split=6;, score=0.998 total time=   1.4s\n",
      "[CV 1/5; 5/10] START criterion=gini, min_samples_split=10.......................\n",
      "[CV 1/5; 5/10] END criterion=gini, min_samples_split=10;, score=0.997 total time=   1.4s\n",
      "[CV 2/5; 6/10] START criterion=entropy, min_samples_split=2.....................\n",
      "[CV 2/5; 6/10] END criterion=entropy, min_samples_split=2;, score=0.998 total time=   1.1s\n",
      "[CV 3/5; 7/10] START criterion=entropy, min_samples_split=4.....................\n",
      "[CV 3/5; 7/10] END criterion=entropy, min_samples_split=4;, score=0.998 total time=   1.1s\n",
      "[CV 1/5; 9/10] START criterion=entropy, min_samples_split=8.....................\n",
      "[CV 1/5; 9/10] END criterion=entropy, min_samples_split=8;, score=0.997 total time=   1.1s\n",
      "[CV 4/5; 10/10] START criterion=entropy, min_samples_split=10...................\n",
      "[CV 4/5; 10/10] END criterion=entropy, min_samples_split=10;, score=0.998 total time=   0.6s\n",
      "[CV 2/5; 2/10] START criterion=gini, min_samples_split=4........................\n",
      "[CV 2/5; 2/10] END criterion=gini, min_samples_split=4;, score=0.998 total time=   1.8s\n",
      "[CV 5/5; 3/10] START criterion=gini, min_samples_split=6........................\n",
      "[CV 5/5; 3/10] END criterion=gini, min_samples_split=6;, score=0.997 total time=   1.5s\n",
      "[CV 3/5; 5/10] START criterion=gini, min_samples_split=10.......................\n",
      "[CV 3/5; 5/10] END criterion=gini, min_samples_split=10;, score=0.997 total time=   1.6s\n",
      "[CV 2/5; 7/10] START criterion=entropy, min_samples_split=4.....................\n",
      "[CV 2/5; 7/10] END criterion=entropy, min_samples_split=4;, score=0.998 total time=   1.0s\n",
      "[CV 5/5; 8/10] START criterion=entropy, min_samples_split=6.....................\n",
      "[CV 5/5; 8/10] END criterion=entropy, min_samples_split=6;, score=0.998 total time=   1.0s\n",
      "[CV 3/5; 10/10] START criterion=entropy, min_samples_split=10...................\n",
      "[CV 3/5; 10/10] END criterion=entropy, min_samples_split=10;, score=0.998 total time=   0.9s\n",
      "[CV 4/5; 1/10] START criterion=gini, min_samples_split=2........................\n",
      "[CV 4/5; 1/10] END criterion=gini, min_samples_split=2;, score=0.998 total time=   1.8s\n",
      "[CV 2/5; 3/10] START criterion=gini, min_samples_split=6........................\n",
      "[CV 2/5; 3/10] END criterion=gini, min_samples_split=6;, score=0.998 total time=   2.0s\n",
      "[CV 5/5; 4/10] START criterion=gini, min_samples_split=8........................\n",
      "[CV 5/5; 4/10] END criterion=gini, min_samples_split=8;, score=0.998 total time=   1.5s\n",
      "[CV 4/5; 6/10] START criterion=entropy, min_samples_split=2.....................\n",
      "[CV 4/5; 6/10] END criterion=entropy, min_samples_split=2;, score=0.998 total time=   1.0s\n",
      "[CV 1/5; 8/10] START criterion=entropy, min_samples_split=6.....................\n",
      "[CV 1/5; 8/10] END criterion=entropy, min_samples_split=6;, score=0.998 total time=   1.0s\n",
      "[CV 4/5; 9/10] START criterion=entropy, min_samples_split=8.....................\n",
      "[CV 4/5; 9/10] END criterion=entropy, min_samples_split=8;, score=0.998 total time=   1.0s\n",
      "[CV 3/5; 1/30] START criterion=gini, min_samples_split=2, n_estimators=200......\n",
      "[CV 3/5; 1/30] END criterion=gini, min_samples_split=2, n_estimators=200;, score=0.999 total time=  27.3s\n",
      "[CV 1/5; 3/30] START criterion=gini, min_samples_split=2, n_estimators=600......\n",
      "[CV 1/5; 3/30] END criterion=gini, min_samples_split=2, n_estimators=600;, score=0.999 total time= 1.5min\n",
      "[CV 4/5; 4/30] START criterion=gini, min_samples_split=2, n_estimators=800......\n",
      "[CV 4/5; 4/30] END criterion=gini, min_samples_split=2, n_estimators=800;, score=0.999 total time= 1.8min\n",
      "[CV 2/5; 6/30] START criterion=gini, min_samples_split=4, n_estimators=200......\n",
      "[CV 2/5; 6/30] END criterion=gini, min_samples_split=4, n_estimators=200;, score=0.999 total time=  33.0s\n",
      "[CV 5/5; 6/30] START criterion=gini, min_samples_split=4, n_estimators=200......\n",
      "[CV 5/5; 6/30] END criterion=gini, min_samples_split=4, n_estimators=200;, score=0.999 total time=  29.7s\n",
      "[CV 4/5; 7/30] START criterion=gini, min_samples_split=4, n_estimators=400......\n",
      "[CV 4/5; 7/30] END criterion=gini, min_samples_split=4, n_estimators=400;, score=0.999 total time=  54.0s\n",
      "[CV 1/5; 9/30] START criterion=gini, min_samples_split=4, n_estimators=800......\n",
      "[CV 1/5; 9/30] END criterion=gini, min_samples_split=4, n_estimators=800;, score=0.999 total time= 1.8min\n",
      "[CV 4/5; 10/30] START criterion=gini, min_samples_split=4, n_estimators=1000....\n",
      "[CV 4/5; 10/30] END criterion=gini, min_samples_split=4, n_estimators=1000;, score=0.999 total time= 2.2min\n",
      "[CV 3/5; 13/30] START criterion=gini, min_samples_split=6, n_estimators=600.....\n",
      "[CV 3/5; 13/30] END criterion=gini, min_samples_split=6, n_estimators=600;, score=0.999 total time= 1.5min\n",
      "[CV 1/5; 15/30] START criterion=gini, min_samples_split=6, n_estimators=1000....\n",
      "[CV 1/5; 15/30] END criterion=gini, min_samples_split=6, n_estimators=1000;, score=0.999 total time= 2.5min\n",
      "[CV 5/5; 17/30] START criterion=entropy, min_samples_split=2, n_estimators=400..\n",
      "[CV 5/5; 17/30] END criterion=entropy, min_samples_split=2, n_estimators=400;, score=0.999 total time=  50.2s\n",
      "[CV 3/5; 19/30] START criterion=entropy, min_samples_split=2, n_estimators=800..\n",
      "[CV 3/5; 19/30] END criterion=entropy, min_samples_split=2, n_estimators=800;, score=0.999 total time= 1.7min\n",
      "[CV 1/5; 21/30] START criterion=entropy, min_samples_split=4, n_estimators=200..\n",
      "[CV 1/5; 21/30] END criterion=entropy, min_samples_split=4, n_estimators=200;, score=0.999 total time=  23.1s\n",
      "[CV 2/5; 21/30] START criterion=entropy, min_samples_split=4, n_estimators=200..\n",
      "[CV 2/5; 21/30] END criterion=entropy, min_samples_split=4, n_estimators=200;, score=0.999 total time=  27.5s\n",
      "[CV 5/5; 21/30] START criterion=entropy, min_samples_split=4, n_estimators=200..\n",
      "[CV 5/5; 21/30] END criterion=entropy, min_samples_split=4, n_estimators=200;, score=0.999 total time=  27.4s\n",
      "[CV 5/5; 22/30] START criterion=entropy, min_samples_split=4, n_estimators=400..\n",
      "[CV 5/5; 22/30] END criterion=entropy, min_samples_split=4, n_estimators=400;, score=0.999 total time=  51.0s\n",
      "[CV 3/5; 24/30] START criterion=entropy, min_samples_split=4, n_estimators=800..\n",
      "[CV 3/5; 24/30] END criterion=entropy, min_samples_split=4, n_estimators=800;, score=0.999 total time= 1.7min\n",
      "[CV 1/5; 26/30] START criterion=entropy, min_samples_split=6, n_estimators=200..\n",
      "[CV 1/5; 26/30] END criterion=entropy, min_samples_split=6, n_estimators=200;, score=0.999 total time=  26.2s\n",
      "[CV 2/5; 26/30] START criterion=entropy, min_samples_split=6, n_estimators=200..\n",
      "[CV 2/5; 26/30] END criterion=entropy, min_samples_split=6, n_estimators=200;, score=0.999 total time=  27.2s\n",
      "[CV 5/5; 26/30] START criterion=entropy, min_samples_split=6, n_estimators=200..\n",
      "[CV 5/5; 26/30] END criterion=entropy, min_samples_split=6, n_estimators=200;, score=0.999 total time=  26.6s\n",
      "[CV 4/5; 27/30] START criterion=entropy, min_samples_split=6, n_estimators=400..\n",
      "[CV 4/5; 27/30] END criterion=entropy, min_samples_split=6, n_estimators=400;, score=0.999 total time=  51.6s\n",
      "[CV 2/5; 29/30] START criterion=entropy, min_samples_split=6, n_estimators=800..\n",
      "[CV 2/5; 29/30] END criterion=entropy, min_samples_split=6, n_estimators=800;, score=0.999 total time= 1.7min\n",
      "[CV 5/5; 30/30] START criterion=entropy, min_samples_split=6, n_estimators=1000.\n",
      "[CV 5/5; 30/30] END criterion=entropy, min_samples_split=6, n_estimators=1000;, score=0.999 total time= 1.4min\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rf_params = {'criterion' : ['gini', 'entropy'],\n",
    "             'min_samples_split' : [2, 4, 6,],\n",
    "             'n_estimators' : [200, 400, 600, 800, 1000]}\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_gscv = GridSearchCV(rf_clf, rf_params, n_jobs=-1, verbose=10)\n",
    "\n",
    "rf_gscv.fit(x_train, y_train)\n",
    "best_rf_clf = rf_gscv.best_estimator_\n",
    "best_rf_params = rf_gscv.best_params_\n",
    "\n",
    "dump(best_rf_clf, 'best_rf_clf.joblib')\n",
    "\n",
    "print(\"Best Hyperparameters:\\n\", best_rf_params)\n",
    "y_pred = best_rf_clf.predict(x_test)\n",
    "print(\"total accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(\"dos accuracy:\",result[0][0]/sum(result[0]))\n",
    "print(\"normal accuracy\",result[1][1]/sum(result[1]))\n",
    "print(\"probe accuracy\",result[2][2]/sum(result[2]))\n",
    "print(\"r2l accuracy\",result[3][3]/sum(result[3]))\n",
    "print(\"u2r accuracy\",result[4][4]/sum(result[4]))\n",
    "\n",
    "plot_confusion_matrix(best_rf_clf, x_test, y_test)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[CV 1/5; 1/30] START learning_rate=1.0, loss=deviance, n_estimators=200.........\n",
      "[CV 1/5; 1/30] END learning_rate=1.0, loss=deviance, n_estimators=200;, score=0.991 total time= 5.7min\n",
      "[CV 2/5; 3/30] START learning_rate=1.0, loss=deviance, n_estimators=600.........\n",
      "[CV 2/5; 3/30] END learning_rate=1.0, loss=deviance, n_estimators=600;, score=0.985 total time=17.0min\n",
      "[CV 5/5; 4/30] START learning_rate=1.0, loss=deviance, n_estimators=800.........\n",
      "[CV 5/5; 4/30] END learning_rate=1.0, loss=deviance, n_estimators=800;, score=0.995 total time=24.0min\n",
      "[CV 3/5; 11/30] START learning_rate=0.1, loss=deviance, n_estimators=200........\n",
      "[CV 3/5; 11/30] END learning_rate=0.1, loss=deviance, n_estimators=200;, score=0.998 total time= 6.0min\n",
      "[CV 2/5; 12/30] START learning_rate=0.1, loss=deviance, n_estimators=400........\n",
      "[CV 2/5; 12/30] END learning_rate=0.1, loss=deviance, n_estimators=400;, score=0.999 total time=12.8min\n",
      "[CV 4/5; 13/30] START learning_rate=0.1, loss=deviance, n_estimators=600........\n",
      "[CV 4/5; 13/30] END learning_rate=0.1, loss=deviance, n_estimators=600;, score=0.998 total time=18.2min\n",
      "[CV 2/5; 15/30] START learning_rate=0.1, loss=deviance, n_estimators=1000.......\n",
      "[CV 2/5; 15/30] END learning_rate=0.1, loss=deviance, n_estimators=1000;, score=0.999 total time=30.2min\n",
      "[CV 5/5; 22/30] START learning_rate=0.01, loss=deviance, n_estimators=400.......\n",
      "[CV 5/5; 22/30] END learning_rate=0.01, loss=deviance, n_estimators=400;, score=0.996 total time=11.5min\n",
      "[CV 3/5; 24/30] START learning_rate=0.01, loss=deviance, n_estimators=800.......\n",
      "[CV 3/5; 24/30] END learning_rate=0.01, loss=deviance, n_estimators=800;, score=0.997 total time=22.8min\n",
      "[CV 1/5; 26/30] START learning_rate=0.01, loss=exponential, n_estimators=200....\n",
      "[CV 1/5; 26/30] END learning_rate=0.01, loss=exponential, n_estimators=200;, score=nan total time=   0.4s\n",
      "[CV 2/5; 26/30] START learning_rate=0.01, loss=exponential, n_estimators=200....\n",
      "[CV 2/5; 26/30] END learning_rate=0.01, loss=exponential, n_estimators=200;, score=nan total time=   0.4s\n",
      "[CV 3/5; 26/30] START learning_rate=0.01, loss=exponential, n_estimators=200....\n",
      "[CV 3/5; 26/30] END learning_rate=0.01, loss=exponential, n_estimators=200;, score=nan total time=   0.4s\n",
      "[CV 4/5; 26/30] START learning_rate=0.01, loss=exponential, n_estimators=200....\n",
      "[CV 4/5; 26/30] END learning_rate=0.01, loss=exponential, n_estimators=200;, score=nan total time=   0.4s\n",
      "[CV 5/5; 26/30] START learning_rate=0.01, loss=exponential, n_estimators=200....\n",
      "[CV 5/5; 26/30] END learning_rate=0.01, loss=exponential, n_estimators=200;, score=nan total time=   0.4s\n",
      "[CV 1/5; 27/30] START learning_rate=0.01, loss=exponential, n_estimators=400....\n",
      "[CV 1/5; 27/30] END learning_rate=0.01, loss=exponential, n_estimators=400;, score=nan total time=   0.4s\n",
      "[CV 2/5; 27/30] START learning_rate=0.01, loss=exponential, n_estimators=400....\n",
      "[CV 2/5; 27/30] END learning_rate=0.01, loss=exponential, n_estimators=400;, score=nan total time=   0.4s\n",
      "[CV 3/5; 27/30] START learning_rate=0.01, loss=exponential, n_estimators=400....\n",
      "[CV 3/5; 27/30] END learning_rate=0.01, loss=exponential, n_estimators=400;, score=nan total time=   0.4s\n",
      "[CV 4/5; 27/30] START learning_rate=0.01, loss=exponential, n_estimators=400....\n",
      "[CV 4/5; 27/30] END learning_rate=0.01, loss=exponential, n_estimators=400;, score=nan total time=   0.4s\n",
      "[CV 5/5; 27/30] START learning_rate=0.01, loss=exponential, n_estimators=400....\n",
      "[CV 5/5; 27/30] END learning_rate=0.01, loss=exponential, n_estimators=400;, score=nan total time=   0.4s\n",
      "[CV 1/5; 28/30] START learning_rate=0.01, loss=exponential, n_estimators=600....\n",
      "[CV 1/5; 28/30] END learning_rate=0.01, loss=exponential, n_estimators=600;, score=nan total time=   0.4s\n",
      "[CV 2/5; 28/30] START learning_rate=0.01, loss=exponential, n_estimators=600....\n",
      "[CV 2/5; 28/30] END learning_rate=0.01, loss=exponential, n_estimators=600;, score=nan total time=   0.4s\n",
      "[CV 3/5; 28/30] START learning_rate=0.01, loss=exponential, n_estimators=600....\n",
      "[CV 3/5; 28/30] END learning_rate=0.01, loss=exponential, n_estimators=600;, score=nan total time=   0.4s\n",
      "[CV 4/5; 28/30] START learning_rate=0.01, loss=exponential, n_estimators=600....\n",
      "[CV 4/5; 28/30] END learning_rate=0.01, loss=exponential, n_estimators=600;, score=nan total time=   0.4s\n",
      "[CV 5/5; 28/30] START learning_rate=0.01, loss=exponential, n_estimators=600....\n",
      "[CV 5/5; 28/30] END learning_rate=0.01, loss=exponential, n_estimators=600;, score=nan total time=   0.4s\n",
      "[CV 1/5; 29/30] START learning_rate=0.01, loss=exponential, n_estimators=800....\n",
      "[CV 1/5; 29/30] END learning_rate=0.01, loss=exponential, n_estimators=800;, score=nan total time=   0.4s\n",
      "[CV 2/5; 29/30] START learning_rate=0.01, loss=exponential, n_estimators=800....\n",
      "[CV 2/5; 29/30] END learning_rate=0.01, loss=exponential, n_estimators=800;, score=nan total time=   0.4s\n",
      "[CV 3/5; 29/30] START learning_rate=0.01, loss=exponential, n_estimators=800....\n",
      "[CV 3/5; 29/30] END learning_rate=0.01, loss=exponential, n_estimators=800;, score=nan total time=   0.4s\n",
      "[CV 4/5; 29/30] START learning_rate=0.01, loss=exponential, n_estimators=800....\n",
      "[CV 4/5; 29/30] END learning_rate=0.01, loss=exponential, n_estimators=800;, score=nan total time=   0.4s\n",
      "[CV 5/5; 29/30] START learning_rate=0.01, loss=exponential, n_estimators=800....\n",
      "[CV 5/5; 29/30] END learning_rate=0.01, loss=exponential, n_estimators=800;, score=nan total time=   0.4s\n",
      "[CV 1/5; 30/30] START learning_rate=0.01, loss=exponential, n_estimators=1000...\n",
      "[CV 1/5; 30/30] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=nan total time=   0.4s\n",
      "[CV 2/5; 30/30] START learning_rate=0.01, loss=exponential, n_estimators=1000...\n",
      "[CV 2/5; 30/30] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=nan total time=   0.4s\n",
      "[CV 3/5; 30/30] START learning_rate=0.01, loss=exponential, n_estimators=1000...\n",
      "[CV 3/5; 30/30] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=nan total time=   0.4s\n",
      "[CV 4/5; 30/30] START learning_rate=0.01, loss=exponential, n_estimators=1000...\n",
      "[CV 4/5; 30/30] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=nan total time=   0.4s\n",
      "[CV 5/5; 30/30] START learning_rate=0.01, loss=exponential, n_estimators=1000...\n",
      "[CV 5/5; 30/30] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=nan total time=   0.4s\n",
      "[CV 3/5; 1/30] START learning_rate=1.0, loss=deviance, n_estimators=200.........\n",
      "[CV 3/5; 1/30] END learning_rate=1.0, loss=deviance, n_estimators=200;, score=0.990 total time= 5.7min\n",
      "[CV 5/5; 2/30] START learning_rate=1.0, loss=deviance, n_estimators=400.........\n",
      "[CV 5/5; 2/30] END learning_rate=1.0, loss=deviance, n_estimators=400;, score=0.995 total time=11.3min\n",
      "[CV 3/5; 4/30] START learning_rate=1.0, loss=deviance, n_estimators=800.........\n",
      "[CV 3/5; 4/30] END learning_rate=1.0, loss=deviance, n_estimators=800;, score=0.990 total time=24.0min\n",
      "[CV 1/5; 6/30] START learning_rate=1.0, loss=exponential, n_estimators=200......\n",
      "[CV 1/5; 6/30] END learning_rate=1.0, loss=exponential, n_estimators=200;, score=nan total time=   0.4s\n",
      "[CV 2/5; 6/30] START learning_rate=1.0, loss=exponential, n_estimators=200......\n",
      "[CV 2/5; 6/30] END learning_rate=1.0, loss=exponential, n_estimators=200;, score=nan total time=   0.4s\n",
      "[CV 3/5; 6/30] START learning_rate=1.0, loss=exponential, n_estimators=200......\n",
      "[CV 3/5; 6/30] END learning_rate=1.0, loss=exponential, n_estimators=200;, score=nan total time=   0.4s\n",
      "[CV 4/5; 6/30] START learning_rate=1.0, loss=exponential, n_estimators=200......\n",
      "[CV 4/5; 6/30] END learning_rate=1.0, loss=exponential, n_estimators=200;, score=nan total time=   0.4s\n",
      "[CV 5/5; 6/30] START learning_rate=1.0, loss=exponential, n_estimators=200......\n",
      "[CV 5/5; 6/30] END learning_rate=1.0, loss=exponential, n_estimators=200;, score=nan total time=   0.4s\n",
      "[CV 1/5; 7/30] START learning_rate=1.0, loss=exponential, n_estimators=400......\n",
      "[CV 1/5; 7/30] END learning_rate=1.0, loss=exponential, n_estimators=400;, score=nan total time=   0.4s\n",
      "[CV 2/5; 7/30] START learning_rate=1.0, loss=exponential, n_estimators=400......\n",
      "[CV 2/5; 7/30] END learning_rate=1.0, loss=exponential, n_estimators=400;, score=nan total time=   0.4s\n",
      "[CV 3/5; 7/30] START learning_rate=1.0, loss=exponential, n_estimators=400......\n",
      "[CV 3/5; 7/30] END learning_rate=1.0, loss=exponential, n_estimators=400;, score=nan total time=   0.4s\n",
      "[CV 4/5; 7/30] START learning_rate=1.0, loss=exponential, n_estimators=400......\n",
      "[CV 4/5; 7/30] END learning_rate=1.0, loss=exponential, n_estimators=400;, score=nan total time=   0.4s\n",
      "[CV 5/5; 7/30] START learning_rate=1.0, loss=exponential, n_estimators=400......\n",
      "[CV 5/5; 7/30] END learning_rate=1.0, loss=exponential, n_estimators=400;, score=nan total time=   0.4s\n",
      "[CV 1/5; 8/30] START learning_rate=1.0, loss=exponential, n_estimators=600......\n",
      "[CV 1/5; 8/30] END learning_rate=1.0, loss=exponential, n_estimators=600;, score=nan total time=   0.4s\n",
      "[CV 2/5; 8/30] START learning_rate=1.0, loss=exponential, n_estimators=600......\n",
      "[CV 2/5; 8/30] END learning_rate=1.0, loss=exponential, n_estimators=600;, score=nan total time=   0.4s\n",
      "[CV 3/5; 8/30] START learning_rate=1.0, loss=exponential, n_estimators=600......\n",
      "[CV 3/5; 8/30] END learning_rate=1.0, loss=exponential, n_estimators=600;, score=nan total time=   0.4s\n",
      "[CV 4/5; 8/30] START learning_rate=1.0, loss=exponential, n_estimators=600......\n",
      "[CV 4/5; 8/30] END learning_rate=1.0, loss=exponential, n_estimators=600;, score=nan total time=   0.4s\n",
      "[CV 5/5; 8/30] START learning_rate=1.0, loss=exponential, n_estimators=600......\n",
      "[CV 5/5; 8/30] END learning_rate=1.0, loss=exponential, n_estimators=600;, score=nan total time=   0.4s\n",
      "[CV 1/5; 9/30] START learning_rate=1.0, loss=exponential, n_estimators=800......\n",
      "[CV 1/5; 9/30] END learning_rate=1.0, loss=exponential, n_estimators=800;, score=nan total time=   0.4s\n",
      "[CV 2/5; 9/30] START learning_rate=1.0, loss=exponential, n_estimators=800......\n",
      "[CV 2/5; 9/30] END learning_rate=1.0, loss=exponential, n_estimators=800;, score=nan total time=   0.4s\n",
      "[CV 3/5; 9/30] START learning_rate=1.0, loss=exponential, n_estimators=800......\n",
      "[CV 3/5; 9/30] END learning_rate=1.0, loss=exponential, n_estimators=800;, score=nan total time=   0.4s\n",
      "[CV 4/5; 9/30] START learning_rate=1.0, loss=exponential, n_estimators=800......\n",
      "[CV 4/5; 9/30] END learning_rate=1.0, loss=exponential, n_estimators=800;, score=nan total time=   0.4s\n",
      "[CV 5/5; 9/30] START learning_rate=1.0, loss=exponential, n_estimators=800......\n",
      "[CV 5/5; 9/30] END learning_rate=1.0, loss=exponential, n_estimators=800;, score=nan total time=   0.4s\n",
      "[CV 1/5; 10/30] START learning_rate=1.0, loss=exponential, n_estimators=1000....\n",
      "[CV 1/5; 10/30] END learning_rate=1.0, loss=exponential, n_estimators=1000;, score=nan total time=   0.4s\n",
      "[CV 2/5; 10/30] START learning_rate=1.0, loss=exponential, n_estimators=1000....\n",
      "[CV 2/5; 10/30] END learning_rate=1.0, loss=exponential, n_estimators=1000;, score=nan total time=   0.4s\n",
      "[CV 3/5; 10/30] START learning_rate=1.0, loss=exponential, n_estimators=1000....\n",
      "[CV 3/5; 10/30] END learning_rate=1.0, loss=exponential, n_estimators=1000;, score=nan total time=   0.4s\n",
      "[CV 4/5; 10/30] START learning_rate=1.0, loss=exponential, n_estimators=1000....\n",
      "[CV 4/5; 10/30] END learning_rate=1.0, loss=exponential, n_estimators=1000;, score=nan total time=   0.4s\n",
      "[CV 5/5; 10/30] START learning_rate=1.0, loss=exponential, n_estimators=1000....\n",
      "[CV 5/5; 10/30] END learning_rate=1.0, loss=exponential, n_estimators=1000;, score=nan total time=   0.4s\n",
      "[CV 1/5; 11/30] START learning_rate=0.1, loss=deviance, n_estimators=200........\n",
      "[CV 1/5; 11/30] END learning_rate=0.1, loss=deviance, n_estimators=200;, score=0.998 total time= 5.9min\n",
      "[CV 4/5; 11/30] START learning_rate=0.1, loss=deviance, n_estimators=200........\n",
      "[CV 4/5; 11/30] END learning_rate=0.1, loss=deviance, n_estimators=200;, score=0.998 total time= 6.0min\n",
      "[CV 3/5; 12/30] START learning_rate=0.1, loss=deviance, n_estimators=400........\n",
      "[CV 3/5; 12/30] END learning_rate=0.1, loss=deviance, n_estimators=400;, score=0.998 total time=12.8min\n",
      "[CV 5/5; 13/30] START learning_rate=0.1, loss=deviance, n_estimators=600........\n",
      "[CV 5/5; 13/30] END learning_rate=0.1, loss=deviance, n_estimators=600;, score=0.998 total time=18.2min\n",
      "[CV 3/5; 15/30] START learning_rate=0.1, loss=deviance, n_estimators=1000.......\n",
      "[CV 3/5; 15/30] END learning_rate=0.1, loss=deviance, n_estimators=1000;, score=0.998 total time=30.0min\n",
      "[CV 1/5; 23/30] START learning_rate=0.01, loss=deviance, n_estimators=600.......\n",
      "[CV 1/5; 23/30] END learning_rate=0.01, loss=deviance, n_estimators=600;, score=0.996 total time=17.2min\n",
      "[CV 4/5; 24/30] START learning_rate=0.01, loss=deviance, n_estimators=800.......\n",
      "[CV 4/5; 24/30] END learning_rate=0.01, loss=deviance, n_estimators=800;, score=0.997 total time=22.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 2/30] START learning_rate=1.0, loss=deviance, n_estimators=400.........\n",
      "[CV 1/5; 2/30] END learning_rate=1.0, loss=deviance, n_estimators=400;, score=0.991 total time=11.3min\n",
      "[CV 5/5; 3/30] START learning_rate=1.0, loss=deviance, n_estimators=600.........\n",
      "[CV 5/5; 3/30] END learning_rate=1.0, loss=deviance, n_estimators=600;, score=0.995 total time=17.7min\n",
      "[CV 3/5; 5/30] START learning_rate=1.0, loss=deviance, n_estimators=1000........\n",
      "[CV 3/5; 5/30] END learning_rate=1.0, loss=deviance, n_estimators=1000;, score=0.990 total time=30.3min\n",
      "[CV 1/5; 13/30] START learning_rate=0.1, loss=deviance, n_estimators=600........\n",
      "[CV 1/5; 13/30] END learning_rate=0.1, loss=deviance, n_estimators=600;, score=0.999 total time=18.5min\n",
      "[CV 4/5; 14/30] START learning_rate=0.1, loss=deviance, n_estimators=800........\n",
      "[CV 4/5; 14/30] END learning_rate=0.1, loss=deviance, n_estimators=800;, score=0.999 total time=24.5min\n",
      "[CV 3/5; 21/30] START learning_rate=0.01, loss=deviance, n_estimators=200.......\n",
      "[CV 3/5; 21/30] END learning_rate=0.01, loss=deviance, n_estimators=200;, score=0.984 total time= 5.9min\n",
      "[CV 1/5; 22/30] START learning_rate=0.01, loss=deviance, n_estimators=400.......\n",
      "[CV 1/5; 22/30] END learning_rate=0.01, loss=deviance, n_estimators=400;, score=0.995 total time=11.4min\n",
      "[CV 2/5; 23/30] START learning_rate=0.01, loss=deviance, n_estimators=600.......\n",
      "[CV 2/5; 23/30] END learning_rate=0.01, loss=deviance, n_estimators=600;, score=0.996 total time=17.2min\n",
      "[CV 5/5; 24/30] START learning_rate=0.01, loss=deviance, n_estimators=800.......\n",
      "[CV 5/5; 24/30] END learning_rate=0.01, loss=deviance, n_estimators=800;, score=0.997 total time=21.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "75 fits failed out of a total of 150.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\", line 525, in fit\n",
      "    self._check_params()\n",
      "  File \"/opt/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\", line 310, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"/opt/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_gb_losses.py\", line 890, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: ExponentialLoss requires 2 classes; got 5 class(es)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.98217856 0.98219444 0.98219444 0.98219444 0.98220238        nan\n",
      "        nan        nan        nan        nan 0.99780111 0.99815833\n",
      " 0.99825359 0.99834885 0.99835679        nan        nan        nan\n",
      "        nan        nan 0.986767   0.99548316 0.99634843 0.9969041\n",
      " 0.99726132        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/30] START learning_rate=1.0, loss=deviance, n_estimators=200.........\n",
      "[CV 4/5; 1/30] END learning_rate=1.0, loss=deviance, n_estimators=200;, score=0.949 total time= 5.7min\n",
      "[CV 3/5; 3/30] START learning_rate=1.0, loss=deviance, n_estimators=600.........\n",
      "[CV 3/5; 3/30] END learning_rate=1.0, loss=deviance, n_estimators=600;, score=0.990 total time=17.2min\n",
      "[CV 1/5; 5/30] START learning_rate=1.0, loss=deviance, n_estimators=1000........\n",
      "[CV 1/5; 5/30] END learning_rate=1.0, loss=deviance, n_estimators=1000;, score=0.991 total time=27.5min\n",
      "[CV 5/5; 11/30] START learning_rate=0.1, loss=deviance, n_estimators=200........\n",
      "[CV 5/5; 11/30] END learning_rate=0.1, loss=deviance, n_estimators=200;, score=0.997 total time= 6.3min\n",
      "[CV 4/5; 12/30] START learning_rate=0.1, loss=deviance, n_estimators=400........\n",
      "[CV 4/5; 12/30] END learning_rate=0.1, loss=deviance, n_estimators=400;, score=0.998 total time=12.7min\n",
      "[CV 1/5; 14/30] START learning_rate=0.1, loss=deviance, n_estimators=800........\n",
      "[CV 1/5; 14/30] END learning_rate=0.1, loss=deviance, n_estimators=800;, score=0.999 total time=24.3min\n",
      "[CV 4/5; 15/30] START learning_rate=0.1, loss=deviance, n_estimators=1000.......\n",
      "[CV 4/5; 15/30] END learning_rate=0.1, loss=deviance, n_estimators=1000;, score=0.998 total time=29.5min\n",
      "[CV 3/5; 23/30] START learning_rate=0.01, loss=deviance, n_estimators=600.......\n",
      "[CV 3/5; 23/30] END learning_rate=0.01, loss=deviance, n_estimators=600;, score=0.996 total time=17.1min\n",
      "[CV 1/5; 25/30] START learning_rate=0.01, loss=deviance, n_estimators=1000......\n",
      "[CV 1/5; 25/30] END learning_rate=0.01, loss=deviance, n_estimators=1000;, score=0.997 total time=24.3min\n",
      "[CV 3/5; 2/30] START learning_rate=1.0, loss=deviance, n_estimators=400.........\n",
      "[CV 3/5; 2/30] END learning_rate=1.0, loss=deviance, n_estimators=400;, score=0.990 total time=11.3min\n",
      "[CV 1/5; 4/30] START learning_rate=1.0, loss=deviance, n_estimators=800.........\n",
      "[CV 1/5; 4/30] END learning_rate=1.0, loss=deviance, n_estimators=800;, score=0.991 total time=22.1min\n",
      "[CV 4/5; 5/30] START learning_rate=1.0, loss=deviance, n_estimators=1000........\n",
      "[CV 4/5; 5/30] END learning_rate=1.0, loss=deviance, n_estimators=1000;, score=0.949 total time=30.4min\n",
      "[CV 2/5; 13/30] START learning_rate=0.1, loss=deviance, n_estimators=600........\n",
      "[CV 2/5; 13/30] END learning_rate=0.1, loss=deviance, n_estimators=600;, score=0.999 total time=18.3min\n",
      "[CV 5/5; 14/30] START learning_rate=0.1, loss=deviance, n_estimators=800........\n",
      "[CV 5/5; 14/30] END learning_rate=0.1, loss=deviance, n_estimators=800;, score=0.998 total time=24.5min\n",
      "[CV 4/5; 21/30] START learning_rate=0.01, loss=deviance, n_estimators=200.......\n",
      "[CV 4/5; 21/30] END learning_rate=0.01, loss=deviance, n_estimators=200;, score=0.988 total time= 5.7min\n",
      "[CV 2/5; 22/30] START learning_rate=0.01, loss=deviance, n_estimators=400.......\n",
      "[CV 2/5; 22/30] END learning_rate=0.01, loss=deviance, n_estimators=400;, score=0.995 total time=11.4min\n",
      "[CV 4/5; 23/30] START learning_rate=0.01, loss=deviance, n_estimators=600.......\n",
      "[CV 4/5; 23/30] END learning_rate=0.01, loss=deviance, n_estimators=600;, score=0.996 total time=17.0min\n",
      "[CV 2/5; 25/30] START learning_rate=0.01, loss=deviance, n_estimators=1000......\n",
      "[CV 2/5; 25/30] END learning_rate=0.01, loss=deviance, n_estimators=1000;, score=0.997 total time=24.1min\n",
      "[CV 5/5; 1/30] START learning_rate=1.0, loss=deviance, n_estimators=200.........\n",
      "[CV 5/5; 1/30] END learning_rate=1.0, loss=deviance, n_estimators=200;, score=0.995 total time= 5.7min\n",
      "[CV 1/5; 3/30] START learning_rate=1.0, loss=deviance, n_estimators=600.........\n",
      "[CV 1/5; 3/30] END learning_rate=1.0, loss=deviance, n_estimators=600;, score=0.991 total time=16.4min\n",
      "[CV 4/5; 4/30] START learning_rate=1.0, loss=deviance, n_estimators=800.........\n",
      "[CV 4/5; 4/30] END learning_rate=1.0, loss=deviance, n_estimators=800;, score=0.949 total time=24.0min\n",
      "[CV 2/5; 11/30] START learning_rate=0.1, loss=deviance, n_estimators=200........\n",
      "[CV 2/5; 11/30] END learning_rate=0.1, loss=deviance, n_estimators=200;, score=0.998 total time= 5.9min\n",
      "[CV 1/5; 12/30] START learning_rate=0.1, loss=deviance, n_estimators=400........\n",
      "[CV 1/5; 12/30] END learning_rate=0.1, loss=deviance, n_estimators=400;, score=0.998 total time=12.8min\n",
      "[CV 3/5; 13/30] START learning_rate=0.1, loss=deviance, n_estimators=600........\n",
      "[CV 3/5; 13/30] END learning_rate=0.1, loss=deviance, n_estimators=600;, score=0.998 total time=18.3min\n",
      "[CV 1/5; 15/30] START learning_rate=0.1, loss=deviance, n_estimators=1000.......\n",
      "[CV 1/5; 15/30] END learning_rate=0.1, loss=deviance, n_estimators=1000;, score=0.999 total time=30.1min\n",
      "[CV 3/5; 22/30] START learning_rate=0.01, loss=deviance, n_estimators=400.......\n",
      "[CV 3/5; 22/30] END learning_rate=0.01, loss=deviance, n_estimators=400;, score=0.996 total time=11.4min\n",
      "[CV 5/5; 23/30] START learning_rate=0.01, loss=deviance, n_estimators=600.......\n",
      "[CV 5/5; 23/30] END learning_rate=0.01, loss=deviance, n_estimators=600;, score=0.997 total time=17.1min\n",
      "[CV 3/5; 25/30] START learning_rate=0.01, loss=deviance, n_estimators=1000......\n",
      "[CV 3/5; 25/30] END learning_rate=0.01, loss=deviance, n_estimators=1000;, score=0.997 total time=23.6min\n",
      "[CV 2/5; 2/30] START learning_rate=1.0, loss=deviance, n_estimators=400.........\n",
      "[CV 2/5; 2/30] END learning_rate=1.0, loss=deviance, n_estimators=400;, score=0.985 total time=11.2min\n",
      "[CV 4/5; 3/30] START learning_rate=1.0, loss=deviance, n_estimators=600.........\n",
      "[CV 4/5; 3/30] END learning_rate=1.0, loss=deviance, n_estimators=600;, score=0.949 total time=17.7min\n",
      "[CV 2/5; 5/30] START learning_rate=1.0, loss=deviance, n_estimators=1000........\n",
      "[CV 2/5; 5/30] END learning_rate=1.0, loss=deviance, n_estimators=1000;, score=0.985 total time=30.0min\n",
      "[CV 5/5; 12/30] START learning_rate=0.1, loss=deviance, n_estimators=400........\n",
      "[CV 5/5; 12/30] END learning_rate=0.1, loss=deviance, n_estimators=400;, score=0.998 total time=12.6min\n",
      "[CV 3/5; 14/30] START learning_rate=0.1, loss=deviance, n_estimators=800........\n",
      "[CV 3/5; 14/30] END learning_rate=0.1, loss=deviance, n_estimators=800;, score=0.998 total time=24.2min\n",
      "[CV 5/5; 15/30] START learning_rate=0.1, loss=deviance, n_estimators=1000.......\n",
      "[CV 5/5; 15/30] END learning_rate=0.1, loss=deviance, n_estimators=1000;, score=0.998 total time=29.4min\n",
      "[CV 1/5; 24/30] START learning_rate=0.01, loss=deviance, n_estimators=800.......\n",
      "[CV 1/5; 24/30] END learning_rate=0.01, loss=deviance, n_estimators=800;, score=0.997 total time=22.9min\n",
      "[CV 4/5; 25/30] START learning_rate=0.01, loss=deviance, n_estimators=1000......\n",
      "[CV 4/5; 25/30] END learning_rate=0.01, loss=deviance, n_estimators=1000;, score=0.998 total time=20.5min\n",
      "[CV 2/5; 1/30] START learning_rate=1.0, loss=deviance, n_estimators=200.........\n",
      "[CV 2/5; 1/30] END learning_rate=1.0, loss=deviance, n_estimators=200;, score=0.985 total time= 5.6min\n",
      "[CV 4/5; 2/30] START learning_rate=1.0, loss=deviance, n_estimators=400.........\n",
      "[CV 4/5; 2/30] END learning_rate=1.0, loss=deviance, n_estimators=400;, score=0.949 total time=11.3min\n",
      "[CV 2/5; 4/30] START learning_rate=1.0, loss=deviance, n_estimators=800.........\n",
      "[CV 2/5; 4/30] END learning_rate=1.0, loss=deviance, n_estimators=800;, score=0.985 total time=23.8min\n",
      "[CV 5/5; 5/30] START learning_rate=1.0, loss=deviance, n_estimators=1000........\n",
      "[CV 5/5; 5/30] END learning_rate=1.0, loss=deviance, n_estimators=1000;, score=0.995 total time=30.7min\n",
      "[CV 2/5; 14/30] START learning_rate=0.1, loss=deviance, n_estimators=800........\n",
      "[CV 2/5; 14/30] END learning_rate=0.1, loss=deviance, n_estimators=800;, score=0.999 total time=24.3min\n",
      "[CV 1/5; 16/30] START learning_rate=0.1, loss=exponential, n_estimators=200.....\n",
      "[CV 1/5; 16/30] END learning_rate=0.1, loss=exponential, n_estimators=200;, score=nan total time=   0.4s\n",
      "[CV 2/5; 16/30] START learning_rate=0.1, loss=exponential, n_estimators=200.....\n",
      "[CV 2/5; 16/30] END learning_rate=0.1, loss=exponential, n_estimators=200;, score=nan total time=   0.4s\n",
      "[CV 3/5; 16/30] START learning_rate=0.1, loss=exponential, n_estimators=200.....\n",
      "[CV 3/5; 16/30] END learning_rate=0.1, loss=exponential, n_estimators=200;, score=nan total time=   0.4s\n",
      "[CV 4/5; 16/30] START learning_rate=0.1, loss=exponential, n_estimators=200.....\n",
      "[CV 4/5; 16/30] END learning_rate=0.1, loss=exponential, n_estimators=200;, score=nan total time=   0.4s\n",
      "[CV 5/5; 16/30] START learning_rate=0.1, loss=exponential, n_estimators=200.....\n",
      "[CV 5/5; 16/30] END learning_rate=0.1, loss=exponential, n_estimators=200;, score=nan total time=   0.4s\n",
      "[CV 1/5; 17/30] START learning_rate=0.1, loss=exponential, n_estimators=400.....\n",
      "[CV 1/5; 17/30] END learning_rate=0.1, loss=exponential, n_estimators=400;, score=nan total time=   0.5s\n",
      "[CV 2/5; 17/30] START learning_rate=0.1, loss=exponential, n_estimators=400.....\n",
      "[CV 2/5; 17/30] END learning_rate=0.1, loss=exponential, n_estimators=400;, score=nan total time=   0.5s\n",
      "[CV 3/5; 17/30] START learning_rate=0.1, loss=exponential, n_estimators=400.....\n",
      "[CV 3/5; 17/30] END learning_rate=0.1, loss=exponential, n_estimators=400;, score=nan total time=   0.5s\n",
      "[CV 4/5; 17/30] START learning_rate=0.1, loss=exponential, n_estimators=400.....\n",
      "[CV 4/5; 17/30] END learning_rate=0.1, loss=exponential, n_estimators=400;, score=nan total time=   0.4s\n",
      "[CV 5/5; 17/30] START learning_rate=0.1, loss=exponential, n_estimators=400.....\n",
      "[CV 5/5; 17/30] END learning_rate=0.1, loss=exponential, n_estimators=400;, score=nan total time=   0.4s\n",
      "[CV 1/5; 18/30] START learning_rate=0.1, loss=exponential, n_estimators=600.....\n",
      "[CV 1/5; 18/30] END learning_rate=0.1, loss=exponential, n_estimators=600;, score=nan total time=   0.4s\n",
      "[CV 2/5; 18/30] START learning_rate=0.1, loss=exponential, n_estimators=600.....\n",
      "[CV 2/5; 18/30] END learning_rate=0.1, loss=exponential, n_estimators=600;, score=nan total time=   0.4s\n",
      "[CV 3/5; 18/30] START learning_rate=0.1, loss=exponential, n_estimators=600.....\n",
      "[CV 3/5; 18/30] END learning_rate=0.1, loss=exponential, n_estimators=600;, score=nan total time=   0.4s\n",
      "[CV 4/5; 18/30] START learning_rate=0.1, loss=exponential, n_estimators=600.....\n",
      "[CV 4/5; 18/30] END learning_rate=0.1, loss=exponential, n_estimators=600;, score=nan total time=   0.4s\n",
      "[CV 5/5; 18/30] START learning_rate=0.1, loss=exponential, n_estimators=600.....\n",
      "[CV 5/5; 18/30] END learning_rate=0.1, loss=exponential, n_estimators=600;, score=nan total time=   0.4s\n",
      "[CV 1/5; 19/30] START learning_rate=0.1, loss=exponential, n_estimators=800.....\n",
      "[CV 1/5; 19/30] END learning_rate=0.1, loss=exponential, n_estimators=800;, score=nan total time=   0.4s\n",
      "[CV 2/5; 19/30] START learning_rate=0.1, loss=exponential, n_estimators=800.....\n",
      "[CV 2/5; 19/30] END learning_rate=0.1, loss=exponential, n_estimators=800;, score=nan total time=   0.4s\n",
      "[CV 3/5; 19/30] START learning_rate=0.1, loss=exponential, n_estimators=800.....\n",
      "[CV 3/5; 19/30] END learning_rate=0.1, loss=exponential, n_estimators=800;, score=nan total time=   0.4s\n",
      "[CV 4/5; 19/30] START learning_rate=0.1, loss=exponential, n_estimators=800.....\n",
      "[CV 4/5; 19/30] END learning_rate=0.1, loss=exponential, n_estimators=800;, score=nan total time=   0.4s\n",
      "[CV 5/5; 19/30] START learning_rate=0.1, loss=exponential, n_estimators=800.....\n",
      "[CV 5/5; 19/30] END learning_rate=0.1, loss=exponential, n_estimators=800;, score=nan total time=   0.4s\n",
      "[CV 1/5; 20/30] START learning_rate=0.1, loss=exponential, n_estimators=1000....\n",
      "[CV 1/5; 20/30] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=nan total time=   0.4s\n",
      "[CV 2/5; 20/30] START learning_rate=0.1, loss=exponential, n_estimators=1000....\n",
      "[CV 2/5; 20/30] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=nan total time=   0.4s\n",
      "[CV 3/5; 20/30] START learning_rate=0.1, loss=exponential, n_estimators=1000....\n",
      "[CV 3/5; 20/30] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=nan total time=   0.4s\n",
      "[CV 4/5; 20/30] START learning_rate=0.1, loss=exponential, n_estimators=1000....\n",
      "[CV 4/5; 20/30] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=nan total time=   0.4s\n",
      "[CV 5/5; 20/30] START learning_rate=0.1, loss=exponential, n_estimators=1000....\n",
      "[CV 5/5; 20/30] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=nan total time=   0.4s\n",
      "[CV 1/5; 21/30] START learning_rate=0.01, loss=deviance, n_estimators=200.......\n",
      "[CV 1/5; 21/30] END learning_rate=0.01, loss=deviance, n_estimators=200;, score=0.988 total time= 6.2min\n",
      "[CV 2/5; 21/30] START learning_rate=0.01, loss=deviance, n_estimators=200.......\n",
      "[CV 2/5; 21/30] END learning_rate=0.01, loss=deviance, n_estimators=200;, score=0.989 total time= 5.9min\n",
      "[CV 5/5; 21/30] START learning_rate=0.01, loss=deviance, n_estimators=200.......\n",
      "[CV 5/5; 21/30] END learning_rate=0.01, loss=deviance, n_estimators=200;, score=0.986 total time= 5.7min\n",
      "[CV 4/5; 22/30] START learning_rate=0.01, loss=deviance, n_estimators=400.......\n",
      "[CV 4/5; 22/30] END learning_rate=0.01, loss=deviance, n_estimators=400;, score=0.996 total time=11.4min\n",
      "[CV 2/5; 24/30] START learning_rate=0.01, loss=deviance, n_estimators=800.......\n",
      "[CV 2/5; 24/30] END learning_rate=0.01, loss=deviance, n_estimators=800;, score=0.997 total time=22.9min\n",
      "[CV 5/5; 25/30] START learning_rate=0.01, loss=deviance, n_estimators=1000......\n",
      "[CV 5/5; 25/30] END learning_rate=0.01, loss=deviance, n_estimators=1000;, score=0.998 total time=20.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      " {'learning_rate': 0.1, 'loss': 'deviance', 'n_estimators': 1000}\n",
      "total accuracy: 0.7588271823988645\n",
      "dos accuracy: 0.7860016090104586\n",
      "normal accuracy 0.9716815981876222\n",
      "probe accuracy 0.6600578273440727\n",
      "r2l accuracy 0.0759592795614722\n",
      "u2r accuracy 0.0425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         dos       0.96      0.79      0.86      7458\n",
      "      normal       0.66      0.97      0.79      9711\n",
      "       probe       0.82      0.66      0.73      2421\n",
      "         r2l       0.99      0.08      0.14      2554\n",
      "         u2r       0.71      0.04      0.08       400\n",
      "\n",
      "    accuracy                           0.76     22544\n",
      "   macro avg       0.83      0.51      0.52     22544\n",
      "weighted avg       0.82      0.76      0.72     22544\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GradientBoosting Classifier\n",
    "gb_params = {'loss' : ['deviance', 'exponential'],\n",
    "             'learning_rate' : [1.0, 0.1, 0.01],\n",
    "             'n_estimators' : [200, 400, 600, 800, 1000]}\n",
    "\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "gb_gscv = GridSearchCV(gb_clf, gb_params, n_jobs=-1, verbose=10)\n",
    "\n",
    "gb_gscv.fit(x_train, y_train)\n",
    "best_gb_clf = gb_gscv.best_estimator_\n",
    "best_gb_params = gb_gscv.best_params_\n",
    "\n",
    "dump(best_gb_clf, 'best_gb_clf.joblib')\n",
    "\n",
    "print(\"Best Hyperparameters:\\n\", best_gb_params)\n",
    "y_pred = best_gb_clf.predict(x_test)\n",
    "print(\"total accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(\"dos accuracy:\",result[0][0]/sum(result[0]))\n",
    "print(\"normal accuracy\",result[1][1]/sum(result[1]))\n",
    "print(\"probe accuracy\",result[2][2]/sum(result[2]))\n",
    "print(\"r2l accuracy\",result[3][3]/sum(result[3]))\n",
    "print(\"u2r accuracy\",result[4][4]/sum(result[4]))\n",
    "\n",
    "plot_confusion_matrix(best_gb_clf, x_test, y_test)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[CV 1/5; 2/30] START C=10, max_iter=1000, penalty=l2............................\n",
      "[CV 1/5; 2/30] END C=10, max_iter=1000, penalty=l2;, score=0.906 total time= 2.3min\n",
      "[CV 5/5; 4/30] START C=10, max_iter=3000, penalty=l2............................\n",
      "[CV 5/5; 4/30] END C=10, max_iter=3000, penalty=l2;, score=0.925 total time= 6.4min\n",
      "[CV 2/5; 8/30] START C=10, max_iter=7000, penalty=l2............................\n",
      "[CV 2/5; 8/30] END C=10, max_iter=7000, penalty=l2;, score=0.951 total time=14.6min\n",
      "[CV 5/5; 10/30] START C=10, max_iter=9000, penalty=l2...........................\n",
      "[CV 5/5; 10/30] END C=10, max_iter=9000, penalty=l2;, score=0.950 total time=18.7min\n",
      "[CV 2/5; 18/30] START C=1, max_iter=7000, penalty=l2............................\n",
      "[CV 2/5; 18/30] END C=1, max_iter=7000, penalty=l2;, score=0.955 total time=14.5min\n",
      "[CV 5/5; 20/30] START C=1, max_iter=9000, penalty=l2............................\n",
      "[CV 5/5; 20/30] END C=1, max_iter=9000, penalty=l2;, score=0.938 total time=18.6min\n",
      "[CV 3/5; 28/30] START C=0.1, max_iter=7000, penalty=l2..........................\n",
      "[CV 3/5; 28/30] END C=0.1, max_iter=7000, penalty=l2;, score=0.946 total time=14.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/30] START C=10, max_iter=1000, penalty=l1............................\n",
      "[CV 3/5; 1/30] END C=10, max_iter=1000, penalty=l1;, score=nan total time=   0.7s\n",
      "[CV 5/5; 2/30] START C=10, max_iter=1000, penalty=l2............................\n",
      "[CV 5/5; 2/30] END C=10, max_iter=1000, penalty=l2;, score=0.874 total time= 2.3min\n",
      "[CV 4/5; 5/30] START C=10, max_iter=5000, penalty=l1............................\n",
      "[CV 4/5; 5/30] END C=10, max_iter=5000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 2/5; 6/30] START C=10, max_iter=5000, penalty=l2............................\n",
      "[CV 2/5; 6/30] END C=10, max_iter=5000, penalty=l2;, score=0.935 total time=10.6min\n",
      "[CV 1/5; 9/30] START C=10, max_iter=9000, penalty=l1............................\n",
      "[CV 1/5; 9/30] END C=10, max_iter=9000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 2/5; 9/30] START C=10, max_iter=9000, penalty=l1............................\n",
      "[CV 2/5; 9/30] END C=10, max_iter=9000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 3/5; 9/30] START C=10, max_iter=9000, penalty=l1............................\n",
      "[CV 3/5; 9/30] END C=10, max_iter=9000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 4/5; 9/30] START C=10, max_iter=9000, penalty=l1............................\n",
      "[CV 4/5; 9/30] END C=10, max_iter=9000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 5/5; 9/30] START C=10, max_iter=9000, penalty=l1............................\n",
      "[CV 5/5; 9/30] END C=10, max_iter=9000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 1/5; 10/30] START C=10, max_iter=9000, penalty=l2...........................\n",
      "[CV 1/5; 10/30] END C=10, max_iter=9000, penalty=l2;, score=0.949 total time=18.7min\n",
      "[CV 4/5; 14/30] START C=1, max_iter=3000, penalty=l2............................\n",
      "[CV 4/5; 14/30] END C=1, max_iter=3000, penalty=l2;, score=0.954 total time= 6.4min\n",
      "[CV 5/5; 16/30] START C=1, max_iter=5000, penalty=l2............................\n",
      "[CV 5/5; 16/30] END C=1, max_iter=5000, penalty=l2;, score=0.932 total time=10.6min\n",
      "[CV 3/5; 20/30] START C=1, max_iter=9000, penalty=l2............................\n",
      "[CV 3/5; 20/30] END C=1, max_iter=9000, penalty=l2;, score=0.922 total time=18.9min\n",
      "[CV 1/5; 25/30] START C=0.1, max_iter=5000, penalty=l1..........................\n",
      "[CV 1/5; 25/30] END C=0.1, max_iter=5000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 2/5; 25/30] START C=0.1, max_iter=5000, penalty=l1..........................\n",
      "[CV 2/5; 25/30] END C=0.1, max_iter=5000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 3/5; 25/30] START C=0.1, max_iter=5000, penalty=l1..........................\n",
      "[CV 3/5; 25/30] END C=0.1, max_iter=5000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 4/5; 25/30] START C=0.1, max_iter=5000, penalty=l1..........................\n",
      "[CV 4/5; 25/30] END C=0.1, max_iter=5000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 5/5; 25/30] START C=0.1, max_iter=5000, penalty=l1..........................\n",
      "[CV 5/5; 25/30] END C=0.1, max_iter=5000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 1/5; 26/30] START C=0.1, max_iter=5000, penalty=l2..........................\n",
      "[CV 1/5; 26/30] END C=0.1, max_iter=5000, penalty=l2;, score=0.939 total time=10.2min\n",
      "[CV 4/5; 28/30] START C=0.1, max_iter=7000, penalty=l2..........................\n",
      "[CV 4/5; 28/30] END C=0.1, max_iter=7000, penalty=l2;, score=0.957 total time=14.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/30] START C=10, max_iter=1000, penalty=l1............................\n",
      "[CV 2/5; 1/30] END C=10, max_iter=1000, penalty=l1;, score=nan total time=   0.7s\n",
      "[CV 4/5; 2/30] START C=10, max_iter=1000, penalty=l2............................\n",
      "[CV 4/5; 2/30] END C=10, max_iter=1000, penalty=l2;, score=0.917 total time= 2.3min\n",
      "[CV 3/5; 5/30] START C=10, max_iter=5000, penalty=l1............................\n",
      "[CV 3/5; 5/30] END C=10, max_iter=5000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 1/5; 6/30] START C=10, max_iter=5000, penalty=l2............................\n",
      "[CV 1/5; 6/30] END C=10, max_iter=5000, penalty=l2;, score=0.941 total time=10.4min\n",
      "[CV 4/5; 8/30] START C=10, max_iter=7000, penalty=l2............................\n",
      "[CV 4/5; 8/30] END C=10, max_iter=7000, penalty=l2;, score=0.942 total time=14.7min\n",
      "[CV 3/5; 12/30] START C=1, max_iter=1000, penalty=l2............................\n",
      "[CV 3/5; 12/30] END C=1, max_iter=1000, penalty=l2;, score=0.930 total time= 2.2min\n",
      "[CV 1/5; 13/30] START C=1, max_iter=3000, penalty=l1............................\n",
      "[CV 1/5; 13/30] END C=1, max_iter=3000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 2/5; 13/30] START C=1, max_iter=3000, penalty=l1............................\n",
      "[CV 2/5; 13/30] END C=1, max_iter=3000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 3/5; 13/30] START C=1, max_iter=3000, penalty=l1............................\n",
      "[CV 3/5; 13/30] END C=1, max_iter=3000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 4/5; 13/30] START C=1, max_iter=3000, penalty=l1............................\n",
      "[CV 4/5; 13/30] END C=1, max_iter=3000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 5/5; 13/30] START C=1, max_iter=3000, penalty=l1............................\n",
      "[CV 5/5; 13/30] END C=1, max_iter=3000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 1/5; 14/30] START C=1, max_iter=3000, penalty=l2............................\n",
      "[CV 1/5; 14/30] END C=1, max_iter=3000, penalty=l2;, score=0.932 total time= 6.3min\n",
      "[CV 3/5; 15/30] START C=1, max_iter=5000, penalty=l1............................\n",
      "[CV 3/5; 15/30] END C=1, max_iter=5000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 5/5; 15/30] START C=1, max_iter=5000, penalty=l1............................\n",
      "[CV 5/5; 15/30] END C=1, max_iter=5000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 2/5; 16/30] START C=1, max_iter=5000, penalty=l2............................\n",
      "[CV 2/5; 16/30] END C=1, max_iter=5000, penalty=l2;, score=0.914 total time=10.5min\n",
      "[CV 4/5; 18/30] START C=1, max_iter=7000, penalty=l2............................\n",
      "[CV 4/5; 18/30] END C=1, max_iter=7000, penalty=l2;, score=0.932 total time=14.6min\n",
      "[CV 3/5; 22/30] START C=0.1, max_iter=1000, penalty=l2..........................\n",
      "[CV 3/5; 22/30] END C=0.1, max_iter=1000, penalty=l2;, score=0.951 total time= 2.2min\n",
      "[CV 1/5; 23/30] START C=0.1, max_iter=3000, penalty=l1..........................\n",
      "[CV 1/5; 23/30] END C=0.1, max_iter=3000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 2/5; 23/30] START C=0.1, max_iter=3000, penalty=l1..........................\n",
      "[CV 2/5; 23/30] END C=0.1, max_iter=3000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 3/5; 23/30] START C=0.1, max_iter=3000, penalty=l1..........................\n",
      "[CV 3/5; 23/30] END C=0.1, max_iter=3000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 4/5; 23/30] START C=0.1, max_iter=3000, penalty=l1..........................\n",
      "[CV 4/5; 23/30] END C=0.1, max_iter=3000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 5/5; 23/30] START C=0.1, max_iter=3000, penalty=l1..........................\n",
      "[CV 5/5; 23/30] END C=0.1, max_iter=3000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 1/5; 24/30] START C=0.1, max_iter=3000, penalty=l2..........................\n",
      "[CV 1/5; 24/30] END C=0.1, max_iter=3000, penalty=l2;, score=0.933 total time= 6.2min\n",
      "[CV 2/5; 26/30] START C=0.1, max_iter=5000, penalty=l2..........................\n",
      "[CV 2/5; 26/30] END C=0.1, max_iter=5000, penalty=l2;, score=0.947 total time=10.3min\n",
      "[CV 5/5; 28/30] START C=0.1, max_iter=7000, penalty=l2..........................\n",
      "[CV 5/5; 28/30] END C=0.1, max_iter=7000, penalty=l2;, score=0.959 total time=13.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "75 fits failed out of a total of 150.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.88735694        nan 0.93036586        nan 0.93243807\n",
      "        nan 0.92187243        nan 0.95176748        nan 0.94441675\n",
      "        nan 0.93486718        nan 0.92729416        nan 0.94421818\n",
      "        nan 0.94311484        nan 0.93936005        nan 0.94337685\n",
      "        nan 0.94509132        nan 0.9536965         nan 0.95109269]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/30] START C=10, max_iter=1000, penalty=l1............................\n",
      "[CV 4/5; 1/30] END C=10, max_iter=1000, penalty=l1;, score=nan total time=   0.7s\n",
      "[CV 3/5; 3/30] START C=10, max_iter=3000, penalty=l1............................\n",
      "[CV 3/5; 3/30] END C=10, max_iter=3000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 1/5; 4/30] START C=10, max_iter=3000, penalty=l2............................\n",
      "[CV 1/5; 4/30] END C=10, max_iter=3000, penalty=l2;, score=0.947 total time= 6.5min\n",
      "[CV 4/5; 6/30] START C=10, max_iter=5000, penalty=l2............................\n",
      "[CV 4/5; 6/30] END C=10, max_iter=5000, penalty=l2;, score=0.944 total time=10.6min\n",
      "[CV 3/5; 10/30] START C=10, max_iter=9000, penalty=l2...........................\n",
      "[CV 3/5; 10/30] END C=10, max_iter=9000, penalty=l2;, score=0.958 total time=18.8min\n",
      "[CV 1/5; 15/30] START C=1, max_iter=5000, penalty=l1............................\n",
      "[CV 1/5; 15/30] END C=1, max_iter=5000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 2/5; 15/30] START C=1, max_iter=5000, penalty=l1............................\n",
      "[CV 2/5; 15/30] END C=1, max_iter=5000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 4/5; 15/30] START C=1, max_iter=5000, penalty=l1............................\n",
      "[CV 4/5; 15/30] END C=1, max_iter=5000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 1/5; 16/30] START C=1, max_iter=5000, penalty=l2............................\n",
      "[CV 1/5; 16/30] END C=1, max_iter=5000, penalty=l2;, score=0.923 total time=10.5min\n",
      "[CV 5/5; 18/30] START C=1, max_iter=7000, penalty=l2............................\n",
      "[CV 5/5; 18/30] END C=1, max_iter=7000, penalty=l2;, score=0.953 total time=14.7min\n",
      "[CV 4/5; 22/30] START C=0.1, max_iter=1000, penalty=l2..........................\n",
      "[CV 4/5; 22/30] END C=0.1, max_iter=1000, penalty=l2;, score=0.950 total time= 2.2min\n",
      "[CV 2/5; 24/30] START C=0.1, max_iter=3000, penalty=l2..........................\n",
      "[CV 2/5; 24/30] END C=0.1, max_iter=3000, penalty=l2;, score=0.952 total time= 6.3min\n",
      "[CV 3/5; 26/30] START C=0.1, max_iter=5000, penalty=l2..........................\n",
      "[CV 3/5; 26/30] END C=0.1, max_iter=5000, penalty=l2;, score=0.961 total time=10.3min\n",
      "[CV 1/5; 29/30] START C=0.1, max_iter=9000, penalty=l1..........................\n",
      "[CV 1/5; 29/30] END C=0.1, max_iter=9000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 2/5; 29/30] START C=0.1, max_iter=9000, penalty=l1..........................\n",
      "[CV 2/5; 29/30] END C=0.1, max_iter=9000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 3/5; 29/30] START C=0.1, max_iter=9000, penalty=l1..........................\n",
      "[CV 3/5; 29/30] END C=0.1, max_iter=9000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 4/5; 29/30] START C=0.1, max_iter=9000, penalty=l1..........................\n",
      "[CV 4/5; 29/30] END C=0.1, max_iter=9000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 5/5; 29/30] START C=0.1, max_iter=9000, penalty=l1..........................\n",
      "[CV 5/5; 29/30] END C=0.1, max_iter=9000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 1/5; 30/30] START C=0.1, max_iter=9000, penalty=l2..........................\n",
      "[CV 1/5; 30/30] END C=0.1, max_iter=9000, penalty=l2;, score=0.959 total time=16.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/30] START C=10, max_iter=1000, penalty=l1............................\n",
      "[CV 1/5; 1/30] END C=10, max_iter=1000, penalty=l1;, score=nan total time=   0.7s\n",
      "[CV 1/5; 3/30] START C=10, max_iter=3000, penalty=l1............................\n",
      "[CV 1/5; 3/30] END C=10, max_iter=3000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 4/5; 3/30] START C=10, max_iter=3000, penalty=l1............................\n",
      "[CV 4/5; 3/30] END C=10, max_iter=3000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 2/5; 4/30] START C=10, max_iter=3000, penalty=l2............................\n",
      "[CV 2/5; 4/30] END C=10, max_iter=3000, penalty=l2;, score=0.936 total time= 6.5min\n",
      "[CV 5/5; 6/30] START C=10, max_iter=5000, penalty=l2............................\n",
      "[CV 5/5; 6/30] END C=10, max_iter=5000, penalty=l2;, score=0.942 total time=10.6min\n",
      "[CV 2/5; 10/30] START C=10, max_iter=9000, penalty=l2...........................\n",
      "[CV 2/5; 10/30] END C=10, max_iter=9000, penalty=l2;, score=0.944 total time=18.8min\n",
      "[CV 5/5; 14/30] START C=1, max_iter=3000, penalty=l2............................\n",
      "[CV 5/5; 14/30] END C=1, max_iter=3000, penalty=l2;, score=0.939 total time= 6.3min\n",
      "[CV 3/5; 18/30] START C=1, max_iter=7000, penalty=l2............................\n",
      "[CV 3/5; 18/30] END C=1, max_iter=7000, penalty=l2;, score=0.940 total time=14.6min\n",
      "[CV 1/5; 21/30] START C=0.1, max_iter=1000, penalty=l1..........................\n",
      "[CV 1/5; 21/30] END C=0.1, max_iter=1000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 2/5; 21/30] START C=0.1, max_iter=1000, penalty=l1..........................\n",
      "[CV 2/5; 21/30] END C=0.1, max_iter=1000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 3/5; 21/30] START C=0.1, max_iter=1000, penalty=l1..........................\n",
      "[CV 3/5; 21/30] END C=0.1, max_iter=1000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 4/5; 21/30] START C=0.1, max_iter=1000, penalty=l1..........................\n",
      "[CV 4/5; 21/30] END C=0.1, max_iter=1000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 5/5; 21/30] START C=0.1, max_iter=1000, penalty=l1..........................\n",
      "[CV 5/5; 21/30] END C=0.1, max_iter=1000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 1/5; 22/30] START C=0.1, max_iter=1000, penalty=l2..........................\n",
      "[CV 1/5; 22/30] END C=0.1, max_iter=1000, penalty=l2;, score=0.947 total time= 2.2min\n",
      "[CV 2/5; 22/30] START C=0.1, max_iter=1000, penalty=l2..........................\n",
      "[CV 2/5; 22/30] END C=0.1, max_iter=1000, penalty=l2;, score=0.917 total time= 2.2min\n",
      "[CV 5/5; 22/30] START C=0.1, max_iter=1000, penalty=l2..........................\n",
      "[CV 5/5; 22/30] END C=0.1, max_iter=1000, penalty=l2;, score=0.932 total time= 2.1min\n",
      "[CV 3/5; 24/30] START C=0.1, max_iter=3000, penalty=l2..........................\n",
      "[CV 3/5; 24/30] END C=0.1, max_iter=3000, penalty=l2;, score=0.934 total time= 6.2min\n",
      "[CV 4/5; 26/30] START C=0.1, max_iter=5000, penalty=l2..........................\n",
      "[CV 4/5; 26/30] END C=0.1, max_iter=5000, penalty=l2;, score=0.925 total time=10.3min\n",
      "[CV 2/5; 30/30] START C=0.1, max_iter=9000, penalty=l2..........................\n",
      "[CV 2/5; 30/30] END C=0.1, max_iter=9000, penalty=l2;, score=0.944 total time=16.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 2/30] START C=10, max_iter=1000, penalty=l2............................\n",
      "[CV 2/5; 2/30] END C=10, max_iter=1000, penalty=l2;, score=0.871 total time= 2.3min\n",
      "[CV 1/5; 5/30] START C=10, max_iter=5000, penalty=l1............................\n",
      "[CV 1/5; 5/30] END C=10, max_iter=5000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 2/5; 5/30] START C=10, max_iter=5000, penalty=l1............................\n",
      "[CV 2/5; 5/30] END C=10, max_iter=5000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 5/5; 5/30] START C=10, max_iter=5000, penalty=l1............................\n",
      "[CV 5/5; 5/30] END C=10, max_iter=5000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 3/5; 6/30] START C=10, max_iter=5000, penalty=l2............................\n",
      "[CV 3/5; 6/30] END C=10, max_iter=5000, penalty=l2;, score=0.900 total time=10.6min\n",
      "[CV 5/5; 8/30] START C=10, max_iter=7000, penalty=l2............................\n",
      "[CV 5/5; 8/30] END C=10, max_iter=7000, penalty=l2;, score=0.938 total time=14.7min\n",
      "[CV 4/5; 12/30] START C=1, max_iter=1000, penalty=l2............................\n",
      "[CV 4/5; 12/30] END C=1, max_iter=1000, penalty=l2;, score=0.953 total time= 2.2min\n",
      "[CV 2/5; 14/30] START C=1, max_iter=3000, penalty=l2............................\n",
      "[CV 2/5; 14/30] END C=1, max_iter=3000, penalty=l2;, score=0.901 total time= 6.4min\n",
      "[CV 3/5; 16/30] START C=1, max_iter=5000, penalty=l2............................\n",
      "[CV 3/5; 16/30] END C=1, max_iter=5000, penalty=l2;, score=0.917 total time=10.5min\n",
      "[CV 1/5; 19/30] START C=1, max_iter=9000, penalty=l1............................\n",
      "[CV 1/5; 19/30] END C=1, max_iter=9000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 2/5; 19/30] START C=1, max_iter=9000, penalty=l1............................\n",
      "[CV 2/5; 19/30] END C=1, max_iter=9000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 3/5; 19/30] START C=1, max_iter=9000, penalty=l1............................\n",
      "[CV 3/5; 19/30] END C=1, max_iter=9000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 4/5; 19/30] START C=1, max_iter=9000, penalty=l1............................\n",
      "[CV 4/5; 19/30] END C=1, max_iter=9000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 5/5; 19/30] START C=1, max_iter=9000, penalty=l1............................\n",
      "[CV 5/5; 19/30] END C=1, max_iter=9000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 1/5; 20/30] START C=1, max_iter=9000, penalty=l2............................\n",
      "[CV 1/5; 20/30] END C=1, max_iter=9000, penalty=l2;, score=0.956 total time=18.8min\n",
      "[CV 4/5; 24/30] START C=0.1, max_iter=3000, penalty=l2..........................\n",
      "[CV 4/5; 24/30] END C=0.1, max_iter=3000, penalty=l2;, score=0.952 total time= 6.2min\n",
      "[CV 5/5; 26/30] START C=0.1, max_iter=5000, penalty=l2..........................\n",
      "[CV 5/5; 26/30] END C=0.1, max_iter=5000, penalty=l2;, score=0.954 total time=10.4min\n",
      "[CV 3/5; 30/30] START C=0.1, max_iter=9000, penalty=l2..........................\n",
      "[CV 3/5; 30/30] END C=0.1, max_iter=9000, penalty=l2;, score=0.951 total time=15.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 2/30] START C=10, max_iter=1000, penalty=l2............................\n",
      "[CV 3/5; 2/30] END C=10, max_iter=1000, penalty=l2;, score=0.868 total time= 2.3min\n",
      "[CV 4/5; 4/30] START C=10, max_iter=3000, penalty=l2............................\n",
      "[CV 4/5; 4/30] END C=10, max_iter=3000, penalty=l2;, score=0.914 total time= 6.4min\n",
      "[CV 3/5; 8/30] START C=10, max_iter=7000, penalty=l2............................\n",
      "[CV 3/5; 8/30] END C=10, max_iter=7000, penalty=l2;, score=0.834 total time=14.7min\n",
      "[CV 1/5; 11/30] START C=1, max_iter=1000, penalty=l1............................\n",
      "[CV 1/5; 11/30] END C=1, max_iter=1000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 2/5; 11/30] START C=1, max_iter=1000, penalty=l1............................\n",
      "[CV 2/5; 11/30] END C=1, max_iter=1000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 3/5; 11/30] START C=1, max_iter=1000, penalty=l1............................\n",
      "[CV 3/5; 11/30] END C=1, max_iter=1000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 4/5; 11/30] START C=1, max_iter=1000, penalty=l1............................\n",
      "[CV 4/5; 11/30] END C=1, max_iter=1000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 5/5; 11/30] START C=1, max_iter=1000, penalty=l1............................\n",
      "[CV 5/5; 11/30] END C=1, max_iter=1000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 1/5; 12/30] START C=1, max_iter=1000, penalty=l2............................\n",
      "[CV 1/5; 12/30] END C=1, max_iter=1000, penalty=l2;, score=0.944 total time= 2.2min\n",
      "[CV 2/5; 12/30] START C=1, max_iter=1000, penalty=l2............................\n",
      "[CV 2/5; 12/30] END C=1, max_iter=1000, penalty=l2;, score=0.948 total time= 2.2min\n",
      "[CV 5/5; 12/30] START C=1, max_iter=1000, penalty=l2............................\n",
      "[CV 5/5; 12/30] END C=1, max_iter=1000, penalty=l2;, score=0.948 total time= 2.2min\n",
      "[CV 3/5; 14/30] START C=1, max_iter=3000, penalty=l2............................\n",
      "[CV 3/5; 14/30] END C=1, max_iter=3000, penalty=l2;, score=0.949 total time= 6.4min\n",
      "[CV 4/5; 16/30] START C=1, max_iter=5000, penalty=l2............................\n",
      "[CV 4/5; 16/30] END C=1, max_iter=5000, penalty=l2;, score=0.950 total time=10.6min\n",
      "[CV 2/5; 20/30] START C=1, max_iter=9000, penalty=l2............................\n",
      "[CV 2/5; 20/30] END C=1, max_iter=9000, penalty=l2;, score=0.946 total time=18.8min\n",
      "[CV 5/5; 24/30] START C=0.1, max_iter=3000, penalty=l2..........................\n",
      "[CV 5/5; 24/30] END C=0.1, max_iter=3000, penalty=l2;, score=0.947 total time= 6.2min\n",
      "[CV 1/5; 27/30] START C=0.1, max_iter=7000, penalty=l1..........................\n",
      "[CV 1/5; 27/30] END C=0.1, max_iter=7000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 2/5; 27/30] START C=0.1, max_iter=7000, penalty=l1..........................\n",
      "[CV 2/5; 27/30] END C=0.1, max_iter=7000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 3/5; 27/30] START C=0.1, max_iter=7000, penalty=l1..........................\n",
      "[CV 3/5; 27/30] END C=0.1, max_iter=7000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 4/5; 27/30] START C=0.1, max_iter=7000, penalty=l1..........................\n",
      "[CV 4/5; 27/30] END C=0.1, max_iter=7000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 5/5; 27/30] START C=0.1, max_iter=7000, penalty=l1..........................\n",
      "[CV 5/5; 27/30] END C=0.1, max_iter=7000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 1/5; 28/30] START C=0.1, max_iter=7000, penalty=l2..........................\n",
      "[CV 1/5; 28/30] END C=0.1, max_iter=7000, penalty=l2;, score=0.948 total time=14.3min\n",
      "[CV 4/5; 30/30] START C=0.1, max_iter=9000, penalty=l2..........................\n",
      "[CV 4/5; 30/30] END C=0.1, max_iter=9000, penalty=l2;, score=0.959 total time=13.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/30] START C=10, max_iter=1000, penalty=l1............................\n",
      "[CV 5/5; 1/30] END C=10, max_iter=1000, penalty=l1;, score=nan total time=   0.8s\n",
      "[CV 2/5; 3/30] START C=10, max_iter=3000, penalty=l1............................\n",
      "[CV 2/5; 3/30] END C=10, max_iter=3000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 5/5; 3/30] START C=10, max_iter=3000, penalty=l1............................\n",
      "[CV 5/5; 3/30] END C=10, max_iter=3000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 3/5; 4/30] START C=10, max_iter=3000, penalty=l2............................\n",
      "[CV 3/5; 4/30] END C=10, max_iter=3000, penalty=l2;, score=0.930 total time= 6.5min\n",
      "[CV 1/5; 7/30] START C=10, max_iter=7000, penalty=l1............................\n",
      "[CV 1/5; 7/30] END C=10, max_iter=7000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 2/5; 7/30] START C=10, max_iter=7000, penalty=l1............................\n",
      "[CV 2/5; 7/30] END C=10, max_iter=7000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 3/5; 7/30] START C=10, max_iter=7000, penalty=l1............................\n",
      "[CV 3/5; 7/30] END C=10, max_iter=7000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 4/5; 7/30] START C=10, max_iter=7000, penalty=l1............................\n",
      "[CV 4/5; 7/30] END C=10, max_iter=7000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 5/5; 7/30] START C=10, max_iter=7000, penalty=l1............................\n",
      "[CV 5/5; 7/30] END C=10, max_iter=7000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 1/5; 8/30] START C=10, max_iter=7000, penalty=l2............................\n",
      "[CV 1/5; 8/30] END C=10, max_iter=7000, penalty=l2;, score=0.944 total time=14.5min\n",
      "[CV 4/5; 10/30] START C=10, max_iter=9000, penalty=l2...........................\n",
      "[CV 4/5; 10/30] END C=10, max_iter=9000, penalty=l2;, score=0.958 total time=18.9min\n",
      "[CV 1/5; 17/30] START C=1, max_iter=7000, penalty=l1............................\n",
      "[CV 1/5; 17/30] END C=1, max_iter=7000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 2/5; 17/30] START C=1, max_iter=7000, penalty=l1............................\n",
      "[CV 2/5; 17/30] END C=1, max_iter=7000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 3/5; 17/30] START C=1, max_iter=7000, penalty=l1............................\n",
      "[CV 3/5; 17/30] END C=1, max_iter=7000, penalty=l1;, score=nan total time=   0.4s\n",
      "[CV 4/5; 17/30] START C=1, max_iter=7000, penalty=l1............................\n",
      "[CV 4/5; 17/30] END C=1, max_iter=7000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 5/5; 17/30] START C=1, max_iter=7000, penalty=l1............................\n",
      "[CV 5/5; 17/30] END C=1, max_iter=7000, penalty=l1;, score=nan total time=   0.5s\n",
      "[CV 1/5; 18/30] START C=1, max_iter=7000, penalty=l2............................\n",
      "[CV 1/5; 18/30] END C=1, max_iter=7000, penalty=l2;, score=0.941 total time=14.6min\n",
      "[CV 4/5; 20/30] START C=1, max_iter=9000, penalty=l2............................\n",
      "[CV 4/5; 20/30] END C=1, max_iter=9000, penalty=l2;, score=0.953 total time=18.8min\n",
      "[CV 2/5; 28/30] START C=0.1, max_iter=7000, penalty=l2..........................\n",
      "[CV 2/5; 28/30] END C=0.1, max_iter=7000, penalty=l2;, score=0.959 total time=14.5min\n",
      "[CV 5/5; 30/30] START C=0.1, max_iter=9000, penalty=l2..........................\n",
      "[CV 5/5; 30/30] END C=0.1, max_iter=9000, penalty=l2;, score=0.942 total time=12.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      " {'C': 0.1, 'max_iter': 7000, 'penalty': 'l2'}\n",
      "total accuracy: 0.7031582682753726\n",
      "dos accuracy: 0.6899973183159024\n",
      "normal accuracy 0.9269900113273607\n",
      "probe accuracy 0.7026022304832714\n",
      "r2l accuracy 0.0011746280344557558\n",
      "u2r accuracy 0.0\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         dos       0.96      0.69      0.80      7458\n",
      "      normal       0.62      0.93      0.74      9711\n",
      "       probe       0.65      0.70      0.67      2421\n",
      "         r2l       0.60      0.00      0.00      2554\n",
      "         u2r       0.00      0.00      0.00       400\n",
      "\n",
      "    accuracy                           0.70     22544\n",
      "   macro avg       0.56      0.46      0.44     22544\n",
      "weighted avg       0.72      0.70      0.66     22544\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SVM Classifier\n",
    "svc_params = {'penalty' : ['l1', 'l2'],\n",
    "             'C' : [10, 1, 0.1],\n",
    "             'max_iter' : [1000, 3000, 5000, 7000, 9000]}\n",
    "\n",
    "svc_clf = LinearSVC()\n",
    "svc_gscv = GridSearchCV(svc_clf, svc_params, n_jobs=-1, verbose=10)\n",
    "\n",
    "svc_gscv.fit(x_train, y_train)\n",
    "best_svc_clf = svc_gscv.best_estimator_\n",
    "best_svc_params = svc_gscv.best_params_\n",
    "\n",
    "dump(best_svc_clf, 'best_svc_clf.joblib')\n",
    "\n",
    "print(\"Best Hyperparameters:\\n\", best_svc_params)\n",
    "y_pred = best_svc_clf.predict(x_test)\n",
    "print(\"total accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(\"dos accuracy:\",result[0][0]/sum(result[0]))\n",
    "print(\"normal accuracy\",result[1][1]/sum(result[1]))\n",
    "print(\"probe accuracy\",result[2][2]/sum(result[2]))\n",
    "print(\"r2l accuracy\",result[3][3]/sum(result[3]))\n",
    "print(\"u2r accuracy\",result[4][4]/sum(result[4]))\n",
    "\n",
    "plot_confusion_matrix(best_svc_clf, x_test, y_test)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ....................... (1 of 4) Processing dt, total=   0.7s\n",
      "[Voting] ....................... (2 of 4) Processing rf, total=  18.6s\n",
      "[Voting] ...................... (4 of 4) Processing svc, total= 9.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total accuracy: 0.745431156848829\n",
      "dos accuracy: 0.7841244301421293\n",
      "normal accuracy 0.9714756461744414\n",
      "probe accuracy 0.6195786864931846\n",
      "r2l accuracy 0.009005481597494126\n",
      "u2r accuracy 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         dos       0.96      0.78      0.86      7458\n",
      "      normal       0.64      0.97      0.77      9711\n",
      "       probe       0.86      0.62      0.72      2421\n",
      "         r2l       1.00      0.01      0.02      2554\n",
      "         u2r       0.00      0.00      0.00       400\n",
      "\n",
      "    accuracy                           0.75     22544\n",
      "   macro avg       0.69      0.48      0.47     22544\n",
      "weighted avg       0.80      0.75      0.70     22544\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ....................... (3 of 4) Processing gb, total=17.3min\n"
     ]
    }
   ],
   "source": [
    "clf_list = [('dt', best_dt_clf), ('rf', best_rf_clf), ('gb', best_gb_clf), ('svc', best_svc_clf)]\n",
    "vote_clf = VotingClassifier(estimators = clf_list, n_jobs=-1, verbose=10)\n",
    "vote_clf.fit(x_train, y_train)\n",
    "\n",
    "dump(vote_clf, 'vote_clf.joblib')\n",
    "\n",
    "y_pred = vote_clf.predict(x_test)\n",
    "print(\"total accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(\"dos accuracy:\",result[0][0]/sum(result[0]))\n",
    "print(\"normal accuracy\",result[1][1]/sum(result[1]))\n",
    "print(\"probe accuracy\",result[2][2]/sum(result[2]))\n",
    "print(\"r2l accuracy\",result[3][3]/sum(result[3]))\n",
    "print(\"u2r accuracy\",result[4][4]/sum(result[4]))\n",
    "\n",
    "plot_confusion_matrix(vote_clf, x_test, y_test)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 20:16:25.015415: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-12 20:16:25.016097: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-12 20:16:25.154728: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1969/1969 [==============================] - 3s 1ms/step - loss: 0.6377 - accuracy: 0.8185 - val_loss: 1.6373 - val_accuracy: 0.6741\n",
      "Epoch 2/200\n",
      "1969/1969 [==============================] - 2s 976us/step - loss: 0.1848 - accuracy: 0.9470 - val_loss: 1.5156 - val_accuracy: 0.6882\n",
      "Epoch 3/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.1430 - accuracy: 0.9583 - val_loss: 1.7345 - val_accuracy: 0.6960\n",
      "Epoch 4/200\n",
      "1969/1969 [==============================] - 2s 983us/step - loss: 0.1262 - accuracy: 0.9653 - val_loss: 1.6964 - val_accuracy: 0.6888\n",
      "Epoch 5/200\n",
      "1969/1969 [==============================] - 2s 976us/step - loss: 0.1127 - accuracy: 0.9683 - val_loss: 1.5514 - val_accuracy: 0.6917\n",
      "Epoch 6/200\n",
      "1969/1969 [==============================] - 2s 986us/step - loss: 0.1195 - accuracy: 0.9660 - val_loss: 1.7517 - val_accuracy: 0.7017\n",
      "Epoch 7/200\n",
      "1969/1969 [==============================] - 2s 983us/step - loss: 0.1068 - accuracy: 0.9702 - val_loss: 1.5601 - val_accuracy: 0.7065\n",
      "Epoch 8/200\n",
      "1969/1969 [==============================] - 2s 982us/step - loss: 0.0999 - accuracy: 0.9711 - val_loss: 1.7065 - val_accuracy: 0.7035\n",
      "Epoch 9/200\n",
      "1969/1969 [==============================] - 2s 990us/step - loss: 0.0914 - accuracy: 0.9739 - val_loss: 1.7487 - val_accuracy: 0.6988\n",
      "Epoch 10/200\n",
      "1969/1969 [==============================] - 2s 972us/step - loss: 0.0954 - accuracy: 0.9729 - val_loss: 1.7890 - val_accuracy: 0.7165\n",
      "Epoch 11/200\n",
      "1969/1969 [==============================] - 2s 971us/step - loss: 0.0923 - accuracy: 0.9738 - val_loss: 1.7918 - val_accuracy: 0.7203\n",
      "Epoch 12/200\n",
      "1969/1969 [==============================] - 2s 975us/step - loss: 0.0832 - accuracy: 0.9764 - val_loss: 1.6113 - val_accuracy: 0.7056\n",
      "Epoch 13/200\n",
      "1969/1969 [==============================] - 2s 974us/step - loss: 0.0760 - accuracy: 0.9789 - val_loss: 1.7417 - val_accuracy: 0.7055\n",
      "Epoch 14/200\n",
      "1969/1969 [==============================] - 2s 986us/step - loss: 0.0830 - accuracy: 0.9771 - val_loss: 1.8365 - val_accuracy: 0.7131\n",
      "Epoch 15/200\n",
      "1969/1969 [==============================] - 2s 974us/step - loss: 0.0791 - accuracy: 0.9772 - val_loss: 1.7696 - val_accuracy: 0.7135\n",
      "Epoch 16/200\n",
      "1969/1969 [==============================] - 2s 988us/step - loss: 0.0781 - accuracy: 0.9766 - val_loss: 1.9335 - val_accuracy: 0.7138\n",
      "Epoch 17/200\n",
      "1969/1969 [==============================] - 2s 990us/step - loss: 0.0736 - accuracy: 0.9786 - val_loss: 2.0113 - val_accuracy: 0.7170\n",
      "Epoch 18/200\n",
      "1969/1969 [==============================] - 2s 997us/step - loss: 0.0711 - accuracy: 0.9804 - val_loss: 1.9654 - val_accuracy: 0.7095\n",
      "Epoch 19/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0713 - accuracy: 0.9805 - val_loss: 1.8170 - val_accuracy: 0.7312\n",
      "Epoch 20/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0634 - accuracy: 0.9826 - val_loss: 1.8380 - val_accuracy: 0.7189\n",
      "Epoch 21/200\n",
      "1969/1969 [==============================] - 2s 983us/step - loss: 0.0608 - accuracy: 0.9823 - val_loss: 1.8596 - val_accuracy: 0.7205\n",
      "Epoch 22/200\n",
      "1969/1969 [==============================] - 2s 973us/step - loss: 0.0654 - accuracy: 0.9814 - val_loss: 1.9272 - val_accuracy: 0.7117\n",
      "Epoch 23/200\n",
      "1969/1969 [==============================] - 2s 968us/step - loss: 0.0651 - accuracy: 0.9813 - val_loss: 1.7855 - val_accuracy: 0.7169\n",
      "Epoch 24/200\n",
      "1969/1969 [==============================] - 2s 967us/step - loss: 0.0671 - accuracy: 0.9812 - val_loss: 1.6967 - val_accuracy: 0.7279\n",
      "Epoch 25/200\n",
      "1969/1969 [==============================] - 2s 986us/step - loss: 0.0650 - accuracy: 0.9822 - val_loss: 1.7462 - val_accuracy: 0.7225\n",
      "Epoch 26/200\n",
      "1969/1969 [==============================] - 2s 989us/step - loss: 0.0613 - accuracy: 0.9832 - val_loss: 1.8604 - val_accuracy: 0.7178\n",
      "Epoch 27/200\n",
      "1969/1969 [==============================] - 2s 988us/step - loss: 0.0625 - accuracy: 0.9830 - val_loss: 1.6784 - val_accuracy: 0.7141\n",
      "Epoch 28/200\n",
      "1969/1969 [==============================] - 2s 994us/step - loss: 0.0614 - accuracy: 0.9836 - val_loss: 1.8387 - val_accuracy: 0.7181\n",
      "Epoch 29/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0609 - accuracy: 0.9834 - val_loss: 1.9054 - val_accuracy: 0.7224\n",
      "Epoch 30/200\n",
      "1969/1969 [==============================] - 2s 987us/step - loss: 0.0590 - accuracy: 0.9826 - val_loss: 1.7447 - val_accuracy: 0.7106\n",
      "Epoch 31/200\n",
      "1969/1969 [==============================] - 2s 983us/step - loss: 0.0603 - accuracy: 0.9834 - val_loss: 1.8616 - val_accuracy: 0.7279\n",
      "Epoch 32/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0656 - accuracy: 0.9821 - val_loss: 1.7315 - val_accuracy: 0.7265\n",
      "Epoch 33/200\n",
      "1969/1969 [==============================] - 2s 993us/step - loss: 0.0608 - accuracy: 0.9832 - val_loss: 1.9622 - val_accuracy: 0.7174\n",
      "Epoch 34/200\n",
      "1969/1969 [==============================] - 2s 998us/step - loss: 0.0596 - accuracy: 0.9830 - val_loss: 2.1797 - val_accuracy: 0.7130\n",
      "Epoch 35/200\n",
      "1969/1969 [==============================] - 2s 979us/step - loss: 0.0607 - accuracy: 0.9835 - val_loss: 2.0066 - val_accuracy: 0.7222\n",
      "Epoch 36/200\n",
      "1969/1969 [==============================] - 2s 983us/step - loss: 0.0567 - accuracy: 0.9842 - val_loss: 1.7989 - val_accuracy: 0.7269\n",
      "Epoch 37/200\n",
      "1969/1969 [==============================] - 2s 990us/step - loss: 0.0555 - accuracy: 0.9847 - val_loss: 1.9934 - val_accuracy: 0.7277\n",
      "Epoch 38/200\n",
      "1969/1969 [==============================] - 2s 976us/step - loss: 0.0559 - accuracy: 0.9843 - val_loss: 1.8742 - val_accuracy: 0.7229\n",
      "Epoch 39/200\n",
      "1969/1969 [==============================] - 2s 979us/step - loss: 0.0543 - accuracy: 0.9848 - val_loss: 1.7543 - val_accuracy: 0.7224\n",
      "Epoch 40/200\n",
      "1969/1969 [==============================] - 2s 980us/step - loss: 0.0558 - accuracy: 0.9840 - val_loss: 1.8535 - val_accuracy: 0.7264\n",
      "Epoch 41/200\n",
      "1969/1969 [==============================] - 2s 979us/step - loss: 0.0548 - accuracy: 0.9852 - val_loss: 1.8457 - val_accuracy: 0.7076\n",
      "Epoch 42/200\n",
      "1969/1969 [==============================] - 2s 974us/step - loss: 0.0533 - accuracy: 0.9850 - val_loss: 1.8538 - val_accuracy: 0.7264\n",
      "Epoch 43/200\n",
      "1969/1969 [==============================] - 2s 983us/step - loss: 0.0521 - accuracy: 0.9852 - val_loss: 1.8660 - val_accuracy: 0.7202\n",
      "Epoch 44/200\n",
      "1969/1969 [==============================] - 2s 983us/step - loss: 0.0544 - accuracy: 0.9854 - val_loss: 1.9924 - val_accuracy: 0.7122\n",
      "Epoch 45/200\n",
      "1969/1969 [==============================] - 2s 980us/step - loss: 0.0572 - accuracy: 0.9836 - val_loss: 1.8130 - val_accuracy: 0.7278\n",
      "Epoch 46/200\n",
      "1969/1969 [==============================] - 2s 965us/step - loss: 0.0556 - accuracy: 0.9845 - val_loss: 1.8666 - val_accuracy: 0.7222\n",
      "Epoch 47/200\n",
      "1969/1969 [==============================] - 2s 967us/step - loss: 0.0551 - accuracy: 0.9848 - val_loss: 1.8824 - val_accuracy: 0.7243\n",
      "Epoch 48/200\n",
      "1969/1969 [==============================] - 2s 988us/step - loss: 0.0539 - accuracy: 0.9857 - val_loss: 1.8431 - val_accuracy: 0.7226\n",
      "Epoch 49/200\n",
      "1969/1969 [==============================] - 2s 977us/step - loss: 0.0562 - accuracy: 0.9839 - val_loss: 1.8022 - val_accuracy: 0.7245\n",
      "Epoch 50/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0505 - accuracy: 0.9859 - val_loss: 1.9111 - val_accuracy: 0.7243\n",
      "Epoch 51/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0550 - accuracy: 0.9845 - val_loss: 1.9214 - val_accuracy: 0.7138\n",
      "Epoch 52/200\n",
      "1969/1969 [==============================] - 2s 993us/step - loss: 0.0507 - accuracy: 0.9856 - val_loss: 2.0092 - val_accuracy: 0.7168\n",
      "Epoch 53/200\n",
      "1969/1969 [==============================] - 2s 970us/step - loss: 0.0587 - accuracy: 0.9844 - val_loss: 1.8653 - val_accuracy: 0.7292\n",
      "Epoch 54/200\n",
      "1969/1969 [==============================] - 2s 971us/step - loss: 0.0543 - accuracy: 0.9850 - val_loss: 1.9044 - val_accuracy: 0.7239\n",
      "Epoch 55/200\n",
      "1969/1969 [==============================] - 2s 987us/step - loss: 0.0579 - accuracy: 0.9849 - val_loss: 2.0871 - val_accuracy: 0.7139\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969/1969 [==============================] - 2s 938us/step - loss: 0.0526 - accuracy: 0.9854 - val_loss: 1.9228 - val_accuracy: 0.7242\n",
      "Epoch 57/200\n",
      "1969/1969 [==============================] - 2s 940us/step - loss: 0.0559 - accuracy: 0.9850 - val_loss: 1.9372 - val_accuracy: 0.7202\n",
      "Epoch 58/200\n",
      "1969/1969 [==============================] - 2s 929us/step - loss: 0.0542 - accuracy: 0.9854 - val_loss: 1.9027 - val_accuracy: 0.7083\n",
      "Epoch 59/200\n",
      "1969/1969 [==============================] - 2s 933us/step - loss: 0.0468 - accuracy: 0.9872 - val_loss: 1.9755 - val_accuracy: 0.7282\n",
      "Epoch 60/200\n",
      "1969/1969 [==============================] - 2s 942us/step - loss: 0.0536 - accuracy: 0.9861 - val_loss: 2.2729 - val_accuracy: 0.7204\n",
      "Epoch 61/200\n",
      "1969/1969 [==============================] - 2s 932us/step - loss: 0.0512 - accuracy: 0.9858 - val_loss: 2.0195 - val_accuracy: 0.7246\n",
      "Epoch 62/200\n",
      "1969/1969 [==============================] - 2s 936us/step - loss: 0.0501 - accuracy: 0.9858 - val_loss: 1.9075 - val_accuracy: 0.7312\n",
      "Epoch 63/200\n",
      "1969/1969 [==============================] - 2s 942us/step - loss: 0.0524 - accuracy: 0.9862 - val_loss: 1.9157 - val_accuracy: 0.7233\n",
      "Epoch 64/200\n",
      "1969/1969 [==============================] - 2s 957us/step - loss: 0.0492 - accuracy: 0.9870 - val_loss: 1.8875 - val_accuracy: 0.7114\n",
      "Epoch 65/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0520 - accuracy: 0.9859 - val_loss: 1.9444 - val_accuracy: 0.7233\n",
      "Epoch 66/200\n",
      "1969/1969 [==============================] - 2s 974us/step - loss: 0.0487 - accuracy: 0.9866 - val_loss: 1.9880 - val_accuracy: 0.7243\n",
      "Epoch 67/200\n",
      "1969/1969 [==============================] - 2s 942us/step - loss: 0.0485 - accuracy: 0.9866 - val_loss: 1.8213 - val_accuracy: 0.7217\n",
      "Epoch 68/200\n",
      "1969/1969 [==============================] - 2s 943us/step - loss: 0.0485 - accuracy: 0.9866 - val_loss: 1.8676 - val_accuracy: 0.7224\n",
      "Epoch 69/200\n",
      "1969/1969 [==============================] - 2s 941us/step - loss: 0.0489 - accuracy: 0.9864 - val_loss: 1.8099 - val_accuracy: 0.7146\n",
      "Epoch 70/200\n",
      "1969/1969 [==============================] - 2s 939us/step - loss: 0.0507 - accuracy: 0.9861 - val_loss: 1.9390 - val_accuracy: 0.7194\n",
      "Epoch 71/200\n",
      "1969/1969 [==============================] - 2s 938us/step - loss: 0.0473 - accuracy: 0.9873 - val_loss: 1.9172 - val_accuracy: 0.7265\n",
      "Epoch 72/200\n",
      "1969/1969 [==============================] - 2s 938us/step - loss: 0.0494 - accuracy: 0.9865 - val_loss: 1.9907 - val_accuracy: 0.7202\n",
      "Epoch 73/200\n",
      "1969/1969 [==============================] - 2s 939us/step - loss: 0.0466 - accuracy: 0.9869 - val_loss: 2.0145 - val_accuracy: 0.7168\n",
      "Epoch 74/200\n",
      "1969/1969 [==============================] - 2s 950us/step - loss: 0.0486 - accuracy: 0.9864 - val_loss: 1.8644 - val_accuracy: 0.7111\n",
      "Epoch 75/200\n",
      "1969/1969 [==============================] - 2s 941us/step - loss: 0.0535 - accuracy: 0.9853 - val_loss: 1.9192 - val_accuracy: 0.7227\n",
      "Epoch 76/200\n",
      "1969/1969 [==============================] - 2s 951us/step - loss: 0.0486 - accuracy: 0.9865 - val_loss: 1.7674 - val_accuracy: 0.7272\n",
      "Epoch 77/200\n",
      "1969/1969 [==============================] - 2s 933us/step - loss: 0.0503 - accuracy: 0.9863 - val_loss: 1.8438 - val_accuracy: 0.7218\n",
      "Epoch 78/200\n",
      "1969/1969 [==============================] - 2s 933us/step - loss: 0.0493 - accuracy: 0.9862 - val_loss: 2.0493 - val_accuracy: 0.7233\n",
      "Epoch 79/200\n",
      "1969/1969 [==============================] - 2s 934us/step - loss: 0.0482 - accuracy: 0.9874 - val_loss: 1.9246 - val_accuracy: 0.7249\n",
      "Epoch 80/200\n",
      "1969/1969 [==============================] - 2s 959us/step - loss: 0.0474 - accuracy: 0.9863 - val_loss: 1.8914 - val_accuracy: 0.7279\n",
      "Epoch 81/200\n",
      "1969/1969 [==============================] - 2s 937us/step - loss: 0.0505 - accuracy: 0.9863 - val_loss: 1.7194 - val_accuracy: 0.7231\n",
      "Epoch 82/200\n",
      "1969/1969 [==============================] - 2s 957us/step - loss: 0.0510 - accuracy: 0.9860 - val_loss: 1.8366 - val_accuracy: 0.7301\n",
      "Epoch 83/200\n",
      "1969/1969 [==============================] - 2s 986us/step - loss: 0.0500 - accuracy: 0.9864 - val_loss: 1.8999 - val_accuracy: 0.7199\n",
      "Epoch 84/200\n",
      "1969/1969 [==============================] - 2s 968us/step - loss: 0.0481 - accuracy: 0.9865 - val_loss: 2.0043 - val_accuracy: 0.7188\n",
      "Epoch 85/200\n",
      "1969/1969 [==============================] - 2s 938us/step - loss: 0.0474 - accuracy: 0.9863 - val_loss: 1.9552 - val_accuracy: 0.7241\n",
      "Epoch 86/200\n",
      "1969/1969 [==============================] - 2s 936us/step - loss: 0.0465 - accuracy: 0.9873 - val_loss: 1.9751 - val_accuracy: 0.7203\n",
      "Epoch 87/200\n",
      "1969/1969 [==============================] - 2s 939us/step - loss: 0.0464 - accuracy: 0.9875 - val_loss: 1.9117 - val_accuracy: 0.7210\n",
      "Epoch 88/200\n",
      "1969/1969 [==============================] - 2s 948us/step - loss: 0.0476 - accuracy: 0.9870 - val_loss: 1.8553 - val_accuracy: 0.7229\n",
      "Epoch 89/200\n",
      "1969/1969 [==============================] - 2s 953us/step - loss: 0.0469 - accuracy: 0.9867 - val_loss: 1.7970 - val_accuracy: 0.7329\n",
      "Epoch 90/200\n",
      "1969/1969 [==============================] - 2s 940us/step - loss: 0.0467 - accuracy: 0.9876 - val_loss: 1.9219 - val_accuracy: 0.7252\n",
      "Epoch 91/200\n",
      "1969/1969 [==============================] - 2s 931us/step - loss: 0.0464 - accuracy: 0.9875 - val_loss: 2.0231 - val_accuracy: 0.7292\n",
      "Epoch 92/200\n",
      "1969/1969 [==============================] - 2s 955us/step - loss: 0.0463 - accuracy: 0.9873 - val_loss: 1.8646 - val_accuracy: 0.7245\n",
      "Epoch 93/200\n",
      "1969/1969 [==============================] - 2s 948us/step - loss: 0.0447 - accuracy: 0.9878 - val_loss: 1.9414 - val_accuracy: 0.7284\n",
      "Epoch 94/200\n",
      "1969/1969 [==============================] - 2s 944us/step - loss: 0.0439 - accuracy: 0.9878 - val_loss: 2.0210 - val_accuracy: 0.7171\n",
      "Epoch 95/200\n",
      "1969/1969 [==============================] - 2s 946us/step - loss: 0.0450 - accuracy: 0.9878 - val_loss: 1.9218 - val_accuracy: 0.7210\n",
      "Epoch 96/200\n",
      "1969/1969 [==============================] - 2s 954us/step - loss: 0.0466 - accuracy: 0.9873 - val_loss: 2.0209 - val_accuracy: 0.7240\n",
      "Epoch 97/200\n",
      "1969/1969 [==============================] - 2s 955us/step - loss: 0.0436 - accuracy: 0.9872 - val_loss: 2.0935 - val_accuracy: 0.7279\n",
      "Epoch 98/200\n",
      "1969/1969 [==============================] - 2s 954us/step - loss: 0.0469 - accuracy: 0.9871 - val_loss: 1.9231 - val_accuracy: 0.7256\n",
      "Epoch 99/200\n",
      "1969/1969 [==============================] - 2s 942us/step - loss: 0.0448 - accuracy: 0.9878 - val_loss: 2.0001 - val_accuracy: 0.7186\n",
      "Epoch 100/200\n",
      "1969/1969 [==============================] - 2s 945us/step - loss: 0.0448 - accuracy: 0.9875 - val_loss: 1.9353 - val_accuracy: 0.7169\n",
      "Epoch 101/200\n",
      "1969/1969 [==============================] - 2s 933us/step - loss: 0.0474 - accuracy: 0.9874 - val_loss: 2.0295 - val_accuracy: 0.7202\n",
      "Epoch 102/200\n",
      "1969/1969 [==============================] - 2s 933us/step - loss: 0.0448 - accuracy: 0.9880 - val_loss: 1.9769 - val_accuracy: 0.7268\n",
      "Epoch 103/200\n",
      "1969/1969 [==============================] - 2s 942us/step - loss: 0.0465 - accuracy: 0.9868 - val_loss: 2.6206 - val_accuracy: 0.7206\n",
      "Epoch 104/200\n",
      "1969/1969 [==============================] - 2s 955us/step - loss: 0.0470 - accuracy: 0.9872 - val_loss: 2.0501 - val_accuracy: 0.7165\n",
      "Epoch 105/200\n",
      "1969/1969 [==============================] - 2s 941us/step - loss: 0.0436 - accuracy: 0.9885 - val_loss: 1.9419 - val_accuracy: 0.7189\n",
      "Epoch 106/200\n",
      "1969/1969 [==============================] - 2s 945us/step - loss: 0.0448 - accuracy: 0.9883 - val_loss: 1.9627 - val_accuracy: 0.7233\n",
      "Epoch 107/200\n",
      "1969/1969 [==============================] - 2s 943us/step - loss: 0.0427 - accuracy: 0.9882 - val_loss: 1.9864 - val_accuracy: 0.7254\n",
      "Epoch 108/200\n",
      "1969/1969 [==============================] - 2s 958us/step - loss: 0.0431 - accuracy: 0.9886 - val_loss: 1.9563 - val_accuracy: 0.7218\n",
      "Epoch 109/200\n",
      "1969/1969 [==============================] - 2s 949us/step - loss: 0.0455 - accuracy: 0.9879 - val_loss: 1.9649 - val_accuracy: 0.7170\n",
      "Epoch 110/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0468 - accuracy: 0.9872 - val_loss: 2.1764 - val_accuracy: 0.7219\n",
      "Epoch 111/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969/1969 [==============================] - 2s 993us/step - loss: 0.0455 - accuracy: 0.9876 - val_loss: 1.9563 - val_accuracy: 0.7188\n",
      "Epoch 112/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0475 - accuracy: 0.9876 - val_loss: 2.0166 - val_accuracy: 0.7272\n",
      "Epoch 113/200\n",
      "1969/1969 [==============================] - 2s 964us/step - loss: 0.0480 - accuracy: 0.9874 - val_loss: 1.8670 - val_accuracy: 0.7267\n",
      "Epoch 114/200\n",
      "1969/1969 [==============================] - 2s 969us/step - loss: 0.0434 - accuracy: 0.9884 - val_loss: 2.1443 - val_accuracy: 0.7236\n",
      "Epoch 115/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0471 - accuracy: 0.9871 - val_loss: 1.8771 - val_accuracy: 0.7261\n",
      "Epoch 116/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0433 - accuracy: 0.9881 - val_loss: 1.8676 - val_accuracy: 0.7279\n",
      "Epoch 117/200\n",
      "1969/1969 [==============================] - 2s 977us/step - loss: 0.0490 - accuracy: 0.9867 - val_loss: 2.1732 - val_accuracy: 0.7306\n",
      "Epoch 118/200\n",
      "1969/1969 [==============================] - 2s 968us/step - loss: 0.0421 - accuracy: 0.9887 - val_loss: 1.9902 - val_accuracy: 0.7264\n",
      "Epoch 119/200\n",
      "1969/1969 [==============================] - 2s 976us/step - loss: 0.0456 - accuracy: 0.9878 - val_loss: 2.0812 - val_accuracy: 0.7236\n",
      "Epoch 120/200\n",
      "1969/1969 [==============================] - 2s 964us/step - loss: 0.0461 - accuracy: 0.9875 - val_loss: 1.9788 - val_accuracy: 0.7185\n",
      "Epoch 121/200\n",
      "1969/1969 [==============================] - 2s 967us/step - loss: 0.0452 - accuracy: 0.9877 - val_loss: 1.8244 - val_accuracy: 0.7081\n",
      "Epoch 122/200\n",
      "1969/1969 [==============================] - 2s 976us/step - loss: 0.0476 - accuracy: 0.9873 - val_loss: 1.9789 - val_accuracy: 0.7200\n",
      "Epoch 123/200\n",
      "1969/1969 [==============================] - 2s 962us/step - loss: 0.0434 - accuracy: 0.9883 - val_loss: 2.1223 - val_accuracy: 0.7139\n",
      "Epoch 124/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0492 - accuracy: 0.9873 - val_loss: 1.8451 - val_accuracy: 0.7224\n",
      "Epoch 125/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0409 - accuracy: 0.9887 - val_loss: 1.9564 - val_accuracy: 0.7230\n",
      "Epoch 126/200\n",
      "1969/1969 [==============================] - 2s 972us/step - loss: 0.0450 - accuracy: 0.9878 - val_loss: 2.0318 - val_accuracy: 0.7285\n",
      "Epoch 127/200\n",
      "1969/1969 [==============================] - 2s 980us/step - loss: 0.0436 - accuracy: 0.9881 - val_loss: 1.9185 - val_accuracy: 0.7220\n",
      "Epoch 128/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0450 - accuracy: 0.9879 - val_loss: 1.8118 - val_accuracy: 0.7257\n",
      "Epoch 129/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0438 - accuracy: 0.9879 - val_loss: 2.0268 - val_accuracy: 0.7183\n",
      "Epoch 130/200\n",
      "1969/1969 [==============================] - 2s 977us/step - loss: 0.0457 - accuracy: 0.9880 - val_loss: 1.9446 - val_accuracy: 0.7262\n",
      "Epoch 131/200\n",
      "1969/1969 [==============================] - 2s 984us/step - loss: 0.0449 - accuracy: 0.9877 - val_loss: 1.9289 - val_accuracy: 0.7214\n",
      "Epoch 132/200\n",
      "1969/1969 [==============================] - 2s 990us/step - loss: 0.0473 - accuracy: 0.9869 - val_loss: 1.9645 - val_accuracy: 0.7177\n",
      "Epoch 133/200\n",
      "1969/1969 [==============================] - 2s 977us/step - loss: 0.0446 - accuracy: 0.9878 - val_loss: 2.0283 - val_accuracy: 0.7258\n",
      "Epoch 134/200\n",
      "1969/1969 [==============================] - 2s 988us/step - loss: 0.0419 - accuracy: 0.9893 - val_loss: 2.0033 - val_accuracy: 0.7117\n",
      "Epoch 135/200\n",
      "1969/1969 [==============================] - 2s 981us/step - loss: 0.0408 - accuracy: 0.9893 - val_loss: 2.1189 - val_accuracy: 0.7145\n",
      "Epoch 136/200\n",
      "1969/1969 [==============================] - 2s 974us/step - loss: 0.0441 - accuracy: 0.9883 - val_loss: 1.9815 - val_accuracy: 0.7238\n",
      "Epoch 137/200\n",
      "1969/1969 [==============================] - 2s 989us/step - loss: 0.0453 - accuracy: 0.9886 - val_loss: 2.0028 - val_accuracy: 0.7190\n",
      "Epoch 138/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0437 - accuracy: 0.9882 - val_loss: 1.9414 - val_accuracy: 0.7240\n",
      "Epoch 139/200\n",
      "1969/1969 [==============================] - 2s 999us/step - loss: 0.0444 - accuracy: 0.9882 - val_loss: 1.8995 - val_accuracy: 0.7172\n",
      "Epoch 140/200\n",
      "1969/1969 [==============================] - 2s 983us/step - loss: 0.0463 - accuracy: 0.9874 - val_loss: 1.8900 - val_accuracy: 0.7138\n",
      "Epoch 141/200\n",
      "1969/1969 [==============================] - 2s 969us/step - loss: 0.0400 - accuracy: 0.9893 - val_loss: 1.9246 - val_accuracy: 0.7272\n",
      "Epoch 142/200\n",
      "1969/1969 [==============================] - 2s 996us/step - loss: 0.0424 - accuracy: 0.9884 - val_loss: 2.1447 - val_accuracy: 0.7227\n",
      "Epoch 143/200\n",
      "1969/1969 [==============================] - 2s 993us/step - loss: 0.0427 - accuracy: 0.9887 - val_loss: 1.9738 - val_accuracy: 0.7222\n",
      "Epoch 144/200\n",
      "1969/1969 [==============================] - 2s 973us/step - loss: 0.0471 - accuracy: 0.9871 - val_loss: 2.0615 - val_accuracy: 0.7202\n",
      "Epoch 145/200\n",
      "1969/1969 [==============================] - 2s 992us/step - loss: 0.0416 - accuracy: 0.9884 - val_loss: 1.9878 - val_accuracy: 0.7271\n",
      "Epoch 146/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0399 - accuracy: 0.9893 - val_loss: 2.0796 - val_accuracy: 0.7217\n",
      "Epoch 147/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0436 - accuracy: 0.9882 - val_loss: 2.0712 - val_accuracy: 0.7244\n",
      "Epoch 148/200\n",
      "1969/1969 [==============================] - 2s 985us/step - loss: 0.0423 - accuracy: 0.9884 - val_loss: 1.9041 - val_accuracy: 0.7240\n",
      "Epoch 149/200\n",
      "1969/1969 [==============================] - 2s 989us/step - loss: 0.0428 - accuracy: 0.9886 - val_loss: 2.0513 - val_accuracy: 0.7225\n",
      "Epoch 150/200\n",
      "1969/1969 [==============================] - 2s 994us/step - loss: 0.0442 - accuracy: 0.9883 - val_loss: 2.0092 - val_accuracy: 0.7268\n",
      "Epoch 151/200\n",
      "1969/1969 [==============================] - 2s 969us/step - loss: 0.0408 - accuracy: 0.9891 - val_loss: 1.8912 - val_accuracy: 0.7216\n",
      "Epoch 152/200\n",
      "1969/1969 [==============================] - 2s 998us/step - loss: 0.0435 - accuracy: 0.9884 - val_loss: 2.0382 - val_accuracy: 0.7225\n",
      "Epoch 153/200\n",
      "1969/1969 [==============================] - 2s 988us/step - loss: 0.0466 - accuracy: 0.9878 - val_loss: 2.1219 - val_accuracy: 0.7275\n",
      "Epoch 154/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0432 - accuracy: 0.9880 - val_loss: 2.0393 - val_accuracy: 0.7196\n",
      "Epoch 155/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0438 - accuracy: 0.9879 - val_loss: 1.9783 - val_accuracy: 0.7229\n",
      "Epoch 156/200\n",
      "1969/1969 [==============================] - 2s 985us/step - loss: 0.0398 - accuracy: 0.9892 - val_loss: 2.0354 - val_accuracy: 0.7174\n",
      "Epoch 157/200\n",
      "1969/1969 [==============================] - 2s 986us/step - loss: 0.0415 - accuracy: 0.9884 - val_loss: 1.9725 - val_accuracy: 0.7234\n",
      "Epoch 158/200\n",
      "1969/1969 [==============================] - 2s 987us/step - loss: 0.0432 - accuracy: 0.9884 - val_loss: 1.8645 - val_accuracy: 0.7347\n",
      "Epoch 159/200\n",
      "1969/1969 [==============================] - 2s 990us/step - loss: 0.0433 - accuracy: 0.9884 - val_loss: 1.9029 - val_accuracy: 0.7277\n",
      "Epoch 160/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0414 - accuracy: 0.9886 - val_loss: 1.9615 - val_accuracy: 0.7324\n",
      "Epoch 161/200\n",
      "1969/1969 [==============================] - 2s 981us/step - loss: 0.0440 - accuracy: 0.9884 - val_loss: 1.9750 - val_accuracy: 0.7290\n",
      "Epoch 162/200\n",
      "1969/1969 [==============================] - 2s 967us/step - loss: 0.0414 - accuracy: 0.9890 - val_loss: 1.8214 - val_accuracy: 0.7216\n",
      "Epoch 163/200\n",
      "1969/1969 [==============================] - 2s 990us/step - loss: 0.0425 - accuracy: 0.9887 - val_loss: 2.0147 - val_accuracy: 0.7169\n",
      "Epoch 164/200\n",
      "1969/1969 [==============================] - 2s 986us/step - loss: 0.0401 - accuracy: 0.9890 - val_loss: 2.2484 - val_accuracy: 0.7319\n",
      "Epoch 165/200\n",
      "1969/1969 [==============================] - 2s 994us/step - loss: 0.0428 - accuracy: 0.9886 - val_loss: 2.1886 - val_accuracy: 0.7258\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969/1969 [==============================] - 2s 946us/step - loss: 0.0419 - accuracy: 0.9888 - val_loss: 2.0483 - val_accuracy: 0.7239\n",
      "Epoch 167/200\n",
      "1969/1969 [==============================] - 2s 934us/step - loss: 0.0419 - accuracy: 0.9887 - val_loss: 2.0229 - val_accuracy: 0.7295\n",
      "Epoch 168/200\n",
      "1969/1969 [==============================] - 2s 942us/step - loss: 0.0367 - accuracy: 0.9900 - val_loss: 1.9419 - val_accuracy: 0.7314\n",
      "Epoch 169/200\n",
      "1969/1969 [==============================] - 2s 933us/step - loss: 0.0444 - accuracy: 0.9887 - val_loss: 1.8228 - val_accuracy: 0.7127\n",
      "Epoch 170/200\n",
      "1969/1969 [==============================] - 2s 943us/step - loss: 0.0377 - accuracy: 0.9895 - val_loss: 1.8580 - val_accuracy: 0.7272\n",
      "Epoch 171/200\n",
      "1969/1969 [==============================] - 2s 950us/step - loss: 0.0415 - accuracy: 0.9892 - val_loss: 1.9236 - val_accuracy: 0.7242\n",
      "Epoch 172/200\n",
      "1969/1969 [==============================] - 2s 932us/step - loss: 0.0416 - accuracy: 0.9888 - val_loss: 1.9204 - val_accuracy: 0.7104\n",
      "Epoch 173/200\n",
      "1969/1969 [==============================] - 2s 947us/step - loss: 0.0428 - accuracy: 0.9881 - val_loss: 1.9410 - val_accuracy: 0.7231\n",
      "Epoch 174/200\n",
      "1969/1969 [==============================] - 2s 933us/step - loss: 0.0425 - accuracy: 0.9886 - val_loss: 1.9399 - val_accuracy: 0.7303\n",
      "Epoch 175/200\n",
      "1969/1969 [==============================] - 2s 984us/step - loss: 0.0428 - accuracy: 0.9884 - val_loss: 1.8654 - val_accuracy: 0.7181\n",
      "Epoch 176/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0423 - accuracy: 0.9886 - val_loss: 1.8850 - val_accuracy: 0.7250\n",
      "Epoch 177/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0440 - accuracy: 0.9884 - val_loss: 1.9551 - val_accuracy: 0.7287\n",
      "Epoch 178/200\n",
      "1969/1969 [==============================] - 2s 1000us/step - loss: 0.0455 - accuracy: 0.9880 - val_loss: 1.9757 - val_accuracy: 0.7232\n",
      "Epoch 179/200\n",
      "1969/1969 [==============================] - 2s 995us/step - loss: 0.0417 - accuracy: 0.9890 - val_loss: 1.9726 - val_accuracy: 0.7233\n",
      "Epoch 180/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0403 - accuracy: 0.9895 - val_loss: 2.0160 - val_accuracy: 0.7227\n",
      "Epoch 181/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0401 - accuracy: 0.9895 - val_loss: 1.9018 - val_accuracy: 0.7272\n",
      "Epoch 182/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0397 - accuracy: 0.9891 - val_loss: 1.8241 - val_accuracy: 0.7199\n",
      "Epoch 183/200\n",
      "1969/1969 [==============================] - 2s 979us/step - loss: 0.0380 - accuracy: 0.9897 - val_loss: 1.8052 - val_accuracy: 0.7059\n",
      "Epoch 184/200\n",
      "1969/1969 [==============================] - 2s 989us/step - loss: 0.0430 - accuracy: 0.9885 - val_loss: 2.0176 - val_accuracy: 0.7197\n",
      "Epoch 185/200\n",
      "1969/1969 [==============================] - 2s 1000us/step - loss: 0.0434 - accuracy: 0.9886 - val_loss: 1.8329 - val_accuracy: 0.7240\n",
      "Epoch 186/200\n",
      "1969/1969 [==============================] - 2s 977us/step - loss: 0.0413 - accuracy: 0.9882 - val_loss: 2.1311 - val_accuracy: 0.7221\n",
      "Epoch 187/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0415 - accuracy: 0.9887 - val_loss: 2.0503 - val_accuracy: 0.7079\n",
      "Epoch 188/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0411 - accuracy: 0.9884 - val_loss: 1.9457 - val_accuracy: 0.7265\n",
      "Epoch 189/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0419 - accuracy: 0.9892 - val_loss: 1.9923 - val_accuracy: 0.7287\n",
      "Epoch 190/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0403 - accuracy: 0.9891 - val_loss: 2.0579 - val_accuracy: 0.7217\n",
      "Epoch 191/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0423 - accuracy: 0.9889 - val_loss: 2.0434 - val_accuracy: 0.7230\n",
      "Epoch 192/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0405 - accuracy: 0.9893 - val_loss: 1.8875 - val_accuracy: 0.7200\n",
      "Epoch 193/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0418 - accuracy: 0.9889 - val_loss: 1.8916 - val_accuracy: 0.7314\n",
      "Epoch 194/200\n",
      "1969/1969 [==============================] - 2s 965us/step - loss: 0.0377 - accuracy: 0.9900 - val_loss: 1.7878 - val_accuracy: 0.7269\n",
      "Epoch 195/200\n",
      "1969/1969 [==============================] - 2s 984us/step - loss: 0.0402 - accuracy: 0.9892 - val_loss: 2.0103 - val_accuracy: 0.7196\n",
      "Epoch 196/200\n",
      "1969/1969 [==============================] - 2s 969us/step - loss: 0.0434 - accuracy: 0.9889 - val_loss: 1.9667 - val_accuracy: 0.7192\n",
      "Epoch 197/200\n",
      "1969/1969 [==============================] - 2s 971us/step - loss: 0.0426 - accuracy: 0.9887 - val_loss: 1.9862 - val_accuracy: 0.7157\n",
      "Epoch 198/200\n",
      "1969/1969 [==============================] - 2s 992us/step - loss: 0.0381 - accuracy: 0.9895 - val_loss: 2.0778 - val_accuracy: 0.7111\n",
      "Epoch 199/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0406 - accuracy: 0.9893 - val_loss: 1.8667 - val_accuracy: 0.7265\n",
      "Epoch 200/200\n",
      "1969/1969 [==============================] - 2s 1ms/step - loss: 0.0383 - accuracy: 0.9901 - val_loss: 1.8328 - val_accuracy: 0.7247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 20:22:53.552420: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization,Dropout,Dense\n",
    "\n",
    "\n",
    "df_train['class'] = df_train['class'].astype('category')\n",
    "df_train['class']= df_train['class'].cat.codes\n",
    "\n",
    "df_test['class'] = df_test['class'].astype('category')\n",
    "df_test['class']= df_test['class'].cat.codes\n",
    "\n",
    "y_test = df_test.iloc[:,41]\n",
    "y_train = df_train.iloc[:, 41]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units= 5, activation = 'softmax'))\n",
    "model.compile(optimizer='adam',metrics=['accuracy'],loss='sparse_categorical_crossentropy')\n",
    "model.fit(x_train, y_train, \n",
    "          epochs=200, \n",
    "          batch_size=64,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/705 [===>..........................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705/705 [==============================] - 0s 418us/step\n",
      "total accuracy: 0.7246717530163236\n",
      "[[5454 1902  102    0    0]\n",
      " [  52 9449  209    1    0]\n",
      " [ 155  833 1433    0    0]\n",
      " [   0 2543   10    1    0]\n",
      " [   0  400    0    0    0]]\n",
      "dos accuracy: 0.7312952534191473\n",
      "normal accuracy 0.9730202862732983\n",
      "probe accuracy 0.5919041718298224\n",
      "r2l accuracy 0.00039154267815191856\n",
      "u2r accuracy 0.0\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.73      0.83      7458\n",
      "           1       0.62      0.97      0.76      9711\n",
      "           2       0.82      0.59      0.69      2421\n",
      "           3       0.50      0.00      0.00      2554\n",
      "           4       0.00      0.00      0.00       400\n",
      "\n",
      "    accuracy                           0.72     22544\n",
      "   macro avg       0.58      0.46      0.46     22544\n",
      "weighted avg       0.73      0.72      0.68     22544\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_result = model.predict_classes(x_test, verbose=1)\n",
    "y_pred = []\n",
    "for label in y_result:\n",
    "    if label == 0:\n",
    "        y_pred.append(0)\n",
    "    elif label == 1:\n",
    "        y_pred.append(1)\n",
    "    elif label == 2:\n",
    "        y_pred.append(2)\n",
    "    elif label == 3:\n",
    "        y_pred.append(3)\n",
    "    else:\n",
    "        y_pred.append(4)\n",
    "\n",
    "print(\"total accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)\n",
    "print(\"dos accuracy:\",result[0][0]/sum(result[0]))\n",
    "print(\"normal accuracy\",result[1][1]/sum(result[1]))\n",
    "print(\"probe accuracy\",result[2][2]/sum(result[2]))\n",
    "print(\"r2l accuracy\",result[3][3]/sum(result[3]))\n",
    "print(\"u2r accuracy\",result[4][4]/sum(result[4]))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.6473 - accuracy: 0.8135 - val_loss: 1.3861 - val_accuracy: 0.6614\n",
      "Epoch 2/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.2385 - accuracy: 0.9255 - val_loss: 1.3226 - val_accuracy: 0.6752\n",
      "Epoch 3/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.1666 - accuracy: 0.9502 - val_loss: 1.4508 - val_accuracy: 0.6774\n",
      "Epoch 4/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.1437 - accuracy: 0.9589 - val_loss: 1.5514 - val_accuracy: 0.6903\n",
      "Epoch 5/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.1191 - accuracy: 0.9672 - val_loss: 1.4630 - val_accuracy: 0.6901\n",
      "Epoch 6/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.1137 - accuracy: 0.9674 - val_loss: 1.4247 - val_accuracy: 0.7198\n",
      "Epoch 7/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.1030 - accuracy: 0.9706 - val_loss: 1.5540 - val_accuracy: 0.6968\n",
      "Epoch 8/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.1004 - accuracy: 0.9716 - val_loss: 1.7201 - val_accuracy: 0.6953\n",
      "Epoch 9/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0931 - accuracy: 0.9736 - val_loss: 1.4564 - val_accuracy: 0.7030\n",
      "Epoch 10/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0918 - accuracy: 0.9732 - val_loss: 1.5823 - val_accuracy: 0.6948\n",
      "Epoch 11/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0874 - accuracy: 0.9758 - val_loss: 1.5302 - val_accuracy: 0.6695\n",
      "Epoch 12/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0852 - accuracy: 0.9757 - val_loss: 1.5241 - val_accuracy: 0.6823\n",
      "Epoch 13/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0837 - accuracy: 0.9762 - val_loss: 1.5297 - val_accuracy: 0.7059\n",
      "Epoch 14/200\n",
      "1969/1969 [==============================] - 4s 2ms/step - loss: 0.0798 - accuracy: 0.9779 - val_loss: 1.7830 - val_accuracy: 0.6870\n",
      "Epoch 15/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0757 - accuracy: 0.9788 - val_loss: 1.6490 - val_accuracy: 0.6958\n",
      "Epoch 16/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0705 - accuracy: 0.9802 - val_loss: 1.7220 - val_accuracy: 0.6853\n",
      "Epoch 17/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0730 - accuracy: 0.9793 - val_loss: 1.7322 - val_accuracy: 0.6735\n",
      "Epoch 18/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0701 - accuracy: 0.9807 - val_loss: 1.6402 - val_accuracy: 0.6943\n",
      "Epoch 19/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0729 - accuracy: 0.9793 - val_loss: 1.6145 - val_accuracy: 0.6992\n",
      "Epoch 20/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0697 - accuracy: 0.9800 - val_loss: 1.6661 - val_accuracy: 0.7011\n",
      "Epoch 21/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0692 - accuracy: 0.9800 - val_loss: 1.8522 - val_accuracy: 0.6791\n",
      "Epoch 22/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0706 - accuracy: 0.9795 - val_loss: 1.8231 - val_accuracy: 0.6987\n",
      "Epoch 23/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0632 - accuracy: 0.9817 - val_loss: 1.6410 - val_accuracy: 0.6940\n",
      "Epoch 24/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0629 - accuracy: 0.9820 - val_loss: 1.6761 - val_accuracy: 0.7225\n",
      "Epoch 25/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0607 - accuracy: 0.9826 - val_loss: 1.6728 - val_accuracy: 0.7173\n",
      "Epoch 26/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0614 - accuracy: 0.9832 - val_loss: 1.8104 - val_accuracy: 0.6770\n",
      "Epoch 27/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0610 - accuracy: 0.9829 - val_loss: 1.8581 - val_accuracy: 0.6891\n",
      "Epoch 28/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0590 - accuracy: 0.9831 - val_loss: 1.7322 - val_accuracy: 0.6547\n",
      "Epoch 29/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0587 - accuracy: 0.9839 - val_loss: 1.5726 - val_accuracy: 0.7111\n",
      "Epoch 30/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0600 - accuracy: 0.9830 - val_loss: 1.7672 - val_accuracy: 0.7083\n",
      "Epoch 31/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0583 - accuracy: 0.9830 - val_loss: 1.7052 - val_accuracy: 0.7129\n",
      "Epoch 32/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0596 - accuracy: 0.9836 - val_loss: 1.7882 - val_accuracy: 0.6568\n",
      "Epoch 33/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0545 - accuracy: 0.9849 - val_loss: 1.6339 - val_accuracy: 0.7107\n",
      "Epoch 34/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0529 - accuracy: 0.9858 - val_loss: 1.8458 - val_accuracy: 0.6882\n",
      "Epoch 35/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0568 - accuracy: 0.9841 - val_loss: 1.6571 - val_accuracy: 0.6899\n",
      "Epoch 36/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0553 - accuracy: 0.9850 - val_loss: 1.8377 - val_accuracy: 0.6876\n",
      "Epoch 37/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0510 - accuracy: 0.9859 - val_loss: 1.6152 - val_accuracy: 0.7149\n",
      "Epoch 38/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0554 - accuracy: 0.9847 - val_loss: 1.7358 - val_accuracy: 0.7036\n",
      "Epoch 39/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0530 - accuracy: 0.9855 - val_loss: 1.6539 - val_accuracy: 0.6985\n",
      "Epoch 40/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0499 - accuracy: 0.9861 - val_loss: 1.7055 - val_accuracy: 0.6941\n",
      "Epoch 41/200\n",
      "1969/1969 [==============================] - 4s 2ms/step - loss: 0.0510 - accuracy: 0.9854 - val_loss: 1.7221 - val_accuracy: 0.6937\n",
      "Epoch 42/200\n",
      "1969/1969 [==============================] - 4s 2ms/step - loss: 0.0493 - accuracy: 0.9863 - val_loss: 1.7296 - val_accuracy: 0.6973\n",
      "Epoch 43/200\n",
      "1969/1969 [==============================] - 4s 2ms/step - loss: 0.0487 - accuracy: 0.9866 - val_loss: 1.8877 - val_accuracy: 0.6646\n",
      "Epoch 44/200\n",
      "1969/1969 [==============================] - 4s 2ms/step - loss: 0.0517 - accuracy: 0.9859 - val_loss: 1.7712 - val_accuracy: 0.7059\n",
      "Epoch 45/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0502 - accuracy: 0.9861 - val_loss: 1.6857 - val_accuracy: 0.7006\n",
      "Epoch 46/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0504 - accuracy: 0.9864 - val_loss: 1.8845 - val_accuracy: 0.7006\n",
      "Epoch 47/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0495 - accuracy: 0.9867 - val_loss: 1.8276 - val_accuracy: 0.6927\n",
      "Epoch 48/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0480 - accuracy: 0.9870 - val_loss: 1.7699 - val_accuracy: 0.6824\n",
      "Epoch 49/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0456 - accuracy: 0.9880 - val_loss: 1.7508 - val_accuracy: 0.7043\n",
      "Epoch 50/200\n",
      "1969/1969 [==============================] - 4s 2ms/step - loss: 0.0489 - accuracy: 0.9868 - val_loss: 1.7881 - val_accuracy: 0.6969\n",
      "Epoch 51/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0463 - accuracy: 0.9877 - val_loss: 1.7781 - val_accuracy: 0.6918\n",
      "Epoch 52/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0446 - accuracy: 0.9877 - val_loss: 1.8055 - val_accuracy: 0.6774\n",
      "Epoch 53/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0468 - accuracy: 0.9872 - val_loss: 1.7550 - val_accuracy: 0.7116\n",
      "Epoch 54/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0462 - accuracy: 0.9874 - val_loss: 1.6703 - val_accuracy: 0.7045\n",
      "Epoch 55/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0484 - accuracy: 0.9869 - val_loss: 1.7661 - val_accuracy: 0.6985\n",
      "Epoch 56/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0457 - accuracy: 0.9872 - val_loss: 1.7805 - val_accuracy: 0.6962\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0479 - accuracy: 0.9870 - val_loss: 1.6949 - val_accuracy: 0.7043\n",
      "Epoch 58/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0447 - accuracy: 0.9878 - val_loss: 1.6820 - val_accuracy: 0.6942\n",
      "Epoch 59/200\n",
      "1969/1969 [==============================] - 4s 2ms/step - loss: 0.0445 - accuracy: 0.9879 - val_loss: 1.7311 - val_accuracy: 0.7162\n",
      "Epoch 60/200\n",
      "1969/1969 [==============================] - 4s 2ms/step - loss: 0.0460 - accuracy: 0.9877 - val_loss: 1.8655 - val_accuracy: 0.7048\n",
      "Epoch 61/200\n",
      "1969/1969 [==============================] - 4s 2ms/step - loss: 0.0450 - accuracy: 0.9881 - val_loss: 1.7398 - val_accuracy: 0.7136\n",
      "Epoch 62/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0467 - accuracy: 0.9879 - val_loss: 1.7934 - val_accuracy: 0.6912\n",
      "Epoch 63/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0432 - accuracy: 0.9880 - val_loss: 1.6957 - val_accuracy: 0.6884\n",
      "Epoch 64/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0444 - accuracy: 0.9878 - val_loss: 2.1604 - val_accuracy: 0.6913\n",
      "Epoch 65/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0444 - accuracy: 0.9881 - val_loss: 1.9163 - val_accuracy: 0.7031\n",
      "Epoch 66/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0412 - accuracy: 0.9885 - val_loss: 1.6983 - val_accuracy: 0.6992\n",
      "Epoch 67/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0420 - accuracy: 0.9888 - val_loss: 1.7219 - val_accuracy: 0.7005\n",
      "Epoch 68/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0441 - accuracy: 0.9881 - val_loss: 1.8345 - val_accuracy: 0.6951\n",
      "Epoch 69/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0424 - accuracy: 0.9882 - val_loss: 1.7872 - val_accuracy: 0.7092\n",
      "Epoch 70/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0440 - accuracy: 0.9883 - val_loss: 1.8067 - val_accuracy: 0.7116\n",
      "Epoch 71/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0441 - accuracy: 0.9877 - val_loss: 1.6122 - val_accuracy: 0.6977\n",
      "Epoch 72/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0425 - accuracy: 0.9886 - val_loss: 1.6415 - val_accuracy: 0.7154\n",
      "Epoch 73/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0420 - accuracy: 0.9886 - val_loss: 1.6619 - val_accuracy: 0.7154\n",
      "Epoch 74/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0423 - accuracy: 0.9887 - val_loss: 1.8973 - val_accuracy: 0.7046\n",
      "Epoch 75/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0378 - accuracy: 0.9901 - val_loss: 1.8179 - val_accuracy: 0.7001\n",
      "Epoch 76/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0411 - accuracy: 0.9890 - val_loss: 1.7924 - val_accuracy: 0.6972\n",
      "Epoch 77/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0429 - accuracy: 0.9880 - val_loss: 1.6786 - val_accuracy: 0.6972\n",
      "Epoch 78/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0411 - accuracy: 0.9888 - val_loss: 1.9347 - val_accuracy: 0.6718\n",
      "Epoch 79/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0415 - accuracy: 0.9891 - val_loss: 1.7676 - val_accuracy: 0.6975\n",
      "Epoch 80/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0411 - accuracy: 0.9893 - val_loss: 1.6777 - val_accuracy: 0.7015\n",
      "Epoch 81/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0398 - accuracy: 0.9894 - val_loss: 1.7456 - val_accuracy: 0.7044\n",
      "Epoch 82/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0406 - accuracy: 0.9891 - val_loss: 1.8700 - val_accuracy: 0.7092\n",
      "Epoch 83/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0397 - accuracy: 0.9889 - val_loss: 1.8509 - val_accuracy: 0.6906\n",
      "Epoch 84/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0410 - accuracy: 0.9890 - val_loss: 1.7381 - val_accuracy: 0.7052\n",
      "Epoch 85/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0388 - accuracy: 0.9892 - val_loss: 1.6599 - val_accuracy: 0.7119\n",
      "Epoch 86/200\n",
      "1969/1969 [==============================] - 4s 2ms/step - loss: 0.0388 - accuracy: 0.9895 - val_loss: 1.7267 - val_accuracy: 0.6916\n",
      "Epoch 87/200\n",
      "1969/1969 [==============================] - 4s 2ms/step - loss: 0.0416 - accuracy: 0.9888 - val_loss: 1.8669 - val_accuracy: 0.6915\n",
      "Epoch 88/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0397 - accuracy: 0.9893 - val_loss: 1.7927 - val_accuracy: 0.6973\n",
      "Epoch 89/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0417 - accuracy: 0.9893 - val_loss: 1.6931 - val_accuracy: 0.7077\n",
      "Epoch 90/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0360 - accuracy: 0.9904 - val_loss: 1.6806 - val_accuracy: 0.7101\n",
      "Epoch 91/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0401 - accuracy: 0.9896 - val_loss: 1.7693 - val_accuracy: 0.7012\n",
      "Epoch 92/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0379 - accuracy: 0.9899 - val_loss: 1.7172 - val_accuracy: 0.7111\n",
      "Epoch 93/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0391 - accuracy: 0.9902 - val_loss: 1.6475 - val_accuracy: 0.7056\n",
      "Epoch 94/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0368 - accuracy: 0.9903 - val_loss: 1.7966 - val_accuracy: 0.7004\n",
      "Epoch 95/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0379 - accuracy: 0.9901 - val_loss: 1.8396 - val_accuracy: 0.7057\n",
      "Epoch 96/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0380 - accuracy: 0.9897 - val_loss: 1.7049 - val_accuracy: 0.7032\n",
      "Epoch 97/200\n",
      "1969/1969 [==============================] - 4s 2ms/step - loss: 0.0402 - accuracy: 0.9894 - val_loss: 2.0048 - val_accuracy: 0.6950\n",
      "Epoch 98/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0411 - accuracy: 0.9896 - val_loss: 1.8781 - val_accuracy: 0.6950\n",
      "Epoch 99/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0392 - accuracy: 0.9897 - val_loss: 1.7355 - val_accuracy: 0.7227\n",
      "Epoch 100/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0383 - accuracy: 0.9901 - val_loss: 1.7192 - val_accuracy: 0.7070\n",
      "Epoch 101/200\n",
      "1969/1969 [==============================] - 4s 2ms/step - loss: 0.0392 - accuracy: 0.9901 - val_loss: 1.7738 - val_accuracy: 0.6981\n",
      "Epoch 102/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0397 - accuracy: 0.9898 - val_loss: 1.7774 - val_accuracy: 0.7131\n",
      "Epoch 103/200\n",
      "1969/1969 [==============================] - 4s 2ms/step - loss: 0.0375 - accuracy: 0.9905 - val_loss: 1.5085 - val_accuracy: 0.7020\n",
      "Epoch 104/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0366 - accuracy: 0.9902 - val_loss: 1.6220 - val_accuracy: 0.7054\n",
      "Epoch 105/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0370 - accuracy: 0.9901 - val_loss: 1.7406 - val_accuracy: 0.6934\n",
      "Epoch 106/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0391 - accuracy: 0.9893 - val_loss: 1.5234 - val_accuracy: 0.7056\n",
      "Epoch 107/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0339 - accuracy: 0.9910 - val_loss: 1.5243 - val_accuracy: 0.7276\n",
      "Epoch 108/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0324 - accuracy: 0.9915 - val_loss: 1.7429 - val_accuracy: 0.6942\n",
      "Epoch 109/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0389 - accuracy: 0.9899 - val_loss: 1.7335 - val_accuracy: 0.7082\n",
      "Epoch 110/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0351 - accuracy: 0.9907 - val_loss: 1.6132 - val_accuracy: 0.6971\n",
      "Epoch 111/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0366 - accuracy: 0.9905 - val_loss: 1.6504 - val_accuracy: 0.7192\n",
      "Epoch 112/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0360 - accuracy: 0.9908 - val_loss: 1.7359 - val_accuracy: 0.7111\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0349 - accuracy: 0.9910 - val_loss: 1.6504 - val_accuracy: 0.7023\n",
      "Epoch 114/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0369 - accuracy: 0.9901 - val_loss: 1.5167 - val_accuracy: 0.7275\n",
      "Epoch 115/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0348 - accuracy: 0.9903 - val_loss: 1.6818 - val_accuracy: 0.7090\n",
      "Epoch 116/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0354 - accuracy: 0.9910 - val_loss: 1.6975 - val_accuracy: 0.7017\n",
      "Epoch 117/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0367 - accuracy: 0.9902 - val_loss: 1.7759 - val_accuracy: 0.6888\n",
      "Epoch 118/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0376 - accuracy: 0.9900 - val_loss: 1.5557 - val_accuracy: 0.6987\n",
      "Epoch 119/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0382 - accuracy: 0.9898 - val_loss: 1.6909 - val_accuracy: 0.6892\n",
      "Epoch 120/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0388 - accuracy: 0.9898 - val_loss: 1.5270 - val_accuracy: 0.7111\n",
      "Epoch 121/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0361 - accuracy: 0.9907 - val_loss: 1.5239 - val_accuracy: 0.7233\n",
      "Epoch 122/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0365 - accuracy: 0.9904 - val_loss: 1.5469 - val_accuracy: 0.7166\n",
      "Epoch 123/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0361 - accuracy: 0.9900 - val_loss: 1.5712 - val_accuracy: 0.7083\n",
      "Epoch 124/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0377 - accuracy: 0.9905 - val_loss: 1.5487 - val_accuracy: 0.7085\n",
      "Epoch 125/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0371 - accuracy: 0.9903 - val_loss: 1.7173 - val_accuracy: 0.6952\n",
      "Epoch 126/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0360 - accuracy: 0.9904 - val_loss: 1.5789 - val_accuracy: 0.7047\n",
      "Epoch 127/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0378 - accuracy: 0.9902 - val_loss: 1.6011 - val_accuracy: 0.6981\n",
      "Epoch 128/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0346 - accuracy: 0.9911 - val_loss: 1.5560 - val_accuracy: 0.6969\n",
      "Epoch 129/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0364 - accuracy: 0.9903 - val_loss: 1.8521 - val_accuracy: 0.7036\n",
      "Epoch 130/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0373 - accuracy: 0.9899 - val_loss: 1.7259 - val_accuracy: 0.6982\n",
      "Epoch 131/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0378 - accuracy: 0.9902 - val_loss: 1.5796 - val_accuracy: 0.7215\n",
      "Epoch 132/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0365 - accuracy: 0.9903 - val_loss: 1.5410 - val_accuracy: 0.7379\n",
      "Epoch 133/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0364 - accuracy: 0.9902 - val_loss: 1.6503 - val_accuracy: 0.6819\n",
      "Epoch 134/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0373 - accuracy: 0.9897 - val_loss: 1.5876 - val_accuracy: 0.7196\n",
      "Epoch 135/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0343 - accuracy: 0.9907 - val_loss: 1.5904 - val_accuracy: 0.7028\n",
      "Epoch 136/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0343 - accuracy: 0.9912 - val_loss: 1.5601 - val_accuracy: 0.7120\n",
      "Epoch 137/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0372 - accuracy: 0.9905 - val_loss: 1.6992 - val_accuracy: 0.7255\n",
      "Epoch 138/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0364 - accuracy: 0.9905 - val_loss: 1.6978 - val_accuracy: 0.7322\n",
      "Epoch 139/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0365 - accuracy: 0.9905 - val_loss: 1.5242 - val_accuracy: 0.7245\n",
      "Epoch 140/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0372 - accuracy: 0.9903 - val_loss: 1.6002 - val_accuracy: 0.7098\n",
      "Epoch 141/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0336 - accuracy: 0.9915 - val_loss: 1.6550 - val_accuracy: 0.7196\n",
      "Epoch 142/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0342 - accuracy: 0.9914 - val_loss: 1.6241 - val_accuracy: 0.7041\n",
      "Epoch 143/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0321 - accuracy: 0.9917 - val_loss: 1.6425 - val_accuracy: 0.7112\n",
      "Epoch 144/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0341 - accuracy: 0.9912 - val_loss: 1.8016 - val_accuracy: 0.6851\n",
      "Epoch 145/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0344 - accuracy: 0.9908 - val_loss: 1.6956 - val_accuracy: 0.6828\n",
      "Epoch 146/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0358 - accuracy: 0.9901 - val_loss: 1.5149 - val_accuracy: 0.7190\n",
      "Epoch 147/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0360 - accuracy: 0.9903 - val_loss: 1.4873 - val_accuracy: 0.6989\n",
      "Epoch 148/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0342 - accuracy: 0.9910 - val_loss: 1.4188 - val_accuracy: 0.7072\n",
      "Epoch 149/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0354 - accuracy: 0.9909 - val_loss: 1.6205 - val_accuracy: 0.7191\n",
      "Epoch 150/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0411 - accuracy: 0.9891 - val_loss: 1.6748 - val_accuracy: 0.7206\n",
      "Epoch 151/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0341 - accuracy: 0.9911 - val_loss: 1.5837 - val_accuracy: 0.7186\n",
      "Epoch 152/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0352 - accuracy: 0.9910 - val_loss: 1.6494 - val_accuracy: 0.7429\n",
      "Epoch 153/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0347 - accuracy: 0.9908 - val_loss: 1.6184 - val_accuracy: 0.7324\n",
      "Epoch 154/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0315 - accuracy: 0.9914 - val_loss: 1.5413 - val_accuracy: 0.7321\n",
      "Epoch 155/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0347 - accuracy: 0.9909 - val_loss: 1.4196 - val_accuracy: 0.7320\n",
      "Epoch 156/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0343 - accuracy: 0.9909 - val_loss: 1.3721 - val_accuracy: 0.7322\n",
      "Epoch 157/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0362 - accuracy: 0.9907 - val_loss: 1.5747 - val_accuracy: 0.7147\n",
      "Epoch 158/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0341 - accuracy: 0.9910 - val_loss: 1.5379 - val_accuracy: 0.7272\n",
      "Epoch 159/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0317 - accuracy: 0.9917 - val_loss: 1.6223 - val_accuracy: 0.7138\n",
      "Epoch 160/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0331 - accuracy: 0.9912 - val_loss: 1.4980 - val_accuracy: 0.7099\n",
      "Epoch 161/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0302 - accuracy: 0.9917 - val_loss: 1.6064 - val_accuracy: 0.7220\n",
      "Epoch 162/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0354 - accuracy: 0.9907 - val_loss: 1.5778 - val_accuracy: 0.7356\n",
      "Epoch 163/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0327 - accuracy: 0.9914 - val_loss: 1.5887 - val_accuracy: 0.7359\n",
      "Epoch 164/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0340 - accuracy: 0.9910 - val_loss: 1.6682 - val_accuracy: 0.7195\n",
      "Epoch 165/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0342 - accuracy: 0.9911 - val_loss: 1.5951 - val_accuracy: 0.7038\n",
      "Epoch 166/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0326 - accuracy: 0.9921 - val_loss: 1.5524 - val_accuracy: 0.6946\n",
      "Epoch 167/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0327 - accuracy: 0.9917 - val_loss: 1.5635 - val_accuracy: 0.7028\n",
      "Epoch 168/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0355 - accuracy: 0.9910 - val_loss: 1.6595 - val_accuracy: 0.7221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0321 - accuracy: 0.9915 - val_loss: 1.5311 - val_accuracy: 0.7293\n",
      "Epoch 170/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0331 - accuracy: 0.9913 - val_loss: 1.4697 - val_accuracy: 0.7552\n",
      "Epoch 171/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0344 - accuracy: 0.9911 - val_loss: 1.4838 - val_accuracy: 0.7574\n",
      "Epoch 172/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0340 - accuracy: 0.9908 - val_loss: 1.4942 - val_accuracy: 0.7176\n",
      "Epoch 173/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0333 - accuracy: 0.9914 - val_loss: 1.4524 - val_accuracy: 0.7173\n",
      "Epoch 174/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0323 - accuracy: 0.9919 - val_loss: 1.5871 - val_accuracy: 0.7272\n",
      "Epoch 175/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0344 - accuracy: 0.9912 - val_loss: 1.5177 - val_accuracy: 0.7360\n",
      "Epoch 176/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0334 - accuracy: 0.9917 - val_loss: 1.6165 - val_accuracy: 0.7217\n",
      "Epoch 177/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0315 - accuracy: 0.9917 - val_loss: 1.5452 - val_accuracy: 0.7165\n",
      "Epoch 178/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0336 - accuracy: 0.9916 - val_loss: 1.5194 - val_accuracy: 0.7155\n",
      "Epoch 179/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0357 - accuracy: 0.9906 - val_loss: 1.5476 - val_accuracy: 0.7214\n",
      "Epoch 180/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0330 - accuracy: 0.9911 - val_loss: 1.7005 - val_accuracy: 0.7104\n",
      "Epoch 181/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0373 - accuracy: 0.9903 - val_loss: 1.6304 - val_accuracy: 0.7406\n",
      "Epoch 182/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0336 - accuracy: 0.9906 - val_loss: 1.5615 - val_accuracy: 0.7256\n",
      "Epoch 183/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0307 - accuracy: 0.9920 - val_loss: 1.5195 - val_accuracy: 0.7345\n",
      "Epoch 184/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0326 - accuracy: 0.9916 - val_loss: 1.5585 - val_accuracy: 0.7142\n",
      "Epoch 185/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0348 - accuracy: 0.9912 - val_loss: 1.4979 - val_accuracy: 0.7220\n",
      "Epoch 186/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0357 - accuracy: 0.9908 - val_loss: 1.3658 - val_accuracy: 0.7321\n",
      "Epoch 187/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0322 - accuracy: 0.9919 - val_loss: 1.7898 - val_accuracy: 0.7291\n",
      "Epoch 188/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0336 - accuracy: 0.9912 - val_loss: 1.6068 - val_accuracy: 0.7301\n",
      "Epoch 189/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0319 - accuracy: 0.9914 - val_loss: 1.5442 - val_accuracy: 0.7175\n",
      "Epoch 190/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0319 - accuracy: 0.9918 - val_loss: 1.5022 - val_accuracy: 0.7275\n",
      "Epoch 191/200\n",
      "1969/1969 [==============================] - 4s 2ms/step - loss: 0.0328 - accuracy: 0.9912 - val_loss: 3.6989 - val_accuracy: 0.7095\n",
      "Epoch 192/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0351 - accuracy: 0.9915 - val_loss: 1.5231 - val_accuracy: 0.7302\n",
      "Epoch 193/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0339 - accuracy: 0.9914 - val_loss: 1.6037 - val_accuracy: 0.7414\n",
      "Epoch 194/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0354 - accuracy: 0.9906 - val_loss: 1.6333 - val_accuracy: 0.7189\n",
      "Epoch 195/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0334 - accuracy: 0.9915 - val_loss: 1.4567 - val_accuracy: 0.7368\n",
      "Epoch 196/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0319 - accuracy: 0.9919 - val_loss: 1.5816 - val_accuracy: 0.7294\n",
      "Epoch 197/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0320 - accuracy: 0.9920 - val_loss: 1.5129 - val_accuracy: 0.7312\n",
      "Epoch 198/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0326 - accuracy: 0.9916 - val_loss: 1.6368 - val_accuracy: 0.7290\n",
      "Epoch 199/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0308 - accuracy: 0.9920 - val_loss: 1.6497 - val_accuracy: 0.7398\n",
      "Epoch 200/200\n",
      "1969/1969 [==============================] - 3s 2ms/step - loss: 0.0319 - accuracy: 0.9916 - val_loss: 1.5880 - val_accuracy: 0.7263\n",
      "INFO:tensorflow:Assets written to: model1/assets\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(units= 5, activation = 'softmax'))\n",
    "model1.compile(optimizer='adam',metrics=['accuracy'],loss='sparse_categorical_crossentropy')\n",
    "model1.fit(x_train, y_train, \n",
    "          epochs=200, \n",
    "          batch_size=64,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "model1.save('model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 90/705 [==>...........................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705/705 [==============================] - 0s 510us/step\n",
      "total accuracy: 0.7263129879347054\n",
      "[[5222 1725  511    0    0]\n",
      " [  44 9446  221    0    0]\n",
      " [ 289  426 1706    0    0]\n",
      " [   1 2487   66    0    0]\n",
      " [   0  399    1    0    0]]\n",
      "dos accuracy: 0.700187717886833\n",
      "normal accuracy 0.972711358253527\n",
      "probe accuracy 0.704667492771582\n",
      "r2l accuracy 0.0\n",
      "u2r accuracy 0.0\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.70      0.80      7458\n",
      "           1       0.65      0.97      0.78      9711\n",
      "           2       0.68      0.70      0.69      2421\n",
      "           3       0.00      0.00      0.00      2554\n",
      "           4       0.00      0.00      0.00       400\n",
      "\n",
      "    accuracy                           0.73     22544\n",
      "   macro avg       0.45      0.48      0.46     22544\n",
      "weighted avg       0.67      0.73      0.68     22544\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_result = model1.predict_classes(x_test, verbose=1)\n",
    "y_pred = []\n",
    "for label in y_result:\n",
    "    if label == 0:\n",
    "        y_pred.append(0)\n",
    "    elif label == 1:\n",
    "        y_pred.append(1)\n",
    "    elif label == 2:\n",
    "        y_pred.append(2)\n",
    "    elif label == 3:\n",
    "        y_pred.append(3)\n",
    "    else:\n",
    "        y_pred.append(4)\n",
    "\n",
    "print(\"total accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)\n",
    "print(\"dos accuracy:\",result[0][0]/sum(result[0]))\n",
    "print(\"normal accuracy\",result[1][1]/sum(result[1]))\n",
    "print(\"probe accuracy\",result[2][2]/sum(result[2]))\n",
    "print(\"r2l accuracy\",result[3][3]/sum(result[3]))\n",
    "print(\"u2r accuracy\",result[4][4]/sum(result[4]))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1969/1969 [==============================] - 7s 3ms/step - loss: 0.8087 - accuracy: 0.7576 - val_loss: 1.3653 - val_accuracy: 0.6396\n",
      "Epoch 2/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.2989 - accuracy: 0.9076 - val_loss: 1.2786 - val_accuracy: 0.6812\n",
      "Epoch 3/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.2122 - accuracy: 0.9377 - val_loss: 1.3190 - val_accuracy: 0.7013\n",
      "Epoch 4/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.1727 - accuracy: 0.9522 - val_loss: 1.4014 - val_accuracy: 0.6934\n",
      "Epoch 5/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.1557 - accuracy: 0.9570 - val_loss: 1.3861 - val_accuracy: 0.6953\n",
      "Epoch 6/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.1354 - accuracy: 0.9636 - val_loss: 1.5606 - val_accuracy: 0.6605\n",
      "Epoch 7/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.1354 - accuracy: 0.9627 - val_loss: 1.5976 - val_accuracy: 0.7146\n",
      "Epoch 8/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.1209 - accuracy: 0.9668 - val_loss: 1.5651 - val_accuracy: 0.6827\n",
      "Epoch 9/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.1131 - accuracy: 0.9687 - val_loss: 1.6840 - val_accuracy: 0.7114\n",
      "Epoch 10/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.1076 - accuracy: 0.9689 - val_loss: 1.8172 - val_accuracy: 0.6944\n",
      "Epoch 11/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.1033 - accuracy: 0.9702 - val_loss: 1.6231 - val_accuracy: 0.6988\n",
      "Epoch 12/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.1026 - accuracy: 0.9703 - val_loss: 1.7273 - val_accuracy: 0.7024\n",
      "Epoch 13/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0947 - accuracy: 0.9730 - val_loss: 1.8693 - val_accuracy: 0.6940\n",
      "Epoch 14/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.1023 - accuracy: 0.9707 - val_loss: 1.7868 - val_accuracy: 0.6875\n",
      "Epoch 15/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0898 - accuracy: 0.9750 - val_loss: 1.6572 - val_accuracy: 0.6847\n",
      "Epoch 16/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0920 - accuracy: 0.9737 - val_loss: 1.7009 - val_accuracy: 0.7079\n",
      "Epoch 17/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0828 - accuracy: 0.9767 - val_loss: 1.8284 - val_accuracy: 0.6881\n",
      "Epoch 18/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0777 - accuracy: 0.9778 - val_loss: 1.8652 - val_accuracy: 0.6931\n",
      "Epoch 19/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0796 - accuracy: 0.9776 - val_loss: 1.7065 - val_accuracy: 0.7005\n",
      "Epoch 20/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0738 - accuracy: 0.9795 - val_loss: 1.6478 - val_accuracy: 0.7030\n",
      "Epoch 21/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0820 - accuracy: 0.9769 - val_loss: 1.7655 - val_accuracy: 0.7044\n",
      "Epoch 22/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0790 - accuracy: 0.9773 - val_loss: 1.8090 - val_accuracy: 0.6893\n",
      "Epoch 23/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0812 - accuracy: 0.9766 - val_loss: 1.9303 - val_accuracy: 0.6952\n",
      "Epoch 24/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0706 - accuracy: 0.9803 - val_loss: 1.7380 - val_accuracy: 0.6938\n",
      "Epoch 25/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0683 - accuracy: 0.9811 - val_loss: 1.7521 - val_accuracy: 0.6807\n",
      "Epoch 26/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0718 - accuracy: 0.9794 - val_loss: 1.9219 - val_accuracy: 0.6903\n",
      "Epoch 27/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0743 - accuracy: 0.9794 - val_loss: 1.7909 - val_accuracy: 0.6939\n",
      "Epoch 28/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0732 - accuracy: 0.9800 - val_loss: 1.7466 - val_accuracy: 0.7173\n",
      "Epoch 29/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0696 - accuracy: 0.9812 - val_loss: 1.8936 - val_accuracy: 0.6868\n",
      "Epoch 30/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0671 - accuracy: 0.9816 - val_loss: 1.5640 - val_accuracy: 0.7122\n",
      "Epoch 31/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0673 - accuracy: 0.9807 - val_loss: 1.7768 - val_accuracy: 0.6979\n",
      "Epoch 32/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0644 - accuracy: 0.9820 - val_loss: 1.8492 - val_accuracy: 0.7017\n",
      "Epoch 33/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0625 - accuracy: 0.9830 - val_loss: 1.7866 - val_accuracy: 0.7040\n",
      "Epoch 34/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0613 - accuracy: 0.9834 - val_loss: 1.7235 - val_accuracy: 0.6952\n",
      "Epoch 35/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0627 - accuracy: 0.9827 - val_loss: 1.8620 - val_accuracy: 0.6914\n",
      "Epoch 36/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0615 - accuracy: 0.9831 - val_loss: 1.8131 - val_accuracy: 0.6977\n",
      "Epoch 37/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0598 - accuracy: 0.9838 - val_loss: 1.7408 - val_accuracy: 0.6965\n",
      "Epoch 38/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0651 - accuracy: 0.9827 - val_loss: 1.9167 - val_accuracy: 0.7025\n",
      "Epoch 39/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0615 - accuracy: 0.9833 - val_loss: 1.8123 - val_accuracy: 0.7021\n",
      "Epoch 40/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0606 - accuracy: 0.9835 - val_loss: 1.7303 - val_accuracy: 0.7048\n",
      "Epoch 41/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0570 - accuracy: 0.9847 - val_loss: 1.7923 - val_accuracy: 0.6977\n",
      "Epoch 42/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0568 - accuracy: 0.9848 - val_loss: 1.4376 - val_accuracy: 0.7102\n",
      "Epoch 43/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0588 - accuracy: 0.9846 - val_loss: 1.7828 - val_accuracy: 0.6956\n",
      "Epoch 44/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0583 - accuracy: 0.9849 - val_loss: 1.7084 - val_accuracy: 0.6967\n",
      "Epoch 45/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0572 - accuracy: 0.9848 - val_loss: 1.7722 - val_accuracy: 0.6964\n",
      "Epoch 46/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0566 - accuracy: 0.9843 - val_loss: 1.8665 - val_accuracy: 0.7026\n",
      "Epoch 47/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0545 - accuracy: 0.9853 - val_loss: 1.8151 - val_accuracy: 0.7098\n",
      "Epoch 48/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0596 - accuracy: 0.9833 - val_loss: 1.7919 - val_accuracy: 0.6963\n",
      "Epoch 49/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0585 - accuracy: 0.9849 - val_loss: 1.6575 - val_accuracy: 0.6956\n",
      "Epoch 50/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0569 - accuracy: 0.9847 - val_loss: 1.7728 - val_accuracy: 0.7087\n",
      "Epoch 51/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0567 - accuracy: 0.9848 - val_loss: 1.8176 - val_accuracy: 0.6783\n",
      "Epoch 52/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0574 - accuracy: 0.9845 - val_loss: 1.7418 - val_accuracy: 0.7015\n",
      "Epoch 53/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0536 - accuracy: 0.9856 - val_loss: 1.7131 - val_accuracy: 0.6985\n",
      "Epoch 54/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0533 - accuracy: 0.9856 - val_loss: 1.6597 - val_accuracy: 0.6934\n",
      "Epoch 55/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0552 - accuracy: 0.9851 - val_loss: 1.6232 - val_accuracy: 0.7131\n",
      "Epoch 56/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0555 - accuracy: 0.9851 - val_loss: 1.6790 - val_accuracy: 0.6977\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0536 - accuracy: 0.9863 - val_loss: 1.5736 - val_accuracy: 0.6986\n",
      "Epoch 58/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0514 - accuracy: 0.9859 - val_loss: 1.7271 - val_accuracy: 0.6973\n",
      "Epoch 59/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0538 - accuracy: 0.9857 - val_loss: 1.8010 - val_accuracy: 0.6930\n",
      "Epoch 60/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0543 - accuracy: 0.9856 - val_loss: 1.9203 - val_accuracy: 0.7069\n",
      "Epoch 61/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0491 - accuracy: 0.9868 - val_loss: 1.7680 - val_accuracy: 0.6985\n",
      "Epoch 62/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0534 - accuracy: 0.9865 - val_loss: 1.7580 - val_accuracy: 0.6994\n",
      "Epoch 63/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0518 - accuracy: 0.9863 - val_loss: 1.5875 - val_accuracy: 0.6953\n",
      "Epoch 64/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0501 - accuracy: 0.9867 - val_loss: 1.6420 - val_accuracy: 0.6950\n",
      "Epoch 65/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0510 - accuracy: 0.9868 - val_loss: 1.8026 - val_accuracy: 0.7018\n",
      "Epoch 66/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0506 - accuracy: 0.9868 - val_loss: 1.5492 - val_accuracy: 0.6910\n",
      "Epoch 67/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0500 - accuracy: 0.9873 - val_loss: 1.7711 - val_accuracy: 0.6933\n",
      "Epoch 68/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0497 - accuracy: 0.9874 - val_loss: 1.5605 - val_accuracy: 0.7112\n",
      "Epoch 69/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0478 - accuracy: 0.9877 - val_loss: 1.7180 - val_accuracy: 0.7006\n",
      "Epoch 70/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0483 - accuracy: 0.9876 - val_loss: 1.5460 - val_accuracy: 0.7176\n",
      "Epoch 71/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0485 - accuracy: 0.9869 - val_loss: 1.5703 - val_accuracy: 0.7050\n",
      "Epoch 72/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0468 - accuracy: 0.9877 - val_loss: 1.5514 - val_accuracy: 0.6993\n",
      "Epoch 73/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0464 - accuracy: 0.9874 - val_loss: 1.8335 - val_accuracy: 0.6953\n",
      "Epoch 74/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0508 - accuracy: 0.9869 - val_loss: 1.6547 - val_accuracy: 0.7099\n",
      "Epoch 75/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0485 - accuracy: 0.9874 - val_loss: 1.9455 - val_accuracy: 0.6776\n",
      "Epoch 76/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0434 - accuracy: 0.9882 - val_loss: 1.7268 - val_accuracy: 0.6965\n",
      "Epoch 77/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0459 - accuracy: 0.9879 - val_loss: 1.5676 - val_accuracy: 0.7107\n",
      "Epoch 78/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0501 - accuracy: 0.9873 - val_loss: 1.7095 - val_accuracy: 0.6665\n",
      "Epoch 79/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0478 - accuracy: 0.9881 - val_loss: 1.7256 - val_accuracy: 0.6903\n",
      "Epoch 80/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0485 - accuracy: 0.9872 - val_loss: 1.6447 - val_accuracy: 0.6921\n",
      "Epoch 81/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0454 - accuracy: 0.9878 - val_loss: 1.5392 - val_accuracy: 0.6963\n",
      "Epoch 82/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0468 - accuracy: 0.9877 - val_loss: 1.7830 - val_accuracy: 0.7042\n",
      "Epoch 83/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0458 - accuracy: 0.9880 - val_loss: 1.7268 - val_accuracy: 0.6812\n",
      "Epoch 84/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0464 - accuracy: 0.9877 - val_loss: 1.8047 - val_accuracy: 0.7040\n",
      "Epoch 85/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0466 - accuracy: 0.9883 - val_loss: 1.6399 - val_accuracy: 0.7110\n",
      "Epoch 86/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0485 - accuracy: 0.9875 - val_loss: 1.5550 - val_accuracy: 0.7256\n",
      "Epoch 87/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0471 - accuracy: 0.9883 - val_loss: 1.6374 - val_accuracy: 0.7023\n",
      "Epoch 88/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0488 - accuracy: 0.9873 - val_loss: 1.8167 - val_accuracy: 0.6804\n",
      "Epoch 89/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0423 - accuracy: 0.9889 - val_loss: 1.6333 - val_accuracy: 0.7091\n",
      "Epoch 90/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0472 - accuracy: 0.9879 - val_loss: 1.7151 - val_accuracy: 0.7064\n",
      "Epoch 91/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0441 - accuracy: 0.9887 - val_loss: 1.6473 - val_accuracy: 0.7167\n",
      "Epoch 92/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0475 - accuracy: 0.9877 - val_loss: 1.8378 - val_accuracy: 0.6997\n",
      "Epoch 93/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0438 - accuracy: 0.9885 - val_loss: 1.7575 - val_accuracy: 0.7094\n",
      "Epoch 94/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0441 - accuracy: 0.9885 - val_loss: 1.7578 - val_accuracy: 0.7155\n",
      "Epoch 95/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0459 - accuracy: 0.9882 - val_loss: 1.7648 - val_accuracy: 0.7099\n",
      "Epoch 96/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0482 - accuracy: 0.9873 - val_loss: 1.8556 - val_accuracy: 0.7083\n",
      "Epoch 97/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0455 - accuracy: 0.9881 - val_loss: 1.8947 - val_accuracy: 0.7087\n",
      "Epoch 98/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0453 - accuracy: 0.9884 - val_loss: 1.7886 - val_accuracy: 0.6958\n",
      "Epoch 99/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0456 - accuracy: 0.9887 - val_loss: 1.7410 - val_accuracy: 0.7115\n",
      "Epoch 100/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0448 - accuracy: 0.9884 - val_loss: 1.7665 - val_accuracy: 0.6940\n",
      "Epoch 101/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0440 - accuracy: 0.9892 - val_loss: 1.8529 - val_accuracy: 0.6862\n",
      "Epoch 102/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0449 - accuracy: 0.9886 - val_loss: 1.8844 - val_accuracy: 0.7058\n",
      "Epoch 103/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0477 - accuracy: 0.9880 - val_loss: 1.5680 - val_accuracy: 0.7060\n",
      "Epoch 104/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0461 - accuracy: 0.9883 - val_loss: 1.7318 - val_accuracy: 0.6870\n",
      "Epoch 105/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0426 - accuracy: 0.9884 - val_loss: 1.7137 - val_accuracy: 0.7002\n",
      "Epoch 106/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0439 - accuracy: 0.9885 - val_loss: 1.8193 - val_accuracy: 0.6996\n",
      "Epoch 107/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0412 - accuracy: 0.9892 - val_loss: 1.6332 - val_accuracy: 0.6933\n",
      "Epoch 108/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0440 - accuracy: 0.9892 - val_loss: 1.9615 - val_accuracy: 0.7068\n",
      "Epoch 109/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0454 - accuracy: 0.9884 - val_loss: 1.7217 - val_accuracy: 0.7011\n",
      "Epoch 110/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0451 - accuracy: 0.9881 - val_loss: 1.7044 - val_accuracy: 0.7209\n",
      "Epoch 111/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0404 - accuracy: 0.9894 - val_loss: 1.9073 - val_accuracy: 0.6961\n",
      "Epoch 112/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0403 - accuracy: 0.9899 - val_loss: 1.7693 - val_accuracy: 0.7033\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0418 - accuracy: 0.9896 - val_loss: 1.7455 - val_accuracy: 0.7135\n",
      "Epoch 114/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0422 - accuracy: 0.9895 - val_loss: 1.6859 - val_accuracy: 0.7096\n",
      "Epoch 115/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0454 - accuracy: 0.9885 - val_loss: 1.7586 - val_accuracy: 0.7160\n",
      "Epoch 116/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0424 - accuracy: 0.9892 - val_loss: 1.5265 - val_accuracy: 0.7289\n",
      "Epoch 117/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0431 - accuracy: 0.9883 - val_loss: 1.8459 - val_accuracy: 0.7091\n",
      "Epoch 118/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0436 - accuracy: 0.9891 - val_loss: 1.7088 - val_accuracy: 0.7203\n",
      "Epoch 119/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0422 - accuracy: 0.9888 - val_loss: 1.6689 - val_accuracy: 0.7126\n",
      "Epoch 120/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0446 - accuracy: 0.9885 - val_loss: 1.6789 - val_accuracy: 0.7168\n",
      "Epoch 121/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0426 - accuracy: 0.9889 - val_loss: 1.9965 - val_accuracy: 0.6993\n",
      "Epoch 122/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0417 - accuracy: 0.9892 - val_loss: 1.6759 - val_accuracy: 0.7127\n",
      "Epoch 123/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0426 - accuracy: 0.9888 - val_loss: 1.5884 - val_accuracy: 0.6942\n",
      "Epoch 124/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0423 - accuracy: 0.9888 - val_loss: 1.7392 - val_accuracy: 0.7087\n",
      "Epoch 125/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0426 - accuracy: 0.9890 - val_loss: 1.7556 - val_accuracy: 0.7171\n",
      "Epoch 126/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0386 - accuracy: 0.9900 - val_loss: 1.8497 - val_accuracy: 0.6965\n",
      "Epoch 127/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0422 - accuracy: 0.9892 - val_loss: 1.7417 - val_accuracy: 0.6986\n",
      "Epoch 128/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0414 - accuracy: 0.9896 - val_loss: 1.5483 - val_accuracy: 0.7123\n",
      "Epoch 129/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0416 - accuracy: 0.9896 - val_loss: 1.5330 - val_accuracy: 0.7214\n",
      "Epoch 130/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0418 - accuracy: 0.9896 - val_loss: 1.6373 - val_accuracy: 0.7203\n",
      "Epoch 131/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0450 - accuracy: 0.9889 - val_loss: 1.6561 - val_accuracy: 0.7235\n",
      "Epoch 132/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0436 - accuracy: 0.9886 - val_loss: 1.7451 - val_accuracy: 0.6910\n",
      "Epoch 133/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0413 - accuracy: 0.9897 - val_loss: 1.6926 - val_accuracy: 0.7238\n",
      "Epoch 134/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0408 - accuracy: 0.9898 - val_loss: 1.7632 - val_accuracy: 0.7211\n",
      "Epoch 135/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0413 - accuracy: 0.9897 - val_loss: 1.6028 - val_accuracy: 0.7095\n",
      "Epoch 136/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0397 - accuracy: 0.9896 - val_loss: 1.8129 - val_accuracy: 0.7287\n",
      "Epoch 137/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0422 - accuracy: 0.9893 - val_loss: 1.8362 - val_accuracy: 0.7083\n",
      "Epoch 138/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0411 - accuracy: 0.9896 - val_loss: 1.7090 - val_accuracy: 0.7236\n",
      "Epoch 139/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0383 - accuracy: 0.9900 - val_loss: 1.7231 - val_accuracy: 0.7133\n",
      "Epoch 140/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0409 - accuracy: 0.9899 - val_loss: 1.8358 - val_accuracy: 0.7054\n",
      "Epoch 141/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0397 - accuracy: 0.9901 - val_loss: 1.7618 - val_accuracy: 0.6988\n",
      "Epoch 142/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0411 - accuracy: 0.9894 - val_loss: 1.7457 - val_accuracy: 0.7223\n",
      "Epoch 143/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0385 - accuracy: 0.9904 - val_loss: 1.7158 - val_accuracy: 0.7195\n",
      "Epoch 144/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0393 - accuracy: 0.9903 - val_loss: 1.5961 - val_accuracy: 0.7292\n",
      "Epoch 145/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0394 - accuracy: 0.9899 - val_loss: 1.7466 - val_accuracy: 0.7142\n",
      "Epoch 146/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0390 - accuracy: 0.9899 - val_loss: 1.6634 - val_accuracy: 0.6975\n",
      "Epoch 147/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0394 - accuracy: 0.9901 - val_loss: 1.6328 - val_accuracy: 0.7074\n",
      "Epoch 148/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0377 - accuracy: 0.9902 - val_loss: 1.6959 - val_accuracy: 0.7205\n",
      "Epoch 149/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0375 - accuracy: 0.9902 - val_loss: 1.8058 - val_accuracy: 0.7076\n",
      "Epoch 150/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0400 - accuracy: 0.9894 - val_loss: 1.7792 - val_accuracy: 0.7091\n",
      "Epoch 151/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0385 - accuracy: 0.9900 - val_loss: 1.6997 - val_accuracy: 0.7252\n",
      "Epoch 152/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0387 - accuracy: 0.9904 - val_loss: 1.7717 - val_accuracy: 0.7205\n",
      "Epoch 153/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0404 - accuracy: 0.9896 - val_loss: 1.8190 - val_accuracy: 0.7144\n",
      "Epoch 154/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0375 - accuracy: 0.9903 - val_loss: 1.7504 - val_accuracy: 0.7138\n",
      "Epoch 155/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0408 - accuracy: 0.9891 - val_loss: 1.9252 - val_accuracy: 0.7189\n",
      "Epoch 156/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0374 - accuracy: 0.9901 - val_loss: 1.6095 - val_accuracy: 0.7300\n",
      "Epoch 157/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0390 - accuracy: 0.9901 - val_loss: 1.9233 - val_accuracy: 0.7078\n",
      "Epoch 158/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0371 - accuracy: 0.9907 - val_loss: 1.8749 - val_accuracy: 0.7222\n",
      "Epoch 159/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0371 - accuracy: 0.9906 - val_loss: 1.5950 - val_accuracy: 0.7152\n",
      "Epoch 160/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0380 - accuracy: 0.9907 - val_loss: 1.7417 - val_accuracy: 0.7238\n",
      "Epoch 161/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0379 - accuracy: 0.9904 - val_loss: 1.8502 - val_accuracy: 0.7219\n",
      "Epoch 162/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0354 - accuracy: 0.9911 - val_loss: 1.6827 - val_accuracy: 0.7228\n",
      "Epoch 163/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0377 - accuracy: 0.9900 - val_loss: 1.8369 - val_accuracy: 0.6905\n",
      "Epoch 164/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0353 - accuracy: 0.9910 - val_loss: 1.6965 - val_accuracy: 0.7043\n",
      "Epoch 165/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0395 - accuracy: 0.9901 - val_loss: 1.5624 - val_accuracy: 0.7171\n",
      "Epoch 166/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0379 - accuracy: 0.9905 - val_loss: 1.6840 - val_accuracy: 0.7262\n",
      "Epoch 167/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0379 - accuracy: 0.9908 - val_loss: 1.7438 - val_accuracy: 0.7334\n",
      "Epoch 168/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0359 - accuracy: 0.9910 - val_loss: 1.8885 - val_accuracy: 0.7005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0377 - accuracy: 0.9904 - val_loss: 1.7591 - val_accuracy: 0.7127\n",
      "Epoch 170/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0385 - accuracy: 0.9903 - val_loss: 1.6569 - val_accuracy: 0.7236\n",
      "Epoch 171/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0376 - accuracy: 0.9902 - val_loss: 1.6241 - val_accuracy: 0.7297\n",
      "Epoch 172/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0384 - accuracy: 0.9906 - val_loss: 1.6275 - val_accuracy: 0.7270\n",
      "Epoch 173/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0412 - accuracy: 0.9898 - val_loss: 1.6452 - val_accuracy: 0.7235\n",
      "Epoch 174/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0379 - accuracy: 0.9898 - val_loss: 1.5589 - val_accuracy: 0.7323\n",
      "Epoch 175/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0380 - accuracy: 0.9905 - val_loss: 1.4965 - val_accuracy: 0.7182\n",
      "Epoch 176/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0367 - accuracy: 0.9904 - val_loss: 1.4544 - val_accuracy: 0.7050\n",
      "Epoch 177/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0375 - accuracy: 0.9904 - val_loss: 1.5425 - val_accuracy: 0.7132\n",
      "Epoch 178/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0352 - accuracy: 0.9914 - val_loss: 1.6735 - val_accuracy: 0.7055\n",
      "Epoch 179/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0377 - accuracy: 0.9902 - val_loss: 1.4605 - val_accuracy: 0.7421\n",
      "Epoch 180/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0371 - accuracy: 0.9908 - val_loss: 1.4960 - val_accuracy: 0.7133\n",
      "Epoch 181/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0357 - accuracy: 0.9909 - val_loss: 1.4602 - val_accuracy: 0.7210\n",
      "Epoch 182/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0390 - accuracy: 0.9899 - val_loss: 1.3831 - val_accuracy: 0.7400\n",
      "Epoch 183/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0368 - accuracy: 0.9904 - val_loss: 1.7199 - val_accuracy: 0.7273\n",
      "Epoch 184/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0362 - accuracy: 0.9909 - val_loss: 1.6437 - val_accuracy: 0.7337\n",
      "Epoch 185/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0336 - accuracy: 0.9911 - val_loss: 1.7791 - val_accuracy: 0.7253\n",
      "Epoch 186/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0370 - accuracy: 0.9910 - val_loss: 1.7153 - val_accuracy: 0.7337\n",
      "Epoch 187/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0365 - accuracy: 0.9909 - val_loss: 1.8966 - val_accuracy: 0.7249\n",
      "Epoch 188/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0356 - accuracy: 0.9909 - val_loss: 1.7078 - val_accuracy: 0.7283\n",
      "Epoch 189/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0364 - accuracy: 0.9910 - val_loss: 1.8126 - val_accuracy: 0.7053\n",
      "Epoch 190/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0362 - accuracy: 0.9906 - val_loss: 1.6480 - val_accuracy: 0.7237\n",
      "Epoch 191/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0343 - accuracy: 0.9915 - val_loss: 1.7782 - val_accuracy: 0.7329\n",
      "Epoch 192/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0358 - accuracy: 0.9908 - val_loss: 1.4672 - val_accuracy: 0.7108\n",
      "Epoch 193/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0354 - accuracy: 0.9911 - val_loss: 1.9139 - val_accuracy: 0.7223\n",
      "Epoch 194/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0321 - accuracy: 0.9920 - val_loss: 1.7194 - val_accuracy: 0.7216\n",
      "Epoch 195/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0328 - accuracy: 0.9913 - val_loss: 1.5357 - val_accuracy: 0.7243\n",
      "Epoch 196/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0366 - accuracy: 0.9912 - val_loss: 1.8012 - val_accuracy: 0.7061\n",
      "Epoch 197/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0332 - accuracy: 0.9914 - val_loss: 1.7628 - val_accuracy: 0.7058\n",
      "Epoch 198/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0355 - accuracy: 0.9910 - val_loss: 1.6503 - val_accuracy: 0.7169\n",
      "Epoch 199/200\n",
      "1969/1969 [==============================] - 5s 2ms/step - loss: 0.0359 - accuracy: 0.9908 - val_loss: 1.6128 - val_accuracy: 0.7364\n",
      "Epoch 200/200\n",
      "1969/1969 [==============================] - 5s 3ms/step - loss: 0.0359 - accuracy: 0.9909 - val_loss: 1.6282 - val_accuracy: 0.7172\n",
      "INFO:tensorflow:Assets written to: model2/assets\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(units= 5, activation = 'softmax'))\n",
    "model2.compile(optimizer='adam',metrics=['accuracy'],loss='sparse_categorical_crossentropy')\n",
    "model2.fit(x_train, y_train, \n",
    "          epochs=200, \n",
    "          batch_size=64,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "model2.save('model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1/705 [..............................] - ETA: 1:19"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705/705 [==============================] - 1s 705us/step\n",
      "total accuracy: 0.7172196593328601\n",
      "[[4965 2319  174    0    0]\n",
      " [  36 9448  227    0    0]\n",
      " [ 134  531 1756    0    0]\n",
      " [   0 2351  203    0    0]\n",
      " [   0  400    0    0    0]]\n",
      "dos accuracy: 0.665728077232502\n",
      "normal accuracy 0.9729173102667078\n",
      "probe accuracy 0.7253201156546881\n",
      "r2l accuracy 0.0\n",
      "u2r accuracy 0.0\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.67      0.79      7458\n",
      "           1       0.63      0.97      0.76      9711\n",
      "           2       0.74      0.73      0.73      2421\n",
      "           3       0.00      0.00      0.00      2554\n",
      "           4       0.00      0.00      0.00       400\n",
      "\n",
      "    accuracy                           0.72     22544\n",
      "   macro avg       0.47      0.47      0.46     22544\n",
      "weighted avg       0.67      0.72      0.67     22544\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_result = model2.predict_classes(x_test, verbose=1)\n",
    "y_pred = []\n",
    "for label in y_result:\n",
    "    if label == 0:\n",
    "        y_pred.append(0)\n",
    "    elif label == 1:\n",
    "        y_pred.append(1)\n",
    "    elif label == 2:\n",
    "        y_pred.append(2)\n",
    "    elif label == 3:\n",
    "        y_pred.append(3)\n",
    "    else:\n",
    "        y_pred.append(4)\n",
    "\n",
    "print(\"total accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)\n",
    "print(\"dos accuracy:\",result[0][0]/sum(result[0]))\n",
    "print(\"normal accuracy\",result[1][1]/sum(result[1]))\n",
    "print(\"probe accuracy\",result[2][2]/sum(result[2]))\n",
    "print(\"r2l accuracy\",result[3][3]/sum(result[3]))\n",
    "print(\"u2r accuracy\",result[4][4]/sum(result[4]))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1969/1969 [==============================] - 8s 3ms/step - loss: 0.9907 - accuracy: 0.6830 - val_loss: 1.5122 - val_accuracy: 0.6265\n",
      "Epoch 2/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.3474 - accuracy: 0.8957 - val_loss: 1.3209 - val_accuracy: 0.6908\n",
      "Epoch 3/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.2727 - accuracy: 0.9188 - val_loss: 1.3034 - val_accuracy: 0.6976\n",
      "Epoch 4/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.2102 - accuracy: 0.9399 - val_loss: 1.4813 - val_accuracy: 0.6985\n",
      "Epoch 5/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.1833 - accuracy: 0.9498 - val_loss: 1.3786 - val_accuracy: 0.6920\n",
      "Epoch 6/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.1575 - accuracy: 0.9581 - val_loss: 1.5313 - val_accuracy: 0.7042\n",
      "Epoch 7/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.1382 - accuracy: 0.9640 - val_loss: 1.4285 - val_accuracy: 0.7232\n",
      "Epoch 8/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.1348 - accuracy: 0.9645 - val_loss: 1.4298 - val_accuracy: 0.6969\n",
      "Epoch 9/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.1252 - accuracy: 0.9675 - val_loss: 1.4026 - val_accuracy: 0.6973\n",
      "Epoch 10/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.1196 - accuracy: 0.9686 - val_loss: 1.4219 - val_accuracy: 0.6773\n",
      "Epoch 11/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.1201 - accuracy: 0.9688 - val_loss: 1.4211 - val_accuracy: 0.6970\n",
      "Epoch 12/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.1247 - accuracy: 0.9679 - val_loss: 1.4233 - val_accuracy: 0.6828\n",
      "Epoch 13/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.1180 - accuracy: 0.9696 - val_loss: 1.6123 - val_accuracy: 0.6979\n",
      "Epoch 14/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.1128 - accuracy: 0.9707 - val_loss: 1.5503 - val_accuracy: 0.7017\n",
      "Epoch 15/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.1037 - accuracy: 0.9732 - val_loss: 1.8810 - val_accuracy: 0.6928\n",
      "Epoch 16/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.1068 - accuracy: 0.9712 - val_loss: 1.6791 - val_accuracy: 0.6938\n",
      "Epoch 17/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.1013 - accuracy: 0.9725 - val_loss: 1.6588 - val_accuracy: 0.6869\n",
      "Epoch 18/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0932 - accuracy: 0.9749 - val_loss: 1.5909 - val_accuracy: 0.6758\n",
      "Epoch 19/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0946 - accuracy: 0.9738 - val_loss: 1.7541 - val_accuracy: 0.6870\n",
      "Epoch 20/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0908 - accuracy: 0.9749 - val_loss: 1.6256 - val_accuracy: 0.6831\n",
      "Epoch 21/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0898 - accuracy: 0.9758 - val_loss: 1.5611 - val_accuracy: 0.6954\n",
      "Epoch 22/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0874 - accuracy: 0.9748 - val_loss: 1.6731 - val_accuracy: 0.7007\n",
      "Epoch 23/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0862 - accuracy: 0.9765 - val_loss: 1.6860 - val_accuracy: 0.6941\n",
      "Epoch 24/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0874 - accuracy: 0.9748 - val_loss: 1.6511 - val_accuracy: 0.6775\n",
      "Epoch 25/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0827 - accuracy: 0.9772 - val_loss: 1.7356 - val_accuracy: 0.5688\n",
      "Epoch 26/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0835 - accuracy: 0.9767 - val_loss: 1.5673 - val_accuracy: 0.7065\n",
      "Epoch 27/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0784 - accuracy: 0.9791 - val_loss: 1.5709 - val_accuracy: 0.6930\n",
      "Epoch 28/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0789 - accuracy: 0.9782 - val_loss: 1.8338 - val_accuracy: 0.6795\n",
      "Epoch 29/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0795 - accuracy: 0.9784 - val_loss: 1.7181 - val_accuracy: 0.6054\n",
      "Epoch 30/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0808 - accuracy: 0.9780 - val_loss: 1.5597 - val_accuracy: 0.7038\n",
      "Epoch 31/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0772 - accuracy: 0.9787 - val_loss: 1.6822 - val_accuracy: 0.6802\n",
      "Epoch 32/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0799 - accuracy: 0.9784 - val_loss: 1.8040 - val_accuracy: 0.6930\n",
      "Epoch 33/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0783 - accuracy: 0.9786 - val_loss: 1.5459 - val_accuracy: 0.6904\n",
      "Epoch 34/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0739 - accuracy: 0.9797 - val_loss: 1.4582 - val_accuracy: 0.6978\n",
      "Epoch 35/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0727 - accuracy: 0.9803 - val_loss: 1.7494 - val_accuracy: 0.6924\n",
      "Epoch 36/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0716 - accuracy: 0.9805 - val_loss: 1.5217 - val_accuracy: 0.6953\n",
      "Epoch 37/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0707 - accuracy: 0.9807 - val_loss: 1.8583 - val_accuracy: 0.6948\n",
      "Epoch 38/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0735 - accuracy: 0.9800 - val_loss: 1.9760 - val_accuracy: 0.6925\n",
      "Epoch 39/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0647 - accuracy: 0.9824 - val_loss: 1.7247 - val_accuracy: 0.7110\n",
      "Epoch 40/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0674 - accuracy: 0.9815 - val_loss: 1.9425 - val_accuracy: 0.7046\n",
      "Epoch 41/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0675 - accuracy: 0.9824 - val_loss: 1.7552 - val_accuracy: 0.7122\n",
      "Epoch 42/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0632 - accuracy: 0.9827 - val_loss: 1.5397 - val_accuracy: 0.7210\n",
      "Epoch 43/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0708 - accuracy: 0.9806 - val_loss: 1.6329 - val_accuracy: 0.6818\n",
      "Epoch 44/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0680 - accuracy: 0.9817 - val_loss: 1.7694 - val_accuracy: 0.7134\n",
      "Epoch 45/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0637 - accuracy: 0.9829 - val_loss: 1.7421 - val_accuracy: 0.7119\n",
      "Epoch 46/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0646 - accuracy: 0.9822 - val_loss: 1.8862 - val_accuracy: 0.6517\n",
      "Epoch 47/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0638 - accuracy: 0.9824 - val_loss: 1.5950 - val_accuracy: 0.7038\n",
      "Epoch 48/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0668 - accuracy: 0.9822 - val_loss: 1.5704 - val_accuracy: 0.6857\n",
      "Epoch 49/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0641 - accuracy: 0.9824 - val_loss: 1.4728 - val_accuracy: 0.6926\n",
      "Epoch 50/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0614 - accuracy: 0.9838 - val_loss: 1.5201 - val_accuracy: 0.6934\n",
      "Epoch 51/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0632 - accuracy: 0.9830 - val_loss: 1.8012 - val_accuracy: 0.7047\n",
      "Epoch 52/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0621 - accuracy: 0.9834 - val_loss: 1.4235 - val_accuracy: 0.7357\n",
      "Epoch 53/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0652 - accuracy: 0.9827 - val_loss: 1.5927 - val_accuracy: 0.5976\n",
      "Epoch 54/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0612 - accuracy: 0.9840 - val_loss: 1.8269 - val_accuracy: 0.7033\n",
      "Epoch 55/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0599 - accuracy: 0.9839 - val_loss: 1.6239 - val_accuracy: 0.6965\n",
      "Epoch 56/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0633 - accuracy: 0.9834 - val_loss: 1.6795 - val_accuracy: 0.7101\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0594 - accuracy: 0.9840 - val_loss: 1.3971 - val_accuracy: 0.6994\n",
      "Epoch 58/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0614 - accuracy: 0.9835 - val_loss: 1.8703 - val_accuracy: 0.6989\n",
      "Epoch 59/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0602 - accuracy: 0.9841 - val_loss: 1.8063 - val_accuracy: 0.7003\n",
      "Epoch 60/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0576 - accuracy: 0.9855 - val_loss: 1.7369 - val_accuracy: 0.7031\n",
      "Epoch 61/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0602 - accuracy: 0.9851 - val_loss: 1.7815 - val_accuracy: 0.6925\n",
      "Epoch 62/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0587 - accuracy: 0.9846 - val_loss: 1.5740 - val_accuracy: 0.7072\n",
      "Epoch 63/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0582 - accuracy: 0.9845 - val_loss: 1.5838 - val_accuracy: 0.7162\n",
      "Epoch 64/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0600 - accuracy: 0.9850 - val_loss: 1.7957 - val_accuracy: 0.6964\n",
      "Epoch 65/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0603 - accuracy: 0.9848 - val_loss: 1.7427 - val_accuracy: 0.6150\n",
      "Epoch 66/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0575 - accuracy: 0.9853 - val_loss: 1.7272 - val_accuracy: 0.7068\n",
      "Epoch 67/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0572 - accuracy: 0.9854 - val_loss: 1.6947 - val_accuracy: 0.6969\n",
      "Epoch 68/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0573 - accuracy: 0.9851 - val_loss: 1.7186 - val_accuracy: 0.7086\n",
      "Epoch 69/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0520 - accuracy: 0.9869 - val_loss: 1.6309 - val_accuracy: 0.7083\n",
      "Epoch 70/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0572 - accuracy: 0.9853 - val_loss: 1.5591 - val_accuracy: 0.7073\n",
      "Epoch 71/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0523 - accuracy: 0.9867 - val_loss: 1.6641 - val_accuracy: 0.7209\n",
      "Epoch 72/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0561 - accuracy: 0.9853 - val_loss: 1.9129 - val_accuracy: 0.7134\n",
      "Epoch 73/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0519 - accuracy: 0.9870 - val_loss: 1.6720 - val_accuracy: 0.7022\n",
      "Epoch 74/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0618 - accuracy: 0.9846 - val_loss: 1.5980 - val_accuracy: 0.7144\n",
      "Epoch 75/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0527 - accuracy: 0.9867 - val_loss: 1.7606 - val_accuracy: 0.6705\n",
      "Epoch 76/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0548 - accuracy: 0.9862 - val_loss: 1.4672 - val_accuracy: 0.7165\n",
      "Epoch 77/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0564 - accuracy: 0.9856 - val_loss: 1.7452 - val_accuracy: 0.7138\n",
      "Epoch 78/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0542 - accuracy: 0.9862 - val_loss: 1.7655 - val_accuracy: 0.7059\n",
      "Epoch 79/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0560 - accuracy: 0.9851 - val_loss: 1.7101 - val_accuracy: 0.6956\n",
      "Epoch 80/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0506 - accuracy: 0.9870 - val_loss: 1.7637 - val_accuracy: 0.7263\n",
      "Epoch 81/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0525 - accuracy: 0.9872 - val_loss: 1.8077 - val_accuracy: 0.7039\n",
      "Epoch 82/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0540 - accuracy: 0.9862 - val_loss: 1.8281 - val_accuracy: 0.7002\n",
      "Epoch 83/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0542 - accuracy: 0.9865 - val_loss: 1.6865 - val_accuracy: 0.7073\n",
      "Epoch 84/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0521 - accuracy: 0.9868 - val_loss: 1.7034 - val_accuracy: 0.7099\n",
      "Epoch 85/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0500 - accuracy: 0.9873 - val_loss: 1.6938 - val_accuracy: 0.7011\n",
      "Epoch 86/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0510 - accuracy: 0.9874 - val_loss: 1.7223 - val_accuracy: 0.7263\n",
      "Epoch 87/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0559 - accuracy: 0.9859 - val_loss: 1.7005 - val_accuracy: 0.6942\n",
      "Epoch 88/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0542 - accuracy: 0.9861 - val_loss: 1.6634 - val_accuracy: 0.6887\n",
      "Epoch 89/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0515 - accuracy: 0.9867 - val_loss: 1.6412 - val_accuracy: 0.7106\n",
      "Epoch 90/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0518 - accuracy: 0.9873 - val_loss: 1.7482 - val_accuracy: 0.6580\n",
      "Epoch 91/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0472 - accuracy: 0.9882 - val_loss: 1.7145 - val_accuracy: 0.6924\n",
      "Epoch 92/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0521 - accuracy: 0.9872 - val_loss: 1.7746 - val_accuracy: 0.6963\n",
      "Epoch 93/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0525 - accuracy: 0.9868 - val_loss: 1.6584 - val_accuracy: 0.7065\n",
      "Epoch 94/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0507 - accuracy: 0.9874 - val_loss: 1.9251 - val_accuracy: 0.7024\n",
      "Epoch 95/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0504 - accuracy: 0.9871 - val_loss: 1.7881 - val_accuracy: 0.6633\n",
      "Epoch 96/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0505 - accuracy: 0.9870 - val_loss: 1.7504 - val_accuracy: 0.6914\n",
      "Epoch 97/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0526 - accuracy: 0.9867 - val_loss: 2.0754 - val_accuracy: 0.6944\n",
      "Epoch 98/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0543 - accuracy: 0.9863 - val_loss: 1.6938 - val_accuracy: 0.6856\n",
      "Epoch 99/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0507 - accuracy: 0.9873 - val_loss: 1.7041 - val_accuracy: 0.7046\n",
      "Epoch 100/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0494 - accuracy: 0.9880 - val_loss: 1.5783 - val_accuracy: 0.7133\n",
      "Epoch 101/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0513 - accuracy: 0.9869 - val_loss: 1.7785 - val_accuracy: 0.6886\n",
      "Epoch 102/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0514 - accuracy: 0.9875 - val_loss: 1.6743 - val_accuracy: 0.7061\n",
      "Epoch 103/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0504 - accuracy: 0.9874 - val_loss: 1.7219 - val_accuracy: 0.6975\n",
      "Epoch 104/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0488 - accuracy: 0.9877 - val_loss: 1.4572 - val_accuracy: 0.7012\n",
      "Epoch 105/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0507 - accuracy: 0.9868 - val_loss: 1.8465 - val_accuracy: 0.6968\n",
      "Epoch 106/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0514 - accuracy: 0.9866 - val_loss: 1.7077 - val_accuracy: 0.5913\n",
      "Epoch 107/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0495 - accuracy: 0.9875 - val_loss: 1.4582 - val_accuracy: 0.7186\n",
      "Epoch 108/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0478 - accuracy: 0.9884 - val_loss: 1.6332 - val_accuracy: 0.6166\n",
      "Epoch 109/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0504 - accuracy: 0.9880 - val_loss: 1.6451 - val_accuracy: 0.7120\n",
      "Epoch 110/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0472 - accuracy: 0.9884 - val_loss: 1.5698 - val_accuracy: 0.6969\n",
      "Epoch 111/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0497 - accuracy: 0.9874 - val_loss: 1.4109 - val_accuracy: 0.6964\n",
      "Epoch 112/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0495 - accuracy: 0.9875 - val_loss: 1.5500 - val_accuracy: 0.6999\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0496 - accuracy: 0.9876 - val_loss: 1.5628 - val_accuracy: 0.7186\n",
      "Epoch 114/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0489 - accuracy: 0.9873 - val_loss: 1.6163 - val_accuracy: 0.7053\n",
      "Epoch 115/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0482 - accuracy: 0.9882 - val_loss: 1.6772 - val_accuracy: 0.7097\n",
      "Epoch 116/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0481 - accuracy: 0.9879 - val_loss: 1.6530 - val_accuracy: 0.7120\n",
      "Epoch 117/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0473 - accuracy: 0.9883 - val_loss: 1.4418 - val_accuracy: 0.7142\n",
      "Epoch 118/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0471 - accuracy: 0.9884 - val_loss: 1.6796 - val_accuracy: 0.6978\n",
      "Epoch 119/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0482 - accuracy: 0.9874 - val_loss: 1.8577 - val_accuracy: 0.7121\n",
      "Epoch 120/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0487 - accuracy: 0.9880 - val_loss: 1.5505 - val_accuracy: 0.7142\n",
      "Epoch 121/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0484 - accuracy: 0.9876 - val_loss: 1.6939 - val_accuracy: 0.6952\n",
      "Epoch 122/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0453 - accuracy: 0.9887 - val_loss: 1.6857 - val_accuracy: 0.7046\n",
      "Epoch 123/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0473 - accuracy: 0.9880 - val_loss: 1.7313 - val_accuracy: 0.7055\n",
      "Epoch 124/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0470 - accuracy: 0.9883 - val_loss: 1.6678 - val_accuracy: 0.7170\n",
      "Epoch 125/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0455 - accuracy: 0.9889 - val_loss: 1.6495 - val_accuracy: 0.7045\n",
      "Epoch 126/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0435 - accuracy: 0.9891 - val_loss: 1.6677 - val_accuracy: 0.7112\n",
      "Epoch 127/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0450 - accuracy: 0.9889 - val_loss: 1.5799 - val_accuracy: 0.7138\n",
      "Epoch 128/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0451 - accuracy: 0.9885 - val_loss: 1.7209 - val_accuracy: 0.7083\n",
      "Epoch 129/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0462 - accuracy: 0.9889 - val_loss: 1.6664 - val_accuracy: 0.7007\n",
      "Epoch 130/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0460 - accuracy: 0.9888 - val_loss: 1.6912 - val_accuracy: 0.6967\n",
      "Epoch 131/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0437 - accuracy: 0.9891 - val_loss: 1.5672 - val_accuracy: 0.7123\n",
      "Epoch 132/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0473 - accuracy: 0.9888 - val_loss: 1.7246 - val_accuracy: 0.7142\n",
      "Epoch 133/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0439 - accuracy: 0.9895 - val_loss: 1.6484 - val_accuracy: 0.7001\n",
      "Epoch 134/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0454 - accuracy: 0.9890 - val_loss: 1.6031 - val_accuracy: 0.7017\n",
      "Epoch 135/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0435 - accuracy: 0.9890 - val_loss: 1.5419 - val_accuracy: 0.7137\n",
      "Epoch 136/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0475 - accuracy: 0.9884 - val_loss: 1.5966 - val_accuracy: 0.7043\n",
      "Epoch 137/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0451 - accuracy: 0.9886 - val_loss: 1.6499 - val_accuracy: 0.6868\n",
      "Epoch 138/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0432 - accuracy: 0.9895 - val_loss: 1.8235 - val_accuracy: 0.6895\n",
      "Epoch 139/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0459 - accuracy: 0.9886 - val_loss: 1.6896 - val_accuracy: 0.6825\n",
      "Epoch 140/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0461 - accuracy: 0.9887 - val_loss: 1.5622 - val_accuracy: 0.6836\n",
      "Epoch 141/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0468 - accuracy: 0.9886 - val_loss: 1.6789 - val_accuracy: 0.6919\n",
      "Epoch 142/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0437 - accuracy: 0.9893 - val_loss: 1.8602 - val_accuracy: 0.5078\n",
      "Epoch 143/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0416 - accuracy: 0.9896 - val_loss: 1.7295 - val_accuracy: 0.6593\n",
      "Epoch 144/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0474 - accuracy: 0.9886 - val_loss: 1.7805 - val_accuracy: 0.6909\n",
      "Epoch 145/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0453 - accuracy: 0.9885 - val_loss: 1.7587 - val_accuracy: 0.6891\n",
      "Epoch 146/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0451 - accuracy: 0.9889 - val_loss: 1.9376 - val_accuracy: 0.6360\n",
      "Epoch 147/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0421 - accuracy: 0.9895 - val_loss: 1.7160 - val_accuracy: 0.7000\n",
      "Epoch 148/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0441 - accuracy: 0.9895 - val_loss: 1.5691 - val_accuracy: 0.7111\n",
      "Epoch 149/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0422 - accuracy: 0.9894 - val_loss: 1.6511 - val_accuracy: 0.5772\n",
      "Epoch 150/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0429 - accuracy: 0.9895 - val_loss: 1.6122 - val_accuracy: 0.5112\n",
      "Epoch 151/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0412 - accuracy: 0.9901 - val_loss: 1.5545 - val_accuracy: 0.7209\n",
      "Epoch 152/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0404 - accuracy: 0.9901 - val_loss: 1.7948 - val_accuracy: 0.7039\n",
      "Epoch 153/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0456 - accuracy: 0.9885 - val_loss: 1.6533 - val_accuracy: 0.6999\n",
      "Epoch 154/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0442 - accuracy: 0.9893 - val_loss: 1.8177 - val_accuracy: 0.6986\n",
      "Epoch 155/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0459 - accuracy: 0.9885 - val_loss: 1.6510 - val_accuracy: 0.7062\n",
      "Epoch 156/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0441 - accuracy: 0.9895 - val_loss: 1.7111 - val_accuracy: 0.7099\n",
      "Epoch 157/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0440 - accuracy: 0.9892 - val_loss: 1.6643 - val_accuracy: 0.6922\n",
      "Epoch 158/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0426 - accuracy: 0.9892 - val_loss: 1.6854 - val_accuracy: 0.7045\n",
      "Epoch 159/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0414 - accuracy: 0.9899 - val_loss: 1.5160 - val_accuracy: 0.6937\n",
      "Epoch 160/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0427 - accuracy: 0.9894 - val_loss: 1.7182 - val_accuracy: 0.7048\n",
      "Epoch 161/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0426 - accuracy: 0.9894 - val_loss: 1.5508 - val_accuracy: 0.7024\n",
      "Epoch 162/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0440 - accuracy: 0.9894 - val_loss: 1.6229 - val_accuracy: 0.7145\n",
      "Epoch 163/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0429 - accuracy: 0.9897 - val_loss: 1.8058 - val_accuracy: 0.7138\n",
      "Epoch 164/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0414 - accuracy: 0.9899 - val_loss: 1.8271 - val_accuracy: 0.6857\n",
      "Epoch 165/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0433 - accuracy: 0.9899 - val_loss: 1.7668 - val_accuracy: 0.7122\n",
      "Epoch 166/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0439 - accuracy: 0.9888 - val_loss: 1.6248 - val_accuracy: 0.7116\n",
      "Epoch 167/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0408 - accuracy: 0.9899 - val_loss: 1.5330 - val_accuracy: 0.6967\n",
      "Epoch 168/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0412 - accuracy: 0.9900 - val_loss: 1.6829 - val_accuracy: 0.7101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0423 - accuracy: 0.9898 - val_loss: 1.7881 - val_accuracy: 0.7127\n",
      "Epoch 170/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0423 - accuracy: 0.9899 - val_loss: 1.6254 - val_accuracy: 0.7160\n",
      "Epoch 171/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0412 - accuracy: 0.9895 - val_loss: 1.7478 - val_accuracy: 0.6970\n",
      "Epoch 172/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0424 - accuracy: 0.9892 - val_loss: 1.7748 - val_accuracy: 0.7066\n",
      "Epoch 173/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0411 - accuracy: 0.9897 - val_loss: 1.5791 - val_accuracy: 0.6965\n",
      "Epoch 174/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0432 - accuracy: 0.9891 - val_loss: 1.5866 - val_accuracy: 0.6967\n",
      "Epoch 175/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0436 - accuracy: 0.9889 - val_loss: 1.6287 - val_accuracy: 0.7022\n",
      "Epoch 176/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0441 - accuracy: 0.9891 - val_loss: 1.5957 - val_accuracy: 0.7163\n",
      "Epoch 177/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0422 - accuracy: 0.9894 - val_loss: 1.8254 - val_accuracy: 0.7104\n",
      "Epoch 178/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0380 - accuracy: 0.9910 - val_loss: 1.8217 - val_accuracy: 0.6662\n",
      "Epoch 179/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0462 - accuracy: 0.9888 - val_loss: 1.5069 - val_accuracy: 0.7015\n",
      "Epoch 180/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0464 - accuracy: 0.9886 - val_loss: 1.6476 - val_accuracy: 0.7161\n",
      "Epoch 181/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0381 - accuracy: 0.9903 - val_loss: 1.5885 - val_accuracy: 0.7027\n",
      "Epoch 182/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0415 - accuracy: 0.9894 - val_loss: 1.7269 - val_accuracy: 0.7127\n",
      "Epoch 183/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0425 - accuracy: 0.9895 - val_loss: 1.6976 - val_accuracy: 0.7003\n",
      "Epoch 184/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0426 - accuracy: 0.9891 - val_loss: 1.5748 - val_accuracy: 0.7217\n",
      "Epoch 185/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0427 - accuracy: 0.9892 - val_loss: 1.6936 - val_accuracy: 0.7113\n",
      "Epoch 186/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0396 - accuracy: 0.9901 - val_loss: 1.6539 - val_accuracy: 0.7097\n",
      "Epoch 187/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0415 - accuracy: 0.9896 - val_loss: 1.6636 - val_accuracy: 0.7123\n",
      "Epoch 188/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0413 - accuracy: 0.9899 - val_loss: 1.7283 - val_accuracy: 0.7075\n",
      "Epoch 189/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0419 - accuracy: 0.9897 - val_loss: 1.6486 - val_accuracy: 0.7039\n",
      "Epoch 190/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0406 - accuracy: 0.9900 - val_loss: 1.6677 - val_accuracy: 0.6927\n",
      "Epoch 191/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0394 - accuracy: 0.9906 - val_loss: 1.6653 - val_accuracy: 0.6995\n",
      "Epoch 192/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0391 - accuracy: 0.9906 - val_loss: 1.6947 - val_accuracy: 0.7140\n",
      "Epoch 193/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0415 - accuracy: 0.9903 - val_loss: 1.8138 - val_accuracy: 0.6916\n",
      "Epoch 194/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0410 - accuracy: 0.9898 - val_loss: 1.7426 - val_accuracy: 0.7131\n",
      "Epoch 195/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0401 - accuracy: 0.9904 - val_loss: 1.6342 - val_accuracy: 0.7159\n",
      "Epoch 196/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0412 - accuracy: 0.9897 - val_loss: 1.6784 - val_accuracy: 0.6782\n",
      "Epoch 197/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0409 - accuracy: 0.9899 - val_loss: 1.6412 - val_accuracy: 0.7043\n",
      "Epoch 198/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0415 - accuracy: 0.9896 - val_loss: 1.6038 - val_accuracy: 0.7015\n",
      "Epoch 199/200\n",
      "1969/1969 [==============================] - 7s 3ms/step - loss: 0.0409 - accuracy: 0.9900 - val_loss: 1.6511 - val_accuracy: 0.6901\n",
      "Epoch 200/200\n",
      "1969/1969 [==============================] - 6s 3ms/step - loss: 0.0393 - accuracy: 0.9905 - val_loss: 1.5635 - val_accuracy: 0.7255\n",
      "INFO:tensorflow:Assets written to: model3/assets\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(units= 5, activation = 'softmax'))\n",
    "model3.compile(optimizer='adam',metrics=['accuracy'],loss='sparse_categorical_crossentropy')\n",
    "model3.fit(x_train, y_train, \n",
    "          epochs=200, \n",
    "          batch_size=64,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "model3.save('model3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1/705 [..............................] - ETA: 1:39"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705/705 [==============================] - 1s 700us/step\n",
      "total accuracy: 0.7254701916252662\n",
      "[[5457 1990   11    0    0]\n",
      " [  40 9452  214    5    0]\n",
      " [ 172  805 1444    0    0]\n",
      " [   0 2548    4    2    0]\n",
      " [   0  400    0    0    0]]\n",
      "dos accuracy: 0.7316975060337892\n",
      "normal accuracy 0.9733292142930697\n",
      "probe accuracy 0.5964477488641058\n",
      "r2l accuracy 0.0007830853563038371\n",
      "u2r accuracy 0.0\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.73      0.83      7458\n",
      "           1       0.62      0.97      0.76      9711\n",
      "           2       0.86      0.60      0.71      2421\n",
      "           3       0.29      0.00      0.00      2554\n",
      "           4       0.00      0.00      0.00       400\n",
      "\n",
      "    accuracy                           0.73     22544\n",
      "   macro avg       0.55      0.46      0.46     22544\n",
      "weighted avg       0.71      0.73      0.68     22544\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_result = model3.predict_classes(x_test, verbose=1)\n",
    "y_pred = []\n",
    "for label in y_result:\n",
    "    if label == 0:\n",
    "        y_pred.append(0)\n",
    "    elif label == 1:\n",
    "        y_pred.append(1)\n",
    "    elif label == 2:\n",
    "        y_pred.append(2)\n",
    "    elif label == 3:\n",
    "        y_pred.append(3)\n",
    "    else:\n",
    "        y_pred.append(4)\n",
    "\n",
    "print(\"total accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)\n",
    "print(\"dos accuracy:\",result[0][0]/sum(result[0]))\n",
    "print(\"normal accuracy\",result[1][1]/sum(result[1]))\n",
    "print(\"probe accuracy\",result[2][2]/sum(result[2]))\n",
    "print(\"r2l accuracy\",result[3][3]/sum(result[3]))\n",
    "print(\"u2r accuracy\",result[4][4]/sum(result[4]))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1969/1969 [==============================] - 10s 4ms/step - loss: 1.2196 - accuracy: 0.5438 - val_loss: 1.5625 - val_accuracy: 0.6311\n",
      "Epoch 2/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.4537 - accuracy: 0.8585 - val_loss: 1.4442 - val_accuracy: 0.6420\n",
      "Epoch 3/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.3385 - accuracy: 0.8954 - val_loss: 1.3435 - val_accuracy: 0.6853\n",
      "Epoch 4/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.2599 - accuracy: 0.9251 - val_loss: 1.3409 - val_accuracy: 0.6845\n",
      "Epoch 5/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.2152 - accuracy: 0.9396 - val_loss: 1.3616 - val_accuracy: 0.6916\n",
      "Epoch 6/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.1836 - accuracy: 0.9504 - val_loss: 1.4376 - val_accuracy: 0.7027\n",
      "Epoch 7/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.1664 - accuracy: 0.9567 - val_loss: 1.4625 - val_accuracy: 0.6906\n",
      "Epoch 8/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.1599 - accuracy: 0.9575 - val_loss: 1.6001 - val_accuracy: 0.7144\n",
      "Epoch 9/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.1471 - accuracy: 0.9618 - val_loss: 1.3199 - val_accuracy: 0.6941\n",
      "Epoch 10/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.1417 - accuracy: 0.9639 - val_loss: 1.6371 - val_accuracy: 0.6637\n",
      "Epoch 11/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.1399 - accuracy: 0.9644 - val_loss: 1.6135 - val_accuracy: 0.6808\n",
      "Epoch 12/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.1321 - accuracy: 0.9669 - val_loss: 1.6008 - val_accuracy: 0.7073\n",
      "Epoch 13/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.1220 - accuracy: 0.9688 - val_loss: 1.7313 - val_accuracy: 0.6790\n",
      "Epoch 14/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.1237 - accuracy: 0.9683 - val_loss: 1.6632 - val_accuracy: 0.6782\n",
      "Epoch 15/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.1203 - accuracy: 0.9694 - val_loss: 1.5189 - val_accuracy: 0.6961\n",
      "Epoch 16/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.1130 - accuracy: 0.9709 - val_loss: 1.7592 - val_accuracy: 0.6973\n",
      "Epoch 17/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.1158 - accuracy: 0.9701 - val_loss: 1.9155 - val_accuracy: 0.7082\n",
      "Epoch 18/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.1020 - accuracy: 0.9741 - val_loss: 1.5028 - val_accuracy: 0.7182\n",
      "Epoch 19/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0987 - accuracy: 0.9749 - val_loss: 1.7493 - val_accuracy: 0.7129\n",
      "Epoch 20/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0982 - accuracy: 0.9743 - val_loss: 1.5031 - val_accuracy: 0.6965\n",
      "Epoch 21/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0971 - accuracy: 0.9750 - val_loss: 1.4311 - val_accuracy: 0.7078\n",
      "Epoch 22/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0976 - accuracy: 0.9734 - val_loss: 1.6697 - val_accuracy: 0.6989\n",
      "Epoch 23/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0999 - accuracy: 0.9734 - val_loss: 1.4809 - val_accuracy: 0.6587\n",
      "Epoch 24/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0925 - accuracy: 0.9751 - val_loss: 1.3569 - val_accuracy: 0.6757\n",
      "Epoch 25/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0934 - accuracy: 0.9748 - val_loss: 1.4209 - val_accuracy: 0.6794\n",
      "Epoch 26/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0893 - accuracy: 0.9756 - val_loss: 1.5234 - val_accuracy: 0.6982\n",
      "Epoch 27/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0900 - accuracy: 0.9756 - val_loss: 1.4152 - val_accuracy: 0.6764\n",
      "Epoch 28/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0831 - accuracy: 0.9775 - val_loss: 1.6795 - val_accuracy: 0.6438\n",
      "Epoch 29/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0839 - accuracy: 0.9771 - val_loss: 1.4701 - val_accuracy: 0.6995\n",
      "Epoch 30/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0820 - accuracy: 0.9776 - val_loss: 1.3970 - val_accuracy: 0.7056\n",
      "Epoch 31/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0786 - accuracy: 0.9790 - val_loss: 1.6975 - val_accuracy: 0.7054\n",
      "Epoch 32/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0778 - accuracy: 0.9788 - val_loss: 1.7839 - val_accuracy: 0.7040\n",
      "Epoch 33/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0780 - accuracy: 0.9784 - val_loss: 1.7325 - val_accuracy: 0.6874\n",
      "Epoch 34/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0755 - accuracy: 0.9791 - val_loss: 1.6593 - val_accuracy: 0.6984\n",
      "Epoch 35/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0776 - accuracy: 0.9788 - val_loss: 1.5975 - val_accuracy: 0.6974\n",
      "Epoch 36/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0782 - accuracy: 0.9785 - val_loss: 1.7834 - val_accuracy: 0.6846\n",
      "Epoch 37/200\n",
      "1969/1969 [==============================] - 9s 4ms/step - loss: 0.0784 - accuracy: 0.9794 - val_loss: 1.6062 - val_accuracy: 0.7002\n",
      "Epoch 38/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0747 - accuracy: 0.9801 - val_loss: 1.7304 - val_accuracy: 0.6764\n",
      "Epoch 39/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0762 - accuracy: 0.9793 - val_loss: 1.7872 - val_accuracy: 0.6784\n",
      "Epoch 40/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0736 - accuracy: 0.9801 - val_loss: 1.6699 - val_accuracy: 0.6744\n",
      "Epoch 41/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0734 - accuracy: 0.9806 - val_loss: 1.5751 - val_accuracy: 0.7028\n",
      "Epoch 42/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0722 - accuracy: 0.9800 - val_loss: 1.7569 - val_accuracy: 0.6742\n",
      "Epoch 43/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0693 - accuracy: 0.9816 - val_loss: 1.5293 - val_accuracy: 0.6985\n",
      "Epoch 44/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0674 - accuracy: 0.9827 - val_loss: 1.8793 - val_accuracy: 0.6903\n",
      "Epoch 45/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0694 - accuracy: 0.9814 - val_loss: 1.6448 - val_accuracy: 0.6910\n",
      "Epoch 46/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0704 - accuracy: 0.9810 - val_loss: 1.6759 - val_accuracy: 0.6964\n",
      "Epoch 47/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0696 - accuracy: 0.9814 - val_loss: 1.6918 - val_accuracy: 0.6931\n",
      "Epoch 48/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0681 - accuracy: 0.9813 - val_loss: 1.6410 - val_accuracy: 0.7025\n",
      "Epoch 49/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0667 - accuracy: 0.9818 - val_loss: 1.7636 - val_accuracy: 0.6915\n",
      "Epoch 50/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0719 - accuracy: 0.9806 - val_loss: 1.6218 - val_accuracy: 0.6704\n",
      "Epoch 51/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0657 - accuracy: 0.9825 - val_loss: 1.6082 - val_accuracy: 0.6890\n",
      "Epoch 52/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0666 - accuracy: 0.9825 - val_loss: 1.8472 - val_accuracy: 0.7057\n",
      "Epoch 53/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0646 - accuracy: 0.9824 - val_loss: 1.4057 - val_accuracy: 0.7299\n",
      "Epoch 54/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0699 - accuracy: 0.9821 - val_loss: 1.5135 - val_accuracy: 0.7072\n",
      "Epoch 55/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0665 - accuracy: 0.9824 - val_loss: 1.8420 - val_accuracy: 0.6866\n",
      "Epoch 56/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0647 - accuracy: 0.9832 - val_loss: 1.4162 - val_accuracy: 0.6983\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0650 - accuracy: 0.9827 - val_loss: 1.7219 - val_accuracy: 0.7185\n",
      "Epoch 58/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0656 - accuracy: 0.9833 - val_loss: 1.4786 - val_accuracy: 0.7214\n",
      "Epoch 59/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0620 - accuracy: 0.9840 - val_loss: 1.6428 - val_accuracy: 0.6997\n",
      "Epoch 60/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0667 - accuracy: 0.9829 - val_loss: 1.4866 - val_accuracy: 0.7067\n",
      "Epoch 61/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0618 - accuracy: 0.9842 - val_loss: 1.6570 - val_accuracy: 0.7011\n",
      "Epoch 62/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0607 - accuracy: 0.9843 - val_loss: 1.6232 - val_accuracy: 0.6934\n",
      "Epoch 63/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0592 - accuracy: 0.9847 - val_loss: 1.7911 - val_accuracy: 0.7005\n",
      "Epoch 64/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0603 - accuracy: 0.9842 - val_loss: 1.5669 - val_accuracy: 0.6838\n",
      "Epoch 65/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0584 - accuracy: 0.9850 - val_loss: 1.8097 - val_accuracy: 0.6932\n",
      "Epoch 66/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0626 - accuracy: 0.9834 - val_loss: 1.7197 - val_accuracy: 0.6971\n",
      "Epoch 67/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0610 - accuracy: 0.9840 - val_loss: 1.8071 - val_accuracy: 0.6929\n",
      "Epoch 68/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0611 - accuracy: 0.9830 - val_loss: 1.7668 - val_accuracy: 0.7047\n",
      "Epoch 69/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0593 - accuracy: 0.9842 - val_loss: 1.5610 - val_accuracy: 0.6939\n",
      "Epoch 70/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0631 - accuracy: 0.9837 - val_loss: 1.7293 - val_accuracy: 0.7054\n",
      "Epoch 71/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0585 - accuracy: 0.9847 - val_loss: 1.4161 - val_accuracy: 0.7019\n",
      "Epoch 72/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0624 - accuracy: 0.9839 - val_loss: 1.6141 - val_accuracy: 0.7075\n",
      "Epoch 73/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0587 - accuracy: 0.9847 - val_loss: 1.4982 - val_accuracy: 0.7031\n",
      "Epoch 74/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0565 - accuracy: 0.9857 - val_loss: 1.4739 - val_accuracy: 0.7167\n",
      "Epoch 75/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0597 - accuracy: 0.9841 - val_loss: 1.4987 - val_accuracy: 0.7143\n",
      "Epoch 76/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0576 - accuracy: 0.9847 - val_loss: 1.7692 - val_accuracy: 0.6757\n",
      "Epoch 77/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0583 - accuracy: 0.9849 - val_loss: 1.5008 - val_accuracy: 0.7032\n",
      "Epoch 78/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0595 - accuracy: 0.9842 - val_loss: 1.6153 - val_accuracy: 0.7025\n",
      "Epoch 79/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0618 - accuracy: 0.9843 - val_loss: 1.5738 - val_accuracy: 0.7054\n",
      "Epoch 80/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0552 - accuracy: 0.9858 - val_loss: 1.5081 - val_accuracy: 0.6958\n",
      "Epoch 81/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0583 - accuracy: 0.9843 - val_loss: 1.6420 - val_accuracy: 0.7165\n",
      "Epoch 82/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0555 - accuracy: 0.9855 - val_loss: 1.6865 - val_accuracy: 0.6969\n",
      "Epoch 83/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0583 - accuracy: 0.9850 - val_loss: 1.4709 - val_accuracy: 0.6860\n",
      "Epoch 84/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0595 - accuracy: 0.9852 - val_loss: 1.5761 - val_accuracy: 0.7089\n",
      "Epoch 85/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0577 - accuracy: 0.9847 - val_loss: 1.6479 - val_accuracy: 0.6882\n",
      "Epoch 86/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0568 - accuracy: 0.9848 - val_loss: 1.6619 - val_accuracy: 0.7099\n",
      "Epoch 87/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0578 - accuracy: 0.9849 - val_loss: 1.4606 - val_accuracy: 0.7267\n",
      "Epoch 88/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0591 - accuracy: 0.9845 - val_loss: 1.7430 - val_accuracy: 0.7006\n",
      "Epoch 89/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0615 - accuracy: 0.9838 - val_loss: 1.5439 - val_accuracy: 0.7065\n",
      "Epoch 90/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0548 - accuracy: 0.9859 - val_loss: 1.5962 - val_accuracy: 0.7107\n",
      "Epoch 91/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0560 - accuracy: 0.9856 - val_loss: 1.7155 - val_accuracy: 0.6975\n",
      "Epoch 92/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0566 - accuracy: 0.9859 - val_loss: 1.4968 - val_accuracy: 0.7077\n",
      "Epoch 93/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0565 - accuracy: 0.9855 - val_loss: 1.5193 - val_accuracy: 0.7037\n",
      "Epoch 94/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0546 - accuracy: 0.9860 - val_loss: 1.6206 - val_accuracy: 0.6992\n",
      "Epoch 95/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0522 - accuracy: 0.9866 - val_loss: 1.7169 - val_accuracy: 0.7005\n",
      "Epoch 96/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0532 - accuracy: 0.9866 - val_loss: 1.7426 - val_accuracy: 0.7046\n",
      "Epoch 97/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0544 - accuracy: 0.9859 - val_loss: 1.4717 - val_accuracy: 0.7015\n",
      "Epoch 98/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0575 - accuracy: 0.9852 - val_loss: 1.3533 - val_accuracy: 0.7013\n",
      "Epoch 99/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0536 - accuracy: 0.9865 - val_loss: 1.5087 - val_accuracy: 0.7109\n",
      "Epoch 100/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0533 - accuracy: 0.9860 - val_loss: 1.8209 - val_accuracy: 0.6814\n",
      "Epoch 101/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0559 - accuracy: 0.9862 - val_loss: 1.6075 - val_accuracy: 0.6992\n",
      "Epoch 102/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0542 - accuracy: 0.9862 - val_loss: 1.6730 - val_accuracy: 0.6944\n",
      "Epoch 103/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0542 - accuracy: 0.9857 - val_loss: 1.4675 - val_accuracy: 0.7081\n",
      "Epoch 104/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0508 - accuracy: 0.9875 - val_loss: 1.4756 - val_accuracy: 0.6981\n",
      "Epoch 105/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0529 - accuracy: 0.9863 - val_loss: 1.5220 - val_accuracy: 0.7058\n",
      "Epoch 106/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0526 - accuracy: 0.9872 - val_loss: 1.5852 - val_accuracy: 0.7056\n",
      "Epoch 107/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0552 - accuracy: 0.9859 - val_loss: 1.4424 - val_accuracy: 0.6969\n",
      "Epoch 108/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0547 - accuracy: 0.9859 - val_loss: 1.4012 - val_accuracy: 0.7099\n",
      "Epoch 109/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0552 - accuracy: 0.9866 - val_loss: 1.4832 - val_accuracy: 0.7022\n",
      "Epoch 110/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0549 - accuracy: 0.9859 - val_loss: 1.4407 - val_accuracy: 0.6989\n",
      "Epoch 111/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0511 - accuracy: 0.9873 - val_loss: 1.6328 - val_accuracy: 0.6767\n",
      "Epoch 112/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0496 - accuracy: 0.9883 - val_loss: 1.6063 - val_accuracy: 0.7061\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0520 - accuracy: 0.9868 - val_loss: 1.5551 - val_accuracy: 0.6784\n",
      "Epoch 114/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0528 - accuracy: 0.9867 - val_loss: 1.7906 - val_accuracy: 0.7014\n",
      "Epoch 115/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0511 - accuracy: 0.9864 - val_loss: 1.7625 - val_accuracy: 0.6919\n",
      "Epoch 116/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0510 - accuracy: 0.9871 - val_loss: 1.4412 - val_accuracy: 0.7045\n",
      "Epoch 117/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0527 - accuracy: 0.9875 - val_loss: 1.4720 - val_accuracy: 0.6918\n",
      "Epoch 118/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0544 - accuracy: 0.9863 - val_loss: 1.3657 - val_accuracy: 0.7153\n",
      "Epoch 119/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0506 - accuracy: 0.9877 - val_loss: 1.4555 - val_accuracy: 0.7086\n",
      "Epoch 120/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0536 - accuracy: 0.9862 - val_loss: 1.6265 - val_accuracy: 0.6984\n",
      "Epoch 121/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0567 - accuracy: 0.9851 - val_loss: 1.7483 - val_accuracy: 0.6936\n",
      "Epoch 122/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0531 - accuracy: 0.9868 - val_loss: 1.5360 - val_accuracy: 0.6929\n",
      "Epoch 123/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0555 - accuracy: 0.9863 - val_loss: 1.6015 - val_accuracy: 0.6870\n",
      "Epoch 124/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0521 - accuracy: 0.9871 - val_loss: 1.6492 - val_accuracy: 0.6737\n",
      "Epoch 125/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0513 - accuracy: 0.9874 - val_loss: 1.5063 - val_accuracy: 0.6789\n",
      "Epoch 126/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0514 - accuracy: 0.9872 - val_loss: 1.6769 - val_accuracy: 0.6782\n",
      "Epoch 127/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0502 - accuracy: 0.9873 - val_loss: 1.6568 - val_accuracy: 0.7194\n",
      "Epoch 128/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0501 - accuracy: 0.9873 - val_loss: 1.5986 - val_accuracy: 0.6992\n",
      "Epoch 129/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0489 - accuracy: 0.9880 - val_loss: 1.5224 - val_accuracy: 0.7039\n",
      "Epoch 130/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0504 - accuracy: 0.9877 - val_loss: 1.7929 - val_accuracy: 0.6779\n",
      "Epoch 131/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0555 - accuracy: 0.9860 - val_loss: 1.7290 - val_accuracy: 0.6906\n",
      "Epoch 132/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0496 - accuracy: 0.9879 - val_loss: 1.7687 - val_accuracy: 0.6794\n",
      "Epoch 133/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0515 - accuracy: 0.9874 - val_loss: 1.6128 - val_accuracy: 0.6841\n",
      "Epoch 134/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0483 - accuracy: 0.9884 - val_loss: 1.7103 - val_accuracy: 0.6842\n",
      "Epoch 135/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0482 - accuracy: 0.9882 - val_loss: 1.6729 - val_accuracy: 0.7003\n",
      "Epoch 136/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0473 - accuracy: 0.9887 - val_loss: 1.8970 - val_accuracy: 0.7041\n",
      "Epoch 137/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0491 - accuracy: 0.9883 - val_loss: 1.6033 - val_accuracy: 0.6957\n",
      "Epoch 138/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0488 - accuracy: 0.9879 - val_loss: 1.5900 - val_accuracy: 0.6981\n",
      "Epoch 139/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0504 - accuracy: 0.9869 - val_loss: 1.7222 - val_accuracy: 0.6785\n",
      "Epoch 140/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0477 - accuracy: 0.9885 - val_loss: 1.4727 - val_accuracy: 0.7150\n",
      "Epoch 141/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0544 - accuracy: 0.9869 - val_loss: 1.6510 - val_accuracy: 0.7074\n",
      "Epoch 142/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0507 - accuracy: 0.9875 - val_loss: 1.6738 - val_accuracy: 0.7145\n",
      "Epoch 143/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0490 - accuracy: 0.9881 - val_loss: 1.8030 - val_accuracy: 0.7015\n",
      "Epoch 144/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0499 - accuracy: 0.9878 - val_loss: 1.6020 - val_accuracy: 0.7047\n",
      "Epoch 145/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0474 - accuracy: 0.9880 - val_loss: 1.5755 - val_accuracy: 0.6930\n",
      "Epoch 146/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0484 - accuracy: 0.9881 - val_loss: 1.5103 - val_accuracy: 0.7090\n",
      "Epoch 147/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0508 - accuracy: 0.9877 - val_loss: 1.5219 - val_accuracy: 0.6928\n",
      "Epoch 148/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0495 - accuracy: 0.9879 - val_loss: 1.6923 - val_accuracy: 0.6915\n",
      "Epoch 149/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0468 - accuracy: 0.9887 - val_loss: 1.6278 - val_accuracy: 0.6938\n",
      "Epoch 150/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0480 - accuracy: 0.9887 - val_loss: 1.4974 - val_accuracy: 0.6964\n",
      "Epoch 151/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0475 - accuracy: 0.9885 - val_loss: 1.5303 - val_accuracy: 0.7116\n",
      "Epoch 152/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0486 - accuracy: 0.9880 - val_loss: 1.3246 - val_accuracy: 0.7099\n",
      "Epoch 153/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0475 - accuracy: 0.9883 - val_loss: 1.5421 - val_accuracy: 0.6965\n",
      "Epoch 154/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0470 - accuracy: 0.9885 - val_loss: 1.5725 - val_accuracy: 0.6902\n",
      "Epoch 155/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0460 - accuracy: 0.9886 - val_loss: 1.7580 - val_accuracy: 0.6973\n",
      "Epoch 156/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0477 - accuracy: 0.9879 - val_loss: 1.6558 - val_accuracy: 0.6872\n",
      "Epoch 157/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0466 - accuracy: 0.9891 - val_loss: 1.5746 - val_accuracy: 0.6716\n",
      "Epoch 158/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0468 - accuracy: 0.9883 - val_loss: 1.4065 - val_accuracy: 0.7084\n",
      "Epoch 159/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0491 - accuracy: 0.9877 - val_loss: 1.7246 - val_accuracy: 0.6711\n",
      "Epoch 160/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0420 - accuracy: 0.9901 - val_loss: 1.6164 - val_accuracy: 0.6832\n",
      "Epoch 161/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0477 - accuracy: 0.9886 - val_loss: 1.6021 - val_accuracy: 0.6902\n",
      "Epoch 162/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0470 - accuracy: 0.9886 - val_loss: 1.5895 - val_accuracy: 0.7023\n",
      "Epoch 163/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0480 - accuracy: 0.9883 - val_loss: 1.6978 - val_accuracy: 0.6829\n",
      "Epoch 164/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0462 - accuracy: 0.9889 - val_loss: 1.8879 - val_accuracy: 0.6694\n",
      "Epoch 165/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0464 - accuracy: 0.9888 - val_loss: 1.5295 - val_accuracy: 0.7107\n",
      "Epoch 166/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0459 - accuracy: 0.9886 - val_loss: 1.6255 - val_accuracy: 0.6758\n",
      "Epoch 167/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0456 - accuracy: 0.9885 - val_loss: 1.6430 - val_accuracy: 0.7139\n",
      "Epoch 168/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0463 - accuracy: 0.9883 - val_loss: 1.8795 - val_accuracy: 0.6860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0485 - accuracy: 0.9885 - val_loss: 1.8487 - val_accuracy: 0.6869\n",
      "Epoch 170/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0450 - accuracy: 0.9896 - val_loss: 1.8266 - val_accuracy: 0.6976\n",
      "Epoch 171/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0460 - accuracy: 0.9890 - val_loss: 1.7701 - val_accuracy: 0.7081\n",
      "Epoch 172/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0460 - accuracy: 0.9888 - val_loss: 1.6788 - val_accuracy: 0.7008\n",
      "Epoch 173/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0446 - accuracy: 0.9893 - val_loss: 1.7356 - val_accuracy: 0.7117\n",
      "Epoch 174/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0444 - accuracy: 0.9891 - val_loss: 1.5718 - val_accuracy: 0.7089\n",
      "Epoch 175/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0467 - accuracy: 0.9890 - val_loss: 1.7936 - val_accuracy: 0.6906\n",
      "Epoch 176/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0442 - accuracy: 0.9894 - val_loss: 1.8160 - val_accuracy: 0.7113\n",
      "Epoch 177/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0425 - accuracy: 0.9898 - val_loss: 1.7389 - val_accuracy: 0.6969\n",
      "Epoch 178/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0440 - accuracy: 0.9896 - val_loss: 1.6151 - val_accuracy: 0.6916\n",
      "Epoch 179/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0470 - accuracy: 0.9886 - val_loss: 1.6868 - val_accuracy: 0.7151\n",
      "Epoch 180/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0428 - accuracy: 0.9898 - val_loss: 1.7749 - val_accuracy: 0.7028\n",
      "Epoch 181/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0441 - accuracy: 0.9896 - val_loss: 1.5646 - val_accuracy: 0.7013\n",
      "Epoch 182/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0434 - accuracy: 0.9891 - val_loss: 1.5726 - val_accuracy: 0.7085\n",
      "Epoch 183/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0444 - accuracy: 0.9897 - val_loss: 1.6419 - val_accuracy: 0.6929\n",
      "Epoch 184/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0443 - accuracy: 0.9895 - val_loss: 1.6258 - val_accuracy: 0.6941\n",
      "Epoch 185/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0432 - accuracy: 0.9897 - val_loss: 1.7243 - val_accuracy: 0.6879\n",
      "Epoch 186/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0508 - accuracy: 0.9873 - val_loss: 1.6948 - val_accuracy: 0.6983\n",
      "Epoch 187/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0475 - accuracy: 0.9882 - val_loss: 1.5357 - val_accuracy: 0.6809\n",
      "Epoch 188/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0426 - accuracy: 0.9899 - val_loss: 1.5849 - val_accuracy: 0.7155\n",
      "Epoch 189/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0471 - accuracy: 0.9887 - val_loss: 1.6873 - val_accuracy: 0.7147\n",
      "Epoch 190/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0456 - accuracy: 0.9885 - val_loss: 1.7554 - val_accuracy: 0.6959\n",
      "Epoch 191/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0446 - accuracy: 0.9887 - val_loss: 1.6646 - val_accuracy: 0.7018\n",
      "Epoch 192/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0469 - accuracy: 0.9886 - val_loss: 1.5059 - val_accuracy: 0.7183\n",
      "Epoch 193/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0457 - accuracy: 0.9890 - val_loss: 1.7386 - val_accuracy: 0.7133\n",
      "Epoch 194/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0440 - accuracy: 0.9896 - val_loss: 1.7925 - val_accuracy: 0.6879\n",
      "Epoch 195/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0471 - accuracy: 0.9888 - val_loss: 1.6988 - val_accuracy: 0.6894\n",
      "Epoch 196/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0433 - accuracy: 0.9893 - val_loss: 1.7529 - val_accuracy: 0.6958\n",
      "Epoch 197/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0438 - accuracy: 0.9894 - val_loss: 1.6009 - val_accuracy: 0.7111\n",
      "Epoch 198/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0418 - accuracy: 0.9897 - val_loss: 1.6486 - val_accuracy: 0.7054\n",
      "Epoch 199/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0488 - accuracy: 0.9882 - val_loss: 1.5397 - val_accuracy: 0.7043\n",
      "Epoch 200/200\n",
      "1969/1969 [==============================] - 8s 4ms/step - loss: 0.0436 - accuracy: 0.9896 - val_loss: 1.7362 - val_accuracy: 0.6619\n",
      "INFO:tensorflow:Assets written to: model4/assets\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(units= 5, activation = 'softmax'))\n",
    "model4.compile(optimizer='adam',metrics=['accuracy'],loss='sparse_categorical_crossentropy')\n",
    "model4.fit(x_train, y_train, \n",
    "          epochs=200, \n",
    "          batch_size=64,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "model4.save('model4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1/705 [..............................] - ETA: 1:57"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705/705 [==============================] - 1s 762us/step\n",
      "total accuracy: 0.6619499645138396\n",
      "[[4060 2995  403    0    0]\n",
      " [  36 9486  188    1    0]\n",
      " [ 143  901 1377    0    0]\n",
      " [   0 2546    8    0    0]\n",
      " [   0  400    0    0    0]]\n",
      "dos accuracy: 0.5443818718155001\n",
      "normal accuracy 0.9768303985171455\n",
      "probe accuracy 0.5687732342007435\n",
      "r2l accuracy 0.0\n",
      "u2r accuracy 0.0\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.54      0.69      7458\n",
      "           1       0.58      0.98      0.73      9711\n",
      "           2       0.70      0.57      0.63      2421\n",
      "           3       0.00      0.00      0.00      2554\n",
      "           4       0.00      0.00      0.00       400\n",
      "\n",
      "    accuracy                           0.66     22544\n",
      "   macro avg       0.45      0.42      0.41     22544\n",
      "weighted avg       0.64      0.66      0.61     22544\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_result = model4.predict_classes(x_test, verbose=1)\n",
    "y_pred = []\n",
    "for label in y_result:\n",
    "    if label == 0:\n",
    "        y_pred.append(0)\n",
    "    elif label == 1:\n",
    "        y_pred.append(1)\n",
    "    elif label == 2:\n",
    "        y_pred.append(2)\n",
    "    elif label == 3:\n",
    "        y_pred.append(3)\n",
    "    else:\n",
    "        y_pred.append(4)\n",
    "\n",
    "print(\"total accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)\n",
    "print(\"dos accuracy:\",result[0][0]/sum(result[0]))\n",
    "print(\"normal accuracy\",result[1][1]/sum(result[1]))\n",
    "print(\"probe accuracy\",result[2][2]/sum(result[2]))\n",
    "print(\"r2l accuracy\",result[3][3]/sum(result[3]))\n",
    "print(\"u2r accuracy\",result[4][4]/sum(result[4]))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1969/1969 [==============================] - 12s 5ms/step - loss: 1.3313 - accuracy: 0.4550 - val_loss: 1.3760 - val_accuracy: 0.6553\n",
      "Epoch 2/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.5480 - accuracy: 0.8384 - val_loss: 1.4422 - val_accuracy: 0.6423\n",
      "Epoch 3/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.4249 - accuracy: 0.8630 - val_loss: 1.2918 - val_accuracy: 0.6629\n",
      "Epoch 4/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.3219 - accuracy: 0.9088 - val_loss: 1.3704 - val_accuracy: 0.6813\n",
      "Epoch 5/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.2706 - accuracy: 0.9284 - val_loss: 1.2361 - val_accuracy: 0.7047\n",
      "Epoch 6/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.2166 - accuracy: 0.9437 - val_loss: 1.1679 - val_accuracy: 0.7081\n",
      "Epoch 7/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1990 - accuracy: 0.9497 - val_loss: 1.1002 - val_accuracy: 0.6995\n",
      "Epoch 8/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1829 - accuracy: 0.9527 - val_loss: 1.0937 - val_accuracy: 0.7008\n",
      "Epoch 9/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1649 - accuracy: 0.9580 - val_loss: 1.0032 - val_accuracy: 0.7221\n",
      "Epoch 10/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1623 - accuracy: 0.9580 - val_loss: 1.0451 - val_accuracy: 0.7129\n",
      "Epoch 11/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1532 - accuracy: 0.9604 - val_loss: 1.2790 - val_accuracy: 0.6821\n",
      "Epoch 12/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1510 - accuracy: 0.9614 - val_loss: 1.0511 - val_accuracy: 0.7047\n",
      "Epoch 13/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1461 - accuracy: 0.9633 - val_loss: 1.2352 - val_accuracy: 0.6641\n",
      "Epoch 14/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1321 - accuracy: 0.9670 - val_loss: 1.1539 - val_accuracy: 0.6978\n",
      "Epoch 15/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1276 - accuracy: 0.9679 - val_loss: 1.1573 - val_accuracy: 0.6869\n",
      "Epoch 16/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1255 - accuracy: 0.9694 - val_loss: 1.1869 - val_accuracy: 0.6994\n",
      "Epoch 17/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1215 - accuracy: 0.9699 - val_loss: 1.3179 - val_accuracy: 0.6934\n",
      "Epoch 18/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1259 - accuracy: 0.9683 - val_loss: 1.2673 - val_accuracy: 0.7094\n",
      "Epoch 19/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1193 - accuracy: 0.9703 - val_loss: 1.3166 - val_accuracy: 0.6978\n",
      "Epoch 20/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1130 - accuracy: 0.9713 - val_loss: 1.2276 - val_accuracy: 0.6718\n",
      "Epoch 21/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1136 - accuracy: 0.9720 - val_loss: 1.1191 - val_accuracy: 0.6708\n",
      "Epoch 22/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1114 - accuracy: 0.9717 - val_loss: 1.0451 - val_accuracy: 0.6782\n",
      "Epoch 23/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1066 - accuracy: 0.9726 - val_loss: 1.1557 - val_accuracy: 0.7000\n",
      "Epoch 24/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1126 - accuracy: 0.9728 - val_loss: 1.1544 - val_accuracy: 0.6950\n",
      "Epoch 25/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1109 - accuracy: 0.9715 - val_loss: 1.2022 - val_accuracy: 0.6829\n",
      "Epoch 26/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1058 - accuracy: 0.9732 - val_loss: 1.1415 - val_accuracy: 0.7138\n",
      "Epoch 27/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1028 - accuracy: 0.9732 - val_loss: 1.1770 - val_accuracy: 0.7099\n",
      "Epoch 28/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1038 - accuracy: 0.9729 - val_loss: 1.2482 - val_accuracy: 0.6606\n",
      "Epoch 29/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.1038 - accuracy: 0.9731 - val_loss: 1.2833 - val_accuracy: 0.6754\n",
      "Epoch 30/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0987 - accuracy: 0.9744 - val_loss: 1.0575 - val_accuracy: 0.7099\n",
      "Epoch 31/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0964 - accuracy: 0.9752 - val_loss: 1.0940 - val_accuracy: 0.7115\n",
      "Epoch 32/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0942 - accuracy: 0.9750 - val_loss: 1.2683 - val_accuracy: 0.6734\n",
      "Epoch 33/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0961 - accuracy: 0.9746 - val_loss: 1.4563 - val_accuracy: 0.6807\n",
      "Epoch 34/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0952 - accuracy: 0.9747 - val_loss: 1.2053 - val_accuracy: 0.7361\n",
      "Epoch 35/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0948 - accuracy: 0.9749 - val_loss: 1.2557 - val_accuracy: 0.6873\n",
      "Epoch 36/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0950 - accuracy: 0.9747 - val_loss: 1.4598 - val_accuracy: 0.6648\n",
      "Epoch 37/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0892 - accuracy: 0.9759 - val_loss: 1.1932 - val_accuracy: 0.7071\n",
      "Epoch 38/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0882 - accuracy: 0.9769 - val_loss: 1.3686 - val_accuracy: 0.6932\n",
      "Epoch 39/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0877 - accuracy: 0.9768 - val_loss: 1.1714 - val_accuracy: 0.7046\n",
      "Epoch 40/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0891 - accuracy: 0.9759 - val_loss: 1.3778 - val_accuracy: 0.6912\n",
      "Epoch 41/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0864 - accuracy: 0.9773 - val_loss: 1.2796 - val_accuracy: 0.7055\n",
      "Epoch 42/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0836 - accuracy: 0.9776 - val_loss: 1.3843 - val_accuracy: 0.6942\n",
      "Epoch 43/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0813 - accuracy: 0.9785 - val_loss: 1.4565 - val_accuracy: 0.6680\n",
      "Epoch 44/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0886 - accuracy: 0.9770 - val_loss: 1.3235 - val_accuracy: 0.7071\n",
      "Epoch 45/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0852 - accuracy: 0.9773 - val_loss: 1.3003 - val_accuracy: 0.6935\n",
      "Epoch 46/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0796 - accuracy: 0.9786 - val_loss: 1.1937 - val_accuracy: 0.7248\n",
      "Epoch 47/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0794 - accuracy: 0.9792 - val_loss: 1.2537 - val_accuracy: 0.6801\n",
      "Epoch 48/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0803 - accuracy: 0.9784 - val_loss: 1.1749 - val_accuracy: 0.7106\n",
      "Epoch 49/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0766 - accuracy: 0.9791 - val_loss: 1.4018 - val_accuracy: 0.7000\n",
      "Epoch 50/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0773 - accuracy: 0.9787 - val_loss: 1.4114 - val_accuracy: 0.6757\n",
      "Epoch 51/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0803 - accuracy: 0.9784 - val_loss: 1.2997 - val_accuracy: 0.6913\n",
      "Epoch 52/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0786 - accuracy: 0.9788 - val_loss: 1.0966 - val_accuracy: 0.7350\n",
      "Epoch 53/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0800 - accuracy: 0.9785 - val_loss: 1.5125 - val_accuracy: 0.7134\n",
      "Epoch 54/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0735 - accuracy: 0.9800 - val_loss: 1.3587 - val_accuracy: 0.7207\n",
      "Epoch 55/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0799 - accuracy: 0.9780 - val_loss: 1.3965 - val_accuracy: 0.7118\n",
      "Epoch 56/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0778 - accuracy: 0.9792 - val_loss: 1.3416 - val_accuracy: 0.7159\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0705 - accuracy: 0.9811 - val_loss: 1.3589 - val_accuracy: 0.7036\n",
      "Epoch 58/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0774 - accuracy: 0.9793 - val_loss: 1.2689 - val_accuracy: 0.7232\n",
      "Epoch 59/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0756 - accuracy: 0.9794 - val_loss: 1.1997 - val_accuracy: 0.7213\n",
      "Epoch 60/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0762 - accuracy: 0.9803 - val_loss: 1.3115 - val_accuracy: 0.7503\n",
      "Epoch 61/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0732 - accuracy: 0.9815 - val_loss: 1.2978 - val_accuracy: 0.7258\n",
      "Epoch 62/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0657 - accuracy: 0.9829 - val_loss: 1.2572 - val_accuracy: 0.7106\n",
      "Epoch 63/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0680 - accuracy: 0.9811 - val_loss: 1.4613 - val_accuracy: 0.6894\n",
      "Epoch 64/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0706 - accuracy: 0.9808 - val_loss: 1.2087 - val_accuracy: 0.7391\n",
      "Epoch 65/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0730 - accuracy: 0.9801 - val_loss: 1.3066 - val_accuracy: 0.6980\n",
      "Epoch 66/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0709 - accuracy: 0.9815 - val_loss: 1.2660 - val_accuracy: 0.7150\n",
      "Epoch 67/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0649 - accuracy: 0.9825 - val_loss: 1.3314 - val_accuracy: 0.7001\n",
      "Epoch 68/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0659 - accuracy: 0.9829 - val_loss: 1.2909 - val_accuracy: 0.7001\n",
      "Epoch 69/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0731 - accuracy: 0.9814 - val_loss: 1.4393 - val_accuracy: 0.6884\n",
      "Epoch 70/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0705 - accuracy: 0.9813 - val_loss: 1.2032 - val_accuracy: 0.6925\n",
      "Epoch 71/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0700 - accuracy: 0.9811 - val_loss: 1.2425 - val_accuracy: 0.6915\n",
      "Epoch 72/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0704 - accuracy: 0.9819 - val_loss: 1.5348 - val_accuracy: 0.6833\n",
      "Epoch 73/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0699 - accuracy: 0.9820 - val_loss: 1.3366 - val_accuracy: 0.7076\n",
      "Epoch 74/200\n",
      "1969/1969 [==============================] - 10s 5ms/step - loss: 0.0647 - accuracy: 0.9831 - val_loss: 1.5505 - val_accuracy: 0.7140\n",
      "Epoch 75/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0731 - accuracy: 0.9808 - val_loss: 1.7058 - val_accuracy: 0.7063\n",
      "Epoch 76/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0695 - accuracy: 0.9821 - val_loss: 1.4873 - val_accuracy: 0.7153\n",
      "Epoch 77/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0664 - accuracy: 0.9828 - val_loss: 1.3484 - val_accuracy: 0.7075\n",
      "Epoch 78/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0688 - accuracy: 0.9819 - val_loss: 1.3834 - val_accuracy: 0.7181\n",
      "Epoch 79/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0680 - accuracy: 0.9824 - val_loss: 1.3357 - val_accuracy: 0.7028\n",
      "Epoch 80/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0635 - accuracy: 0.9835 - val_loss: 1.3251 - val_accuracy: 0.7161\n",
      "Epoch 81/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0663 - accuracy: 0.9832 - val_loss: 1.2781 - val_accuracy: 0.6975\n",
      "Epoch 82/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0649 - accuracy: 0.9834 - val_loss: 1.4860 - val_accuracy: 0.7045\n",
      "Epoch 83/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0640 - accuracy: 0.9833 - val_loss: 1.2411 - val_accuracy: 0.7169\n",
      "Epoch 84/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0672 - accuracy: 0.9827 - val_loss: 1.3293 - val_accuracy: 0.7013\n",
      "Epoch 85/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0659 - accuracy: 0.9832 - val_loss: 1.3242 - val_accuracy: 0.6954\n",
      "Epoch 86/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0600 - accuracy: 0.9844 - val_loss: 1.2031 - val_accuracy: 0.7266\n",
      "Epoch 87/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0650 - accuracy: 0.9830 - val_loss: 1.3912 - val_accuracy: 0.7251\n",
      "Epoch 88/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0640 - accuracy: 0.9840 - val_loss: 1.3247 - val_accuracy: 0.6993\n",
      "Epoch 89/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0611 - accuracy: 0.9839 - val_loss: 1.5277 - val_accuracy: 0.6884\n",
      "Epoch 90/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0625 - accuracy: 0.9840 - val_loss: 1.3546 - val_accuracy: 0.6977\n",
      "Epoch 91/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0637 - accuracy: 0.9844 - val_loss: 1.3749 - val_accuracy: 0.6983\n",
      "Epoch 92/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0635 - accuracy: 0.9835 - val_loss: 1.1858 - val_accuracy: 0.7484\n",
      "Epoch 93/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0630 - accuracy: 0.9846 - val_loss: 1.4635 - val_accuracy: 0.7079\n",
      "Epoch 94/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0632 - accuracy: 0.9839 - val_loss: 1.2843 - val_accuracy: 0.7354\n",
      "Epoch 95/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0620 - accuracy: 0.9847 - val_loss: 1.3577 - val_accuracy: 0.7104\n",
      "Epoch 96/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0611 - accuracy: 0.9850 - val_loss: 1.3746 - val_accuracy: 0.7140\n",
      "Epoch 97/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0597 - accuracy: 0.9847 - val_loss: 1.3798 - val_accuracy: 0.7135\n",
      "Epoch 98/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0654 - accuracy: 0.9834 - val_loss: 1.3886 - val_accuracy: 0.7087\n",
      "Epoch 99/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0601 - accuracy: 0.9845 - val_loss: 1.2050 - val_accuracy: 0.7373\n",
      "Epoch 100/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0676 - accuracy: 0.9833 - val_loss: 1.2077 - val_accuracy: 0.7261\n",
      "Epoch 101/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0593 - accuracy: 0.9848 - val_loss: 1.3249 - val_accuracy: 0.7336\n",
      "Epoch 102/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0619 - accuracy: 0.9849 - val_loss: 1.3103 - val_accuracy: 0.7223\n",
      "Epoch 103/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0608 - accuracy: 0.9848 - val_loss: 1.5702 - val_accuracy: 0.6881\n",
      "Epoch 104/200\n",
      "1969/1969 [==============================] - 10s 5ms/step - loss: 0.0604 - accuracy: 0.9848 - val_loss: 1.2484 - val_accuracy: 0.7388\n",
      "Epoch 105/200\n",
      "1969/1969 [==============================] - 10s 5ms/step - loss: 0.0599 - accuracy: 0.9852 - val_loss: 1.5195 - val_accuracy: 0.7217\n",
      "Epoch 106/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0629 - accuracy: 0.9834 - val_loss: 1.3310 - val_accuracy: 0.7041\n",
      "Epoch 107/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0645 - accuracy: 0.9828 - val_loss: 1.2989 - val_accuracy: 0.6904\n",
      "Epoch 108/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0614 - accuracy: 0.9839 - val_loss: 1.4020 - val_accuracy: 0.7103\n",
      "Epoch 109/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0615 - accuracy: 0.9840 - val_loss: 1.3335 - val_accuracy: 0.6835\n",
      "Epoch 110/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0621 - accuracy: 0.9841 - val_loss: 1.1957 - val_accuracy: 0.7128\n",
      "Epoch 111/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0565 - accuracy: 0.9856 - val_loss: 1.3789 - val_accuracy: 0.7078\n",
      "Epoch 112/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0584 - accuracy: 0.9854 - val_loss: 1.2539 - val_accuracy: 0.7201\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0582 - accuracy: 0.9851 - val_loss: 1.3692 - val_accuracy: 0.7164\n",
      "Epoch 114/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0566 - accuracy: 0.9860 - val_loss: 1.3242 - val_accuracy: 0.7223\n",
      "Epoch 115/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0578 - accuracy: 0.9854 - val_loss: 1.3929 - val_accuracy: 0.6967\n",
      "Epoch 116/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0590 - accuracy: 0.9851 - val_loss: 1.3415 - val_accuracy: 0.7123\n",
      "Epoch 117/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0603 - accuracy: 0.9847 - val_loss: 1.3326 - val_accuracy: 0.7065\n",
      "Epoch 118/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0585 - accuracy: 0.9846 - val_loss: 1.3097 - val_accuracy: 0.7108\n",
      "Epoch 119/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0554 - accuracy: 0.9866 - val_loss: 1.2814 - val_accuracy: 0.7253\n",
      "Epoch 120/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0571 - accuracy: 0.9857 - val_loss: 1.2790 - val_accuracy: 0.7092\n",
      "Epoch 121/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0590 - accuracy: 0.9850 - val_loss: 1.3498 - val_accuracy: 0.7318\n",
      "Epoch 122/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0566 - accuracy: 0.9860 - val_loss: 1.2515 - val_accuracy: 0.7197\n",
      "Epoch 123/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0569 - accuracy: 0.9863 - val_loss: 1.3925 - val_accuracy: 0.7084\n",
      "Epoch 124/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0611 - accuracy: 0.9847 - val_loss: 1.1613 - val_accuracy: 0.7451\n",
      "Epoch 125/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0576 - accuracy: 0.9856 - val_loss: 1.1900 - val_accuracy: 0.7203\n",
      "Epoch 126/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0563 - accuracy: 0.9860 - val_loss: 1.1349 - val_accuracy: 0.7218\n",
      "Epoch 127/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0597 - accuracy: 0.9848 - val_loss: 1.2525 - val_accuracy: 0.7238\n",
      "Epoch 128/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0546 - accuracy: 0.9869 - val_loss: 1.3964 - val_accuracy: 0.7186\n",
      "Epoch 129/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0577 - accuracy: 0.9861 - val_loss: 1.2964 - val_accuracy: 0.7225\n",
      "Epoch 130/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0556 - accuracy: 0.9861 - val_loss: 1.4048 - val_accuracy: 0.7146\n",
      "Epoch 131/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0574 - accuracy: 0.9861 - val_loss: 1.5494 - val_accuracy: 0.6894\n",
      "Epoch 132/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0541 - accuracy: 0.9866 - val_loss: 1.4199 - val_accuracy: 0.6901\n",
      "Epoch 133/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0558 - accuracy: 0.9860 - val_loss: 1.4102 - val_accuracy: 0.7334\n",
      "Epoch 134/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0556 - accuracy: 0.9860 - val_loss: 1.5364 - val_accuracy: 0.7083\n",
      "Epoch 135/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0533 - accuracy: 0.9870 - val_loss: 1.3398 - val_accuracy: 0.7071\n",
      "Epoch 136/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0554 - accuracy: 0.9863 - val_loss: 1.3774 - val_accuracy: 0.7212\n",
      "Epoch 137/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0602 - accuracy: 0.9849 - val_loss: 1.3521 - val_accuracy: 0.6871\n",
      "Epoch 138/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0564 - accuracy: 0.9864 - val_loss: 1.3135 - val_accuracy: 0.6979\n",
      "Epoch 139/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0575 - accuracy: 0.9859 - val_loss: 1.3752 - val_accuracy: 0.7128\n",
      "Epoch 140/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0573 - accuracy: 0.9856 - val_loss: 1.3957 - val_accuracy: 0.7164\n",
      "Epoch 141/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0534 - accuracy: 0.9869 - val_loss: 1.4953 - val_accuracy: 0.7159\n",
      "Epoch 142/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0546 - accuracy: 0.9867 - val_loss: 1.6438 - val_accuracy: 0.7260\n",
      "Epoch 143/200\n",
      "1969/1969 [==============================] - 10s 5ms/step - loss: 0.0532 - accuracy: 0.9866 - val_loss: 1.3097 - val_accuracy: 0.7222\n",
      "Epoch 144/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0554 - accuracy: 0.9869 - val_loss: 1.3605 - val_accuracy: 0.6874\n",
      "Epoch 145/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0536 - accuracy: 0.9872 - val_loss: 1.2743 - val_accuracy: 0.7146\n",
      "Epoch 146/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0531 - accuracy: 0.9872 - val_loss: 1.4115 - val_accuracy: 0.7034\n",
      "Epoch 147/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0524 - accuracy: 0.9869 - val_loss: 1.3248 - val_accuracy: 0.7219\n",
      "Epoch 148/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0513 - accuracy: 0.9878 - val_loss: 1.5634 - val_accuracy: 0.7051\n",
      "Epoch 149/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0544 - accuracy: 0.9862 - val_loss: 1.3178 - val_accuracy: 0.6715\n",
      "Epoch 150/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0564 - accuracy: 0.9862 - val_loss: 1.3013 - val_accuracy: 0.7018\n",
      "Epoch 151/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0565 - accuracy: 0.9860 - val_loss: 1.6792 - val_accuracy: 0.7108\n",
      "Epoch 152/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0525 - accuracy: 0.9872 - val_loss: 1.4949 - val_accuracy: 0.6973\n",
      "Epoch 153/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0526 - accuracy: 0.9873 - val_loss: 1.2920 - val_accuracy: 0.7297\n",
      "Epoch 154/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0491 - accuracy: 0.9879 - val_loss: 1.4570 - val_accuracy: 0.7279\n",
      "Epoch 155/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0585 - accuracy: 0.9854 - val_loss: 1.4909 - val_accuracy: 0.7006\n",
      "Epoch 156/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0552 - accuracy: 0.9867 - val_loss: 1.5850 - val_accuracy: 0.7137\n",
      "Epoch 157/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0542 - accuracy: 0.9866 - val_loss: 1.5597 - val_accuracy: 0.7167\n",
      "Epoch 158/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0571 - accuracy: 0.9867 - val_loss: 1.4492 - val_accuracy: 0.7040\n",
      "Epoch 159/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0601 - accuracy: 0.9853 - val_loss: 1.6414 - val_accuracy: 0.7050\n",
      "Epoch 160/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0516 - accuracy: 0.9873 - val_loss: 1.6587 - val_accuracy: 0.7153\n",
      "Epoch 161/200\n",
      "1969/1969 [==============================] - 10s 5ms/step - loss: 0.0530 - accuracy: 0.9872 - val_loss: 1.6747 - val_accuracy: 0.6921\n",
      "Epoch 162/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0560 - accuracy: 0.9861 - val_loss: 1.5949 - val_accuracy: 0.7001\n",
      "Epoch 163/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0544 - accuracy: 0.9866 - val_loss: 1.5335 - val_accuracy: 0.7014\n",
      "Epoch 164/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0578 - accuracy: 0.9860 - val_loss: 1.3968 - val_accuracy: 0.6866\n",
      "Epoch 165/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0506 - accuracy: 0.9872 - val_loss: 1.5080 - val_accuracy: 0.7176\n",
      "Epoch 166/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0516 - accuracy: 0.9879 - val_loss: 1.5599 - val_accuracy: 0.7185\n",
      "Epoch 167/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0519 - accuracy: 0.9869 - val_loss: 1.4174 - val_accuracy: 0.7195\n",
      "Epoch 168/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0516 - accuracy: 0.9878 - val_loss: 1.5377 - val_accuracy: 0.7035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200\n",
      "1969/1969 [==============================] - 10s 5ms/step - loss: 0.0516 - accuracy: 0.9875 - val_loss: 1.4464 - val_accuracy: 0.7031\n",
      "Epoch 170/200\n",
      "1969/1969 [==============================] - 10s 5ms/step - loss: 0.0536 - accuracy: 0.9875 - val_loss: 1.5148 - val_accuracy: 0.7226\n",
      "Epoch 171/200\n",
      "1969/1969 [==============================] - 10s 5ms/step - loss: 0.0500 - accuracy: 0.9884 - val_loss: 1.3811 - val_accuracy: 0.7104\n",
      "Epoch 172/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0542 - accuracy: 0.9874 - val_loss: 1.5096 - val_accuracy: 0.7381\n",
      "Epoch 173/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0537 - accuracy: 0.9866 - val_loss: 1.4949 - val_accuracy: 0.7095\n",
      "Epoch 174/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0549 - accuracy: 0.9865 - val_loss: 1.5594 - val_accuracy: 0.7142\n",
      "Epoch 175/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0497 - accuracy: 0.9886 - val_loss: 1.4192 - val_accuracy: 0.7190\n",
      "Epoch 176/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0513 - accuracy: 0.9878 - val_loss: 1.6854 - val_accuracy: 0.6971\n",
      "Epoch 177/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0513 - accuracy: 0.9875 - val_loss: 1.4918 - val_accuracy: 0.7352\n",
      "Epoch 178/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0521 - accuracy: 0.9875 - val_loss: 1.4624 - val_accuracy: 0.7195\n",
      "Epoch 179/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0499 - accuracy: 0.9882 - val_loss: 1.6310 - val_accuracy: 0.7074\n",
      "Epoch 180/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0497 - accuracy: 0.9881 - val_loss: 1.6661 - val_accuracy: 0.7159\n",
      "Epoch 181/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0538 - accuracy: 0.9866 - val_loss: 1.3788 - val_accuracy: 0.7189\n",
      "Epoch 182/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0532 - accuracy: 0.9876 - val_loss: 1.3501 - val_accuracy: 0.7069\n",
      "Epoch 183/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0512 - accuracy: 0.9876 - val_loss: 1.3523 - val_accuracy: 0.7148\n",
      "Epoch 184/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0528 - accuracy: 0.9877 - val_loss: 1.4184 - val_accuracy: 0.7217\n",
      "Epoch 185/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0513 - accuracy: 0.9879 - val_loss: 1.4937 - val_accuracy: 0.7341\n",
      "Epoch 186/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0518 - accuracy: 0.9880 - val_loss: 1.4579 - val_accuracy: 0.7185\n",
      "Epoch 187/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0514 - accuracy: 0.9873 - val_loss: 1.5523 - val_accuracy: 0.7122\n",
      "Epoch 188/200\n",
      "1969/1969 [==============================] - 10s 5ms/step - loss: 0.0517 - accuracy: 0.9877 - val_loss: 1.5521 - val_accuracy: 0.7084\n",
      "Epoch 189/200\n",
      "1969/1969 [==============================] - 10s 5ms/step - loss: 0.0509 - accuracy: 0.9879 - val_loss: 1.3633 - val_accuracy: 0.7005\n",
      "Epoch 190/200\n",
      "1969/1969 [==============================] - 10s 5ms/step - loss: 0.0491 - accuracy: 0.9882 - val_loss: 1.4180 - val_accuracy: 0.7370\n",
      "Epoch 191/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0540 - accuracy: 0.9863 - val_loss: 1.6591 - val_accuracy: 0.7161\n",
      "Epoch 192/200\n",
      "1969/1969 [==============================] - 10s 5ms/step - loss: 0.0509 - accuracy: 0.9878 - val_loss: 1.5025 - val_accuracy: 0.7047\n",
      "Epoch 193/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0526 - accuracy: 0.9875 - val_loss: 1.4362 - val_accuracy: 0.7226\n",
      "Epoch 194/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0487 - accuracy: 0.9880 - val_loss: 1.5419 - val_accuracy: 0.7107\n",
      "Epoch 195/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0502 - accuracy: 0.9884 - val_loss: 1.5476 - val_accuracy: 0.7231\n",
      "Epoch 196/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0480 - accuracy: 0.9886 - val_loss: 1.4750 - val_accuracy: 0.7068\n",
      "Epoch 197/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0462 - accuracy: 0.9889 - val_loss: 1.6169 - val_accuracy: 0.7238\n",
      "Epoch 198/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0516 - accuracy: 0.9879 - val_loss: 1.6148 - val_accuracy: 0.7178\n",
      "Epoch 199/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0504 - accuracy: 0.9879 - val_loss: 1.5396 - val_accuracy: 0.7032\n",
      "Epoch 200/200\n",
      "1969/1969 [==============================] - 9s 5ms/step - loss: 0.0473 - accuracy: 0.9890 - val_loss: 1.5372 - val_accuracy: 0.7133\n",
      "INFO:tensorflow:Assets written to: model5/assets\n"
     ]
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(units=128, activation = 'relu',kernel_initializer= 'he_uniform'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(units= 5, activation = 'softmax'))\n",
    "model5.compile(optimizer='adam',metrics=['accuracy'],loss='sparse_categorical_crossentropy')\n",
    "model5.fit(x_train, y_train, \n",
    "          epochs=200, \n",
    "          batch_size=64,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "model5.save('model5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705/705 [==============================] - 1s 867us/step\n",
      "total accuracy: 0.7132718239886444\n",
      "[[5115 1858  485    0    0]\n",
      " [  36 9469  205    1    0]\n",
      " [ 185  740 1496    0    0]\n",
      " [   0 2491   63    0    0]\n",
      " [   0  400    0    0    0]]\n",
      "dos accuracy: 0.6858407079646017\n",
      "normal accuracy 0.9750798064051076\n",
      "probe accuracy 0.6179264766625362\n",
      "r2l accuracy 0.0\n",
      "u2r accuracy 0.0\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.69      0.80      7458\n",
      "           1       0.63      0.98      0.77      9711\n",
      "           2       0.67      0.62      0.64      2421\n",
      "           3       0.00      0.00      0.00      2554\n",
      "           4       0.00      0.00      0.00       400\n",
      "\n",
      "    accuracy                           0.71     22544\n",
      "   macro avg       0.45      0.46      0.44     22544\n",
      "weighted avg       0.66      0.71      0.66     22544\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_result = model5.predict_classes(x_test, verbose=1)\n",
    "y_pred = []\n",
    "for label in y_result:\n",
    "    if label == 0:\n",
    "        y_pred.append(0)\n",
    "    elif label == 1:\n",
    "        y_pred.append(1)\n",
    "    elif label == 2:\n",
    "        y_pred.append(2)\n",
    "    elif label == 3:\n",
    "        y_pred.append(3)\n",
    "    else:\n",
    "        y_pred.append(4)\n",
    "\n",
    "print(\"total accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)\n",
    "print(\"dos accuracy:\",result[0][0]/sum(result[0]))\n",
    "print(\"normal accuracy\",result[1][1]/sum(result[1]))\n",
    "print(\"probe accuracy\",result[2][2]/sum(result[2]))\n",
    "print(\"r2l accuracy\",result[3][3]/sum(result[3]))\n",
    "print(\"u2r accuracy\",result[4][4]/sum(result[4]))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
